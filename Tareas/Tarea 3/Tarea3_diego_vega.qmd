---
title: "MA0501 – Tarea 3"
author: 
  - name: "Diego Alberto Vega Víquez - C38367" 
    email: "diegovv13@gmail.com"
  - name: "José Carlos Quintero Cedeño - C26152" 
    email: "jose.quinterocedeno@ucr.ac.cr"
  - name: "Gabriel Valverde Guzmán - C38060"
    email: "GABRIEL.VALVERDEGUZMAN@ucr.ac.cr"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 1
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    include-in-header: 
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    toc: true
    toc-depth: 1
    toc-location: left
    toc_float: yes
    html-math-method: katex
    css: styles.css
    df_print: paged
    theme: flatly
    highlight: tango
    embed-resources: true
---

\newpage

# Ejercicio 1

::: {.callout-note}
## Instrucción

Desarrolle funciones iterativas y recursivas en R para los algoritmos de los métodos de:

- iteración de punto fijo,
- bisección,
- método de Newton–Raphson,
- método de la secante, y
- método de Steffensen vistos en clase,

y el método Regula–Falsi que se describe en el ejercicio 11.
:::

## Solución

**Metodo de punto fijo**
```{r}
#| code-fold: true
punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_temp <- p0
  while (i <= n) {
    p <- g(p0_temp)
    if (abs(p - p0_temp) < tol) {
      return(p)
    }
    i <- i + 1
    p0_temp <- p
  }
  return(NULL)
}

punto.fijo.rec <- function(p0, tol, n, g) {
  p1 <- g(p0)
  if (abs(p1 - p0) < tol || n < 1) {
    if (n >= 1) {
      return(p1)
    }
    else{
      return(Inf)
    }
  }
  else{
    return(punto.fijo.rec(p1, tol, n - 1, g))
  }
}

f1 <- function(x) sqrt(1 + 1/x)

punto.fijo(1, 1e-8, 20, f1)
punto.fijo.rec(1, 1e-8, 20, f1)
```
**Método de bisección**
```{r}
#| code-fold: true
biseccion <- function(a, b, tol, n, g) {
  i <- 1
  a1 <- a
  b1 <- b
  if (g(a) * g(b) > 0) {
    return("No se cumplen las hipotesis")
  }
  else{
    while (i <= n) {
      x <- (a1 + b1) / 2
      if (g(a1) * g(x) > 0) {
        a1 <- x
      }
      else{
        b1 <- x
      }
      if (abs(b1 - a1) < tol) {
        return(list("valor" = x, "iteraciones" = i))
      }
      i <- i + 1
    }
    return(NULL)
  }
}

biseccion.rec <- function(a, b, tol, n, g) {
  a1 <- a
  b1 <- b
  x <- (a + b) / 2
  if (abs(b1 - a1) < tol || n < 1) {
    if (n >= 1) {
      return(x)
    }
    else{
      return(x)
    }
  }
  else {
    if (g(a) * g(x) > 0) {
      return(biseccion.rec(x, b1, tol, n - 1, g))
    }
    else {
      return(biseccion.rec(a1, x, tol, n - 1, g))
    }
  }
}

f <- function(x) (cos(x))^2 - 2 * sin(x)

biseccion(0, 1, 1e-8, 100, f)
biseccion.rec(0, 1, 1e-8, 100, f)
```

**Método de Newton-Raphson**
```{r}
#| code-fold: true
newton.raphson <- function(x0, tol, n, f, df) {
  i <- 1
  x0_temp <- x0
  while (i <= n) {
    x <- x0_temp - f(x0_temp) / df(x0_temp)
    if (abs(x - x0_temp) < tol) {
      return(list("valor" = x, "iteraciones" = i))
    }
    i <- i + 1
    x0_temp <- x
  }
  return(NULL)
}

newton.raphson.rec <- function(x0, tol, n, f, df) {
  x0_temp <- x0
  x <- x0_temp - f(x0_temp) / df(x0_temp)
  if (abs(x - x0) < tol || n < 1) {
    if (n >= 1) {
      return(x)
    }
    else{
      return(NULL)
    }
  }
  else{
    return(newton.raphson.rec(x, tol, n - 1, f, df))
  }
}

f <- function(x) (cos(x))^2 - 2 * sin(x)
df <- function(x) - 2 * cos(x) * sin(x) - 2 * sin(x)

newton.raphson(0.5, 1e-8, 100, f, df)
newton.raphson.rec(0.5, 1e-8, 100, f, df)
```

**Método de la secante**
```{r}
#| code-fold: true
secante <- function(x0, x1, tol, n, f) {
  i <- 2
  x0.temp <- x0
  x1.temp <- x1
  while (i <= n) {
    x <- x1.temp - ((x0.temp - x1.temp) * f(x1.temp)) / (f(x0.temp) - f(x1.temp))
    if (abs(x - x0.temp) < tol) {
      return(list("valor" = x, "iteraciones" = i))
    }
    x0.temp <- x1.temp
    x1.temp <- x
    i <- i + 1
  }
  return(NULL)
}

secante.rec <- function(x0, x1, tol, n, f) {
  x <- x1 - ((x0 - x1) * f(x1)) / (f(x0) - f(x1))
  if (abs(x - x0) < tol || n < 1) {
    if (n >= 1) {
      return(x)
    }
    else{
      return(NULL)
    }
  }
  else{
    return(secante.rec(x1, x, tol, n - 1, f))
  }
}

g <- function(x) cos(x) - x^2

secante(0, 1, 1e-8, 100, g)
secante.rec(0, 1, 1e-8, 100, g)
```
**Método de Steffensen**
```{r}
#| code-fold: true
steffensen <- function(x0, tol, n, f) {
  i <- 2
  x0.temp <- x0
  while (i <= n) {
    x1 <- f(x0.temp)
    x2 <- f(x1)
    x <- x0.temp - (x1 - x0.temp)^2 / (x2 - 2 * x1 + x0.temp)
    if (abs(x - x0.temp) < tol) {
      return(list("valor" = x, "iteraciones" = i))
    }
    i <- i + 1
    x0.temp <- x
  }
  return(NULL)
}

steffensen.rec <- function(x0, tol, n, f) {
  x1 <- f(x0)
  x2 <- f(x1)
  x <- x0 - (x1-x0)^2/(x2-2*x1+x0)
  if(abs(x - x0) < tol || n < 1){
    if(n>=1){
      return(x)
    }
    else{
      return(NULL)
    }
  }
  else{
    return(steffensen.rec(x, tol, n-1, f))
  }
}

f1 <- function(x) sqrt(1 + 1/x)

steffensen(1, 1e-8, 100, f1)
steffensen.rec(1, 1e-8, 100, f1)
```
**Método Regula Falsi**
```{r}
#| code-fold: true
regula.falsi <- function(a,b, tol, n, f){
  i <- 1
  a.temp <- a
  b.temp <- b
  while (i <= n) {
    x <- (a.temp*f(b.temp) - b.temp*f(a.temp))/(f(b.temp) - f(a.temp))
    if(f(a.temp)*f(x) > 0){
      a.temp <- x
    }
    else{
      b.temp <- x
    }
    if(abs(b.temp-a.temp) < tol) {
      return(list("valor" = x, "iteraciones" = i))
    }
    i <- i + 1
  }
  return(NULL)
}

regula.falsi.rec <- function(a, b, tol, n, f) {
  a.temp <- a
  b.temp <- b
  x <- (a.temp*f(b.temp) - b.temp*f(a.temp))/(f(b.temp) - f(a.temp))
  if(abs(b.temp-a.temp)<tol || n<1){
    if(n>=1){
      return(x)
    }
    else{
      return(NULL)
    }
  }
  else{
    if(f(a)*f(x) > 0){
      return(regula.falsi.rec(x, b.temp, tol, n-1, f))
    }
    else{
      return(regula.falsi.rec(a.temp, x, tol, n-1, f))
    }
  }
}

f <- function(x) (cos(x))^2 - 2 * sin(x)

regula.falsi(0, 1, 1e-8, 100, f)
regula.falsi.rec(0, 1, 1e-8, 100, f)

```

# Ejercicio 2

::: {.callout-note}
## Instrucción

Use el procedimiento `Bisección` para resolver las siguientes ecuaciones, con por lo menos 5 dígitos significativos. Compare los resultados con los que se obtienen usando directamente `R`.

a) $\cos^2(x) = 2\sin(x)$ para $0 \leq x \leq 1$

b) $-x^3 + x + 1 = 0$ para $1 \leq x \leq 2$

Para cada una de las ecuaciones anteriores, de acuerdo con los teoremas de error vistos en clase, calcule el número de iteraciones necesarias para obtener la precisión pedida, y compare estos resultados con los obtenidos con **R**.
:::

## Solución

a)
```{r}
#| code-fold: true
f <- function(x) cos(x)^2 - 2*sin(x)
biseccion(0, 1, 5*1e-5, 30, f)

#Calculo con R
raiz.a <- uniroot(f, c(0,1))$root
sprintf("%.20f", raiz.a)  

-log(5*10^-5*raiz.a)/log(2)
```
b)
```{r}
#| code-fold: true
g <- function(x) -x^3 + x + 1
biseccion(1, 2, 5 * 1e-5, 30, g)

#Calculo con R
raiz.b <- uniroot(g, c(1,2))$root
sprintf("%.20f", raiz.b)

-log(5*10^-5*raiz.b)/log(2)

```


# Ejercicio 3

::: {.callout-note}
## Instrucción

Use el procedimiento `Bisección` para resolver las siguientes ecuaciones, con por lo menos 4 dígitos significativos. Compare los resultados con los que se obtienen usando directamente `R`.

a) $e^x + 4x - 5 = 0$

b) $x^4 - 2x - 1 = 0$

Determine gráficamente un intervalo inicial $[a, b]$, con $a, b \in \mathbb{Z}$.
:::

## Solución

Grafico de las funciones:
```{r}
#| code-fold: true
#| warning: false
#| message: false
library(ggplot2)

f1 <- function(x) exp(x) + 4*x - 5
f2 <- function(x) x^4 - 2*x - 1

ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = f1, aes(colour = "f1(x) = exp(x)+4x-5"), size = 1.2) +
  stat_function(fun = f2, aes(colour = "f2(x) = x^4-2x-1"), size = 1.2) +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed") +
  scale_colour_manual(values = c("blue", "darkgreen")) +
  labs(y = "y", colour = "Funciones",
       title = "Gráfico de f1 y f2 en [-3,3]") +
  theme_minimal()
```

Con base en este grafico, es razonable determinar que f1 = 0 en el intervalo [0,1], y que f2 = 0 en el intervalo [1,2] y en [-1,0]


a)
```{r}
#| code-fold: true
biseccion(0,1,1e-4,30,f1)

#Calculo con R
raiz.a <- uniroot(f1,c(0,1))$root
sprintf("%.20f", raiz.a)  
```
b)
```{r}
#| code-fold: true
#Raiz en intervalo [1,2]
biseccion(1,2,1e-4,30,f2)$valor

#Calculo con R
raiz.b.1 <- uniroot(f2,c(1,2))$root
sprintf("%.20f", raiz.b.1)

#Raiz en intervalo [-1,0]
biseccion(-1,0,1e-4,30,f2)$valor

#Calculo con R
raiz.b.2 <- uniroot(f2,c(-1,0))$root
sprintf("%.20f", raiz.b.2)
```

# Ejercicio 4

::: {.callout-note}
## Instrucción

Para cada una de las siguientes ecuaciones, determine el intervalo $[a, b]$ en el cual la iteración de punto fijo converge. Estime el número de iteraciones necesarias para obtener aproximaciones con 5 dígitos significativos, y efectúe los cálculos con el procedimiento `PuntoFijo`. Compare los resultados con los que se obtienen usando directamente `R`.

a) $e^{-x} - x = 0$

b) $e^x = 3x$
:::

## Solución

```{r}
f1 <- function(x) exp(-x) - x
f1.f <- function(x) exp(-x)
f2 <- function(x) exp(x) - 3*x
f2.f <- function(x) exp(x)/3
```

a)
```{r}
punto.fijo(0.5, 1e-5, 18, f1.f)


#Calculo con R
pto.fijo.a <- uniroot(f1, c(0,1))$root
sprintf("%.20f", pto.fijo.a)

```
b)
```{r}
punto.fijo(0.5, 1e-5, 30, f2.f)

#Calculo con R
pto.fijo.b <- uniroot(f2, c(0,1))$root
sprintf("%.20f", pto.fijo.b)
```


# Ejercicio 5

::: {.callout-note}
## Instrucción

Repita el ejercicio anterior usando el método de Steffensen.
:::

## Solución

a)
```{r}
steffensen(0.5, 1e-5, 30, f1.f)

#Calculo con R
pto.fijo.a <- uniroot(f1, c(0,1))$root
sprintf("%.20f", pto.fijo.a)

```
b)
```{r}
steffensen(0.5, 1e-5, 30, f2.f)

#Calculo con R
pto.fijo.b <- uniroot(f2, c(0,1))$root
sprintf("%.20f", pto.fijo.b)
```

# Ejercicio 6

::: {.callout-note}
## Instrucción

Para cada una de las siguientes ecuaciones, determine una función $g$ y un intervalo $[a, b]$ en el cual la iteración de punto fijo converja a una solución positiva de la ecuación. Debe probar que $g(x)$ cumple las hipótesis del teorema de Banach. Encuentre esta solución usando el procedimiento `PuntoFijo`. Compare los resultados con los que se obtienen usando directamente **R**.

a) $3x^2 - e^x = 0$

b) $x - \cos(x) = 0$
:::

## Solución

```{r}
g1 <- function(x) 3*x^2 - exp(x)
g1.g <- function(x) (sqrt(3)/3) * exp(x/2)
g2 <- function(x) x - cos(x)
g2.g <- function(x) cos(x)
```

a)
```{r}
punto.fijo(0.5, 1e-8, 30, g1.g)

#Calculo con R
pto.fijo.a <- uniroot(g1, c(0,1))$root
sprintf("%.20f", pto.fijo.a)
```

b)
```{r}
punto.fijo(0.5, 1e-8, 60, g2.g)

#Calculo con R
pto.fijo.b <- uniroot(g2, c(0,1))$root
sprintf("%.20f", pto.fijo.b)
```
c)
```{r}
g <- function(x) sqrt(x + 2)

punto.fijo(3.5,1e-8, 60,g)
```

# Ejercicio 7

::: {.callout-note}
## Instrucción

Use el procedimiento `Newton` para resolver las siguientes ecuaciones, con por lo menos 4 dígitos significativos. Compare los resultados con los que se obtienen usando directamente `R`.

a) $\cos(x) - x^2 = 0$

b) $4\sin(x) = e^x$
:::

## Solución

```{r}
f1 <- function(x) cos(x) - x^2
df1 <- function(x) -sin(x) - 2*x
f2 <- function(x) exp(x) - 4*sin(x)
df2 <- function(x) exp(x) -4*cos(x)
```

a)
```{r}
newton.raphson(0.5, 1e-4, 10, f1, df1)

#Calculo con R
raiz.a <- uniroot(f1, c(0,1))$root
sprintf("%.20f", raiz.a)
```
b)
```{r}
newton.raphson(0.5, 1e-4, 10, f2, df2)

#Calculo con R
raiz.b.1 <- uniroot(f2, c(0,1))$root
sprintf("%.20f", raiz.b.1)

newton.raphson(1.5, 1e-4, 10, f2, df2)

#Calculo con R
raiz.b.2 <- uniroot(f2, c(1,2))$root
sprintf("%.20f", raiz.b.2)
```

# Ejercicio 8

::: {.callout-note}
## Instrucción

Repita el ejercicio anterior usando el procedimiento `Secante`, y compare los resultados.
:::

## Solución

```{r}
f1 <- function(x) cos(x) - x^2
df1 <- function(x) -sin(x) - 2*x
f2 <- function(x) exp(x) - 4*sin(x)
df2 <- function(x) exp(x) -4*cos(x)
```

a)
```{r}
secante(0,1,1e-4,10,f1)

#Calculo con R
raiz.a <- uniroot(f1, c(0,1))$root
sprintf("%.20f", raiz.a)
```
b)
```{r}
secante(0,1, 1e-4, 10, f2)

#Calculo con R
raiz.b.1 <- uniroot(f2, c(0,1))$root
sprintf("%.20f", raiz.b.1)

secante(1,2, 1e-4, 10, f2)

#Calculo con R
raiz.b.2 <- uniroot(f2, c(1,2))$root
sprintf("%.20f", raiz.b.2)
```

# Ejercicio 9

::: {.callout-note}
## Instrucción

Suponga que $p$ es un cero de multiplicidad $m$ de $f$ donde $f^{(m)}$ es continua en un intervalo abierto que contiene a $p$. Demuestre que el método de punto fijo siguiente:

$$
g(x) = x - m \frac{f(x)}{f'(x)}
$$

cumple que $g'(p) = 0$.
:::

## Solución



# Ejercicio 10

::: {.callout-note}
## Instrucción

Demuestre que el método de Newton aplicado a $x^m - R$ y a $1 - (R/x^m)$ para calcular $\sqrt[m]{R}$ da dos fórmulas diferentes de iterativas. Para $R > 0$ y $m \geq 0$, ¿cuál fórmula es mejor y por qué?
:::

## Solución


# Ejercicio 11

::: {.callout-note}
## Instrucción

**Método Regula Falsi**: Este es otro método para encontrar una raíz de la ecuación $f(x) = 0$ que se encuentra en el intervalo $[a, b]$. El método es similar al de la bisección en que se generan intervalos $[a_i, b_i]$ encerrando la raíz de $f(x) = 0$, y es similar al método de la secante en la forma de obtener los nuevos intervalos aproximados.

Suponiendo que el intervalo $[a_i, b_i]$ contiene una raíz de $f(x) = 0$, calculamos la intersección con el eje $x$ de la recta que pasa por los puntos $(a_i, f(a_i))$ y $(b_i, f(b_i))$, denotando este punto por $p_i$.

- Si $f(p_i)f(a_i) < 0$, se define $a_{i+1} = a_i$ y $b_{i+1} = p_i$;
- En caso contrario, se define $a_{i+1} = p_i$ y $b_{i+1} = b_i$.

  a) Dé una interpretación geométrica del método Regula Falsi con ayuda del siguiente gráfico:
  
  ![](images/Ejercicio11a.png){width=400px fig-align="center"}
  
  b) Calcule la ecuación de la recta que pasa por $(a_i, f(a_i))$ y $(b_i, f(b_i))$.

  c) Pruebe que:  
  $$
  p_i = \frac{a_i f(b_i) - b_i f(a_i)}{f(b_i) - f(a_i)}
  $$
  
  d) Escriba un algoritmo en pseudocódigo para el método *Regula Falsi*.
  
  e) Escriba una función en R para este algoritmo.
:::



# Ejercicio 12

::: {.callout-note}
## Instrucción

Use el método de *Regula Falsi* para aproximar la solución de las siguientes ecuaciones con 6 dígitos significativos, determine el intervalo inicial gráficamente. Compare los resultados con los que se obtienen usando directamente **R**.

a) $\log(1 + x) - x^2 = 0$

b) $x^3 + 2x - 1 = 0$
:::

## Solución

```{r}
f1 <- function(x) log(1 + x) - x^2
f2 <- function(x) x^3 + 2*x - 1
```
a)
```{r}
regula.falsi(-0.5, 0.5, 1e-6, 100, f1)

#Calculo con R
raiz.a.1 <- uniroot(f1, c(-0.5,0.5))$root
sprintf("%.20f", raiz.a.1)

regula.falsi(0.5, 1, 1e-6, 100, f1)

#Calculo con R
raiz.a.2 <- uniroot(f1, c(0.5,1))$root
sprintf("%.20f", raiz.a.2)
```

b)
```{r}
regula.falsi(0,1,1e-6, 100,f2)

#Calculo con R
raiz.b <- uniroot(f2, c(0,1))$root
sprintf("%.20f", raiz.b)
```

# Ejercicio 13

::: {.callout-note}
## Instrucción

Pruebe que la sucesión construida por:

$$
x_{n+1} = 
\begin{cases}
1 & \text{si } n = 0 \\
\dfrac{a + (m - 1)x_n^m}{m x_n^{m-1}} & \text{si } n \geq 1
\end{cases}
$$

converge a $\sqrt[m]{a}$, con $a > 0$.
:::

## Solución



# Ejercicio 14

::: {.callout-note}
## Instrucción

Demuestre que las siguientes sucesiones $(x_n)$ convergen linealmente a $x = 0$. Encuentre cuántos términos deben generarse antes de que $|x_n - x| \leq 5 \times 10^{-2}$.

a) $x_n = \dfrac{1}{n}$, $n \geq 0$

b) $x_n = \dfrac{1}{n^2}$, $n \geq 0$
:::

## Solución

::: {.callout-caution collapse="true"}
##### Prueba a)
 $x_n=\frac{1}{n}$

  **Convergencia a $0$.** 
  
  Vea que como $\lim\limits_{n\to\infty}\frac{1}{n}=0$, la sucesión converge a $x=0$.
  
  Tomando $x_n=\frac{1}{n}$,
  $$\frac{x_{n+1}}{x_n}=\frac{1/(n+1)}{1/n}=\frac{n}{n+1}\xrightarrow[n\to\infty]{}1$$
  Vea entonces que $\lim\limits_{n\to\infty}\frac{|x_{n+1-0}|}{|x_n-0|^1} = 1>0$ lo que implica convergencia lineal.
  
  **Tolerancia:** $|x_n|\le 5\times 10^{-2}$
  $$\frac{1}{n}\le 0.05 \;\;\Longleftrightarrow\;\; n\ge 20.$$
  Se requieren **$n=20$** (el término 20 ya cumple la cota).

:::

::: {.callout-caution collapse="true"}
##### Prueba b)

$x_n=\frac{1}{n^2}$
  
  **Convergencia a $0$.** 

  Note que $\lim\limits_{n\to\infty}\frac{1}{n^2}=0$.
  
  Con $x_n=\frac{1}{n^2}$,
  $$\frac{x_{n+1}}{x_n}=\frac{1/(n+1)^2}{1/n^2}=\Big(\frac{n}{n+1}\Big)^2\xrightarrow[n\to\infty]{}1$$
  **Tolerancia $|x_n|\le 5\times 10^{-2}$.**
  $$\frac{1}{n^2}\le 0.05 \;\;\Longleftrightarrow\;\; n\ge \sqrt{20}\approx 4.472.$$
  Por lo tanto, basta con **$n=5$**.

  - Ambas sucesiones **convergen linealmente a $0$**
  - Para cumplir $|x_n|\le 5\times 10^{-2}$:
    - $x_n=\frac{1}{n}$: **$n\ge 20$**.
    - $x_n=\frac{1}{n^2}$: **$n\ge 5$**.

:::

# Ejercicio 15

::: {.callout-note}
## Instrucción

Demuestre que la sucesión definida por $x_n = 1/n^k$, $n \geq 1$, para cualquier entero positivo $k$, converge linealmente a $x = 0$. Para cada par de enteros $k$ y $m$, determinar un número $N$ para el cual $1/N^k < 10^{-m}$.
:::

## Solución



# Ejercicio 16

::: {.callout-note}
## Instrucción

Demostrar que la sucesión $x_n = 10^{-2^n}$ converge cuadráticamente a cero.
:::

## Solución

Hay que demostrar que existe $\lambda$ positiva, tal que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - 0|}{|x_n - 0|^2} = \lambda,
$$
\begin{align*}
\lim_{n\to\infty}\frac{|x_{n+1}-0|}{|x_n-0|^2}
&= \lim_{n\to\infty}\frac{|10^{-2^{\,n+1}}|}{|10^{-2^{\,n}}|^{2}} \\
&= \lim_{n\to\infty}\frac{10^{-2^{\,n+1}}}{\big(10^{-2^{\,n}}\big)^{2}}
= \lim_{n\to\infty}\frac{10^{-2\cdot 2^{\,n}}}{10^{-2\cdot 2^{\,n}}}
= 1>0
\end{align*}

Por lo tanto, $\lambda=1>0$ y la sucesión converge cuadráticamente a $0$.

# Ejercicio 17

::: {.callout-note}
## Instrucción

Para las siguientes sucesiones $(x_n)$, linealmente convergentes, use el método $\Delta^2$ de Aitken para generar una sucesión $(\bar{x}_n)$ hasta que $|\bar{x}_n - x| \leq 5 \times 10^{-2}$.

a) $x_n = \dfrac{1}{n}$, $n \geq 0$

b) $x_n = \dfrac{1}{n^2}$, $n \geq 0$
:::

## Solución

# Ejercicio 18

::: {.callout-note}
## Instrucción

Un conocido método numérico para minimizar funciones *estrictamente unimodales* (una función $f$ que decrece estrictamente (o crece estrictamente) hasta su punto mínimo (máximo)) se llama la función estrictamente unimodal en un intervalo $[a, b]$, y uno de ellos es el **Método Igualmente Espaciado**. La idea geométrica se ilustra en el siguiente gráfico:

![](images/Ejercicio18.png){width=300px fig-align="center"}

Se toma $a_1 = a$ y $b_1 = b$, y además:
$$
x_1 := \frac{2a + b}{3}, \qquad x_2 := \frac{a + 2b}{3}
$$

Entonces, como se ilustra en el gráfico:

- Si $f(x_1) > f(x_2)$, entonces se toma:
  $$
  a_2 = x_1, \quad b_2 = b_1, \quad p_1 = a_2
  $$
  y el nuevo intervalo es:
  $$
  [a_2, b_2] = [x_1, b_1] = \left[ \frac{2a + b}{3}, b \right]
  $$

- En caso contrario, se toma:
  $$
  a_2 = a_1, \quad b_2 = x_2, \quad p_1 = b_2
  $$
  y el nuevo intervalo es:
  $$
  [a_2, b_2] = [a_1, x_2] = \left[ a_1, \frac{a + 2b}{3} \right]
  $$

Con el nuevo intervalo se aplica el proceso de nuevo, y así sucesivamente hasta que $a_n$ y $b_n$ estén suficientemente cerca. La sucesión $(p_n)$ converge al punto $p$ donde $f$ alcanza el mínimo.

a) De acuerdo con lo anterior, pruebe que $x_1 < x_2$.

b) Construya un algoritmo en pseudocódigo y la correspondiente función en **R** para el método *Igualmente Espaciado* descrito anteriormente.

c) Pruebe que el intervalo construido con el algoritmo anterior satisface la relación:
$$
|b_n - a_n| \leq \left( \frac{2}{3} \right)^{n-1} (b - a)
$$

¿Qué se puede concluir, respecto al error absoluto?
:::

## Solución

b)
```{r}
#| code-fold: true
ig.espaciado <- function(a, b, tol, n, f) {
  i <- 1
  a.temp <- a
  b.temp <- b
  while (i <= n) {
    x1 <- (2 * a.temp + b.temp) / 3
    x2 <- (a.temp + 2 * b.temp) / 3
    if (f(x1) > f(x2)) {
      a.temp <- x1
      x <- a.temp
    }
    else{
      b.temp <- x2
      x <- b.temp
    }
    if (abs(b.temp - a.temp) < tol) {
      return(list("valor" = x, "iteraciones" = i))
    }
    i <- i + 1
  }
  return(NULL)
}
f <- function(x) x^4 - 4*x^3 +3*x^2
f(ig.espaciado(1, 4, 1e-5, 1000, f)$valor)
```

# Ejercicio 19

::: {.callout-note}
## Instrucción

En este ejercicio se presenta el **Método Dicotómico** para minimizar funciones estrictamente unimodales. En este método primeramente se evalúa la función $f(x)$ en dos puntos que están a una distancia de $\varepsilon/2$ del punto medio del intervalo $[a, b]$, es decir se calculan:

$$
f\left( \frac{a + b - \varepsilon}{2} \right) \quad \text{y} \quad f\left( \frac{a + b + \varepsilon}{2} \right),
$$

y se procede a compararlos. Si

### a)

$$
f\left( \frac{a + b - \varepsilon}{2} \right) \leq f\left( \frac{a + b + \varepsilon}{2} \right),
$$

entonces se descarta el intervalo $\left] \frac{a + b + \varepsilon}{2}, b \right]$ y así el punto mínimo $x^*$ debe estar en el intervalo $\left[a, \frac{a + b + \varepsilon}{2} \right],$ el cual será el nuevo intervalo de búsqueda.

Si

$$
f\left( \frac{a + b - \varepsilon}{2} \right) > f\left( \frac{a + b + \varepsilon}{2} \right),
$$

entonces se descarta el intervalo $\left[a, \frac{a + b - \varepsilon}{2} \right[$ y en este caso el punto mínimo $x^*$ debe estar en el intervalo $\left[ \frac{a + b - \varepsilon}{2}, b \right].$

Pruebe que en cualquiera de los casos la longitud del intervalo de búsqueda se reduce de $b - a$ a $\frac{b - a + \varepsilon}{2}.$

### b)

Suponga que se tiene el caso:

$$
f\left( \frac{a + b - \varepsilon}{2} \right) \leq f\left( \frac{a + b + \varepsilon}{2} \right),
$$

entonces se descarta el intervalo $\left] \frac{a + b + \varepsilon}{2}, b \right]$, y el nuevo intervalo de búsqueda es $\left[ a, \frac{a + b + \varepsilon}{2} \right].$

Ahora se debe calcular:

$$
f\left( \frac{3a + b - \varepsilon}{4} \right) \quad \text{y} \quad f\left( \frac{3a + b + 3\varepsilon}{4} \right),
$$

y surgen de nuevo dos posibilidades. Si

$$
f\left( \frac{3a + b - \varepsilon}{4} \right) \leq f\left( \frac{3a + b + 3\varepsilon}{4} \right),
$$

entonces se descarta el intervalo $\left] \frac{3a + b + 3\varepsilon}{4}, \frac{a + b + \varepsilon}{2} \right]$
y el nuevo intervalo de búsqueda es $\left[a, \frac{3a + b + 3\varepsilon}{4} \right]$. En el otro caso se descarta el intervalo $\left[a, \frac{3a + b - \varepsilon}{4} \right]$ y el nuevo intervalo de búsqueda es $\left[ \frac{3a + b - \varepsilon}{4}, \frac{a + b + \varepsilon}{2} \right].$

Pruebe que en cualquiera de los casos la longitud del intervalo de búsqueda se reduce de $\frac{b - a + \varepsilon}{2}$ a $\frac{b - a + 3\varepsilon}{4}.$


### c)

Repitiendo el proceso $n$ veces deben calcularse:

$$
f\left( \frac{(2^n - 1)a + b - \varepsilon}{2^n} \right) \quad \text{y} \quad f\left( \frac{(2^n - 1)a + b + (2^n - 1)\varepsilon}{2^n} \right),
$$

pruebe que en cualquiera de los dos casos la longitud del intervalo de búsqueda es

$$
\frac{b - a + (2^n - 1)\varepsilon}{2^n}.
$$

Con lo cual, si $x_n$ representa la $n$-ésima aproximación del punto mínimo $x^*$, entonces pruebe que:

$$
|x^* - x_n| \leq \frac{b - a + (2^n - 1)\varepsilon}{2^n}.
$$

El proceso se ilustra en la Figuras 1 y 2.

![Figura 1](images/Figura1.png){width=500px fig-align="center"}

![Figura 2](images/Figura2.png){width=500px fig-align="center"}


### d)

Construya un algoritmo en pseudocódigo y la correspondiente función **R** para el *Método Dicotómico* descrito anteriormente.

### e)

¿Cuántas iteraciones se requieren teóricamente para minimizar 
$$
f(x) = x^4 - 4x^3 + 3x^2
$$
en el intervalo $[1, 4]$ si se desea que el resultado tenga un error absoluto menor a $10^{-5}$?  

Encuentre el mínimo.
:::

## Solución

Sea $f$ estrictamente unimodal en $[a,b]$. En el **método dicotómico** se evalúa $f$ en los dos puntos a distancia $\varepsilon/2$ del punto medio:
$$
x_1=\frac{a+b-\varepsilon}{2},\qquad
x_2=\frac{a+b+\varepsilon}{2},
$$

::: {.callout-caution collapse="true"}
##### Prueba a) 
Como $f$ es estrictamente unimodal, si $f(x_1)\le f(x_2)$ el mínimo $x^\star$ no puede estar a la derecha de $x_2$; si $f(x_1)>f(x_2)$, $x^\star$ no puede estar a la izquierda de $x_1$. Por ello es válido descartar, respectivamente, los subintervalos indicados.

  **Caso (a)**: $f(x_1)\le f(x_2)$.

  El nuevo intervalo es $[a,\tfrac{a+b+\varepsilon}{2}]$. Su longitud es
\begin{align*}
\Bigg(\frac{a+b+\varepsilon}{2}\Bigg)-a
&= \frac{a+b+\varepsilon-2a}{2}
= \frac{b-a+\varepsilon}{2}.
\end{align*}

  **Caso (b)**: $f(x_1)>f(x_2)$.

  El nuevo intervalo es $\left[\,\tfrac{a+b-\varepsilon}{2},\,b\,\right]$. Su longitud es
\begin{align*}
b-\Bigg(\frac{a+b-\varepsilon}{2}\Bigg)
&= \frac{2b-a-b+\varepsilon}{2}
= \frac{b-a+\varepsilon}{2}.
\end{align*}

  En ambos casos la longitud pasa de $b-a$ a $\dfrac{b-a+\varepsilon}{2}$.

  Una iteración del método dicotómico reduce el tamaño del intervalo de búsqueda desde $b-a$ hasta
$$
\frac{b-a+\varepsilon}{2},
$$
  manteniendo dentro del nuevo intervalo al punto de mínimo $x^\star$. Reiterando el procedimiento se obtiene una secuencia de intervalos que contienen $x^\star$ y cuyas longitudes decrecen geométricamente (con corrección $\varepsilon$). $\square$

:::

::: {.callout-caution collapse="true"}
##### Prueba b) 

Volvemos a tener dos posibilidades:

  Caso 1: $f(x_3)\le f(x_4)$

  Se descarta el subintervalo $\left]\tfrac{3a+b+3\varepsilon}{4},\;\tfrac{a+b+\varepsilon}{2}\right]$
  y el nuevo intervalo es
$$
I_2=\left[a,\;\frac{3a+b+3\varepsilon}{4}\right].
$$

  **Longitud**:
\begin{align*}
|I_2|
&=\frac{3a+b+3\varepsilon}{4}-a
 =\frac{3a+b+3\varepsilon-4a}{4}
 =\frac{b-a+3\varepsilon}{4}.
\end{align*}

  Caso 2: $f(x_3)> f(x_4)$

  Se descarta el subintervalo $\left[a,\;\tfrac{3a+b-\varepsilon}{4}\right]$ y el nuevo intervalo es
$$
I_2=\left[\frac{3a+b-\varepsilon}{4},\;\frac{a+b+\varepsilon}{2}\right].
$$

  **Longitud**:
\begin{align*}
|I_2|
&=\frac{a+b+\varepsilon}{2}-\frac{3a+b-\varepsilon}{4}
=\frac{2a+2b+2\varepsilon-3a-b+\varepsilon}{4}
=\frac{b-a+3\varepsilon}{4}.
\end{align*}

  Tras la primera evaluación, la longitud había pasado de $b-a$ a $\dfrac{b-a+\varepsilon}{2}$.
  Después de la segunda evaluación (dentro del mismo caso b) ), **en cualquiera de las dos comparaciones** la longitud del intervalo se reduce a
$$
\frac{b-a+3\varepsilon}{4}.
$$

  La unimodalidad estricta garantiza en ambos casos que el punto de mínimo $x^\star$ permanece dentro del nuevo intervalo.

:::

::: {.callout-caution collapse="true"}
##### Prueba c) 

Procederemos por inducción

  Caso Base $n=1$. 
$$
L_1=\frac{b-a+\varepsilon}{2}=\frac{b-a+(2^1-1)\varepsilon}{2^1}.
$$

  Paso inductivo. Suponga que
$$
L_k=\frac{b-a+(2^k-1)\varepsilon}{2^k}.
$$
  Entonces, usando $L_{k+1}=T(L_k)=\dfrac{L_k+\varepsilon}{2}$,
\begin{align*}
L_{k+1}
&=\frac{1}{2}\left(\frac{b-a+(2^k-1)\varepsilon}{2^k}+\varepsilon\right)\\
&=\frac{b-a+(2^k-1)\varepsilon+2^k\varepsilon}{2^{k+1}}
=\frac{b-a+(2^{k+1}-1)\varepsilon}{2^{k+1}}.
\end{align*}
  Queda demostrada por inducción la expresión
$$
\boxed{\,L_n=\dfrac{b-a+(2^n-1)\varepsilon}{2^n}\, }.
$$

  En cada iteración, el mínimo $x^\star$ permanece dentro del intervalo de búsqueda y la aproximación $x_n$ también se toma dentro de ese intervalo. Por lo tanto, la distancia entre ambos no excede la **longitud** del intervalo vigente:
$$
|x^\star-x_n|\le L_n
=\frac{b-a+(2^n-1)\varepsilon}{2^n}.
$$

  El método dicotómico reduce la longitud del intervalo según
$$
L_n=\frac{b-a+(2^n-1)\varepsilon}{2^n},
$$
  y garantiza la cota de error
$$
\boxed{\,|x^\star-x_n|\le \dfrac{b-a+(2^n-1)\varepsilon}{2^n}\, } \qquad \square
$$
:::

d)
```{r}
#| code-fold: true
dicotomico <- function(a, b, e, tol, n, f) {
  i <- 1
  a.temp <- a
  b.temp <- b
  while (i <= n) {
    x1 <- (a.temp + b.temp - e)/2
    x2 <- (a.temp + b.temp + e)/2
    if(f(x1) <= f(x2)){
      b.temp <- x2
      x <- b.temp
    }
    else{
      a.temp <- x1
      x <- a.temp
    }
    if(abs(b.temp - a.temp) < tol){
      return(list("valor" = x, "iteraciones" = i))
    }
    i <- i+1
  }
  return(NULL)
}
```

e)
```{r}
f <- function(x) x^4 -4*x^3 + 3*x^2

dicotomico(1, 4, 0, 1e-8, 1000, f)
```



# Ejercicio 20

::: {.callout-note}
## Instrucción

El objetivo de este ejercicio es demostrar la convergencia del método de Newton–Raphson sin utilizar el Teorema de punto fijo de Banach.

**Teorema:** Sea $f \in C^2[a, b]$. Si $x \in [a, b]$ con $f(x) = 0$ y $f'(x) \neq 0$, entonces existe $\varepsilon > 0$ tal que el método de Newton–Raphson:
$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$
genera una sucesión que está bien definida y converge a $x$ cuando $n \to \infty$, para todo $x_0 \in [x - \varepsilon, x + \varepsilon]$.

Para esto haga lo siguiente:

a) Pruebe que si $(r_n)_{n \geq 0}$ es una sucesión de números positivos o nulos que verifican: $$r_{n+1} \leq r_n^2,$$ y si $r_0 < 1$, entonces $(r_n)$ converge a 0. Además, pruebe que: $$r_n \leq (r_0)^{2^n}.$$

b) Pruebe que:

$$
x_{n+1} - x = \frac{(x_n - x)f'(x_n) - f(x_n) + f(x)}{f'(x_n)}. \tag{1}
$$

c) Pruebe que existen números estrictamente positivos $\epsilon_1$ y $M$ tales que:

$$
\text{para todo } y \in [x - \epsilon_1, x + \epsilon_1] \text{ se tiene que } |f'(y)| \geq M.
$$

d) Justifique por qué:

$$
\max_{y \in [x - \epsilon_1, x + \epsilon_1]} |f''(y)| := L < \infty.
$$

e) Pruebe que el valor absoluto de la parte derecha del numerador de la ecuación (1) es igual a:

$$
\left| \int_{x_n}^{x} f''(t)(x - t) \, dt \right|. \tag{2}
$$

f) Pruebe que (2) está acotado por:

$$
\frac{L}{2} |x_n - x|^2.
$$

g) Pruebe que si se define

$$
r_n := \frac{L}{2M} |x_n - x|
\quad \text{entonces} \quad
r_{n+1} \leq r_n^2.
$$

h) Sea $\epsilon = \min\left(\epsilon_1, \frac{2M}{L} \right)$, pruebe que si $|x_n - x| < \epsilon$, entonces use la parte (a) para probar que $|x_{n+1} - x| < \epsilon$, y concluya el resultado.

:::

## Solución

# Ejercicio 21

::: {.callout-note}
## Instrucción

Demuestre que en el caso de ceros de multiplicidad $m$, el **Método de Newton Modificado**:

$$
x_{n+1} = x_n - m \frac{f(x_n)}{f'(x_n)}
$$

es cuadráticamente convergente. Sugerencia: Use series de Taylor.
:::

## Solución

::: {.callout-caution collapse="true"}
##### Prueba

Como la raíz tiene multiplicidad $m$, existe una función $g$ de clase $C^1$ con $g(\alpha)\ne 0$ tal que
$$
f(x)=(x-\alpha)^m\,g(x),\qquad
f'(x)=m(x-\alpha)^{m-1}g(x)+(x-\alpha)^m g'(x).
$$

Denotemos el error $e_n:=x_n-\alpha$ y abreviemos $g_n:=g(\alpha+e_n)$, $g_n':=g'(\alpha+e_n)$. Entonces
$$
\frac{f(x_n)}{f'(x_n)}
=\frac{e_n^m g_n}{e_n^{m-1}\big(m g_n+e_n g_n'\big)}
=e_n\,\frac{g_n}{m g_n+e_n g_n'}.
$$

El paso de Newton modificado queda
$$
e_{n+1}=x_{n+1}-\alpha
=e_n- m\,e_n\,\frac{g_n}{m g_n+e_n g_n'}
= e_n\!\left(1-\frac{m g_n}{m g_n+e_n g_n'}\right).
$$

Escriba el denominador como $m g_n\!\left(1+\dfrac{e_n g_n'}{m g_n}\right)$; usando la expansión
$$
\frac{1}{1+z}=1-z+z^2+O(z^3)\quad (z\to 0),
$$
se obtiene
$$
\frac{m g_n}{m g_n+e_n g_n'}
= 1-\frac{e_n g_n'}{m g_n}+O(e_n^2).
$$
Sustituyendo en la expresión de $e_{n+1}$:
$$
e_{n+1}
= e_n\!\left(1-\Big[1-\frac{e_n g_n'}{m g_n}+O(e_n^2)\Big]\right)
= \frac{g_n'}{m g_n}\,e_n^2+O(e_n^3).
$$

Como $g$ es $C^1$ y $g(\alpha)\ne 0$, tenemos $g_n\to g(\alpha)$, $g_n'\to g'(\alpha)$ cuando $e_n\to 0$, y por tanto existe una constante
$$
C=\frac{g'(\alpha)}{m\,g(\alpha)}
$$
tal que
$$
e_{n+1}= C\,e_n^2+O(e_n^3).
$$

La relación de error $e_{n+1}=C e_n^2+O(e_n^3)$ muestra que el **método de Newton modificado es cuadráticamente convergente** para raíces de multiplicidad $m$.

> **Comentario.** Para $m=1$ (raíz simple) se recupera el Newton clásico con $e_{n+1}=\dfrac{f''(\alpha)}{2f'(\alpha)}e_n^2+O(e_n^3)$. Para $m\ge2$, el factor cuadrático es $C=\dfrac{g'(\alpha)}{m g(\alpha)}$, donde $f(x)=(x-\alpha)^m g(x)$ y $g(\alpha)\ne 0$.

:::

# Ejercicio 22

::: {.callout-note}
## Instrucción

El objetivo de este ejercicio es demostrar la convergencia del método de la **Secante** sin utilizar el Teorema de punto fijo de Banach.

**Teorema:**  
Sea $f$ una función de clase $C^2[a, b]$, con $a < b$. Si existe un punto $x$ tal que $f(x) = 0$ y $f'(x) \neq 0$, entonces existe un número $\varepsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$, la sucesión generada por el método de la secante

$$
x_{n+1} = x_n - \frac{(x_n - x_{n-1})f(x_n)}{f(x_n) - f(x_{n-1})}
$$

está bien definida dentro del intervalo $[x - \varepsilon, x + \varepsilon]$ y converge a $x$, cero de la ecuación $f(x) = 0$.

a) Sea $(r_n)$ una sucesión de números reales positivos tales que: $$r_{n+1} \leq r_n r_{n-1}$$ Pruebe que si $r_0 < 1$ y $r_1 < 1$, entonces la sucesión $(r_n)$ está acotada por 1 y converge a 0. Además, pruebe que existe una constante $C$ tal que: $$r_n \leq C r^\rho,$$ donde $r$ es un número estrictamente menor que 1, y $$\rho = \frac{1 + \sqrt{5}}{2} \approx 1.618. $$

b) Se denota: $$ f[x_n, x_{n-1}] := \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}$$ pruebe que: $$
f(x_n) = (x_n - x) f[x_n, x] $$

c) Pruebe que: $$x_{n+1} - x = (x_n - x) \cdot \frac{f[x_n, x_{n-1}] - f[x_n, x]}{f[x_n, x_{n-1}]}$$

d) Pruebe que: $$x_{n+1} - x = \frac{(x_n - x)(x_n - x_{n-1}) f[x_n, x_{n-1}, x]}{f[x_n, x_{n-1}]},$$ donde: $$f[x_n, x_{n-1}, x] := \frac{f[x_n, x_{n-1}] - f[x_n, x]}{x_n - x}$$

e) Sea $[x - \epsilon_1, x + \epsilon_1]$ un intervalo sobre el cual $|f'(x)| \geq M$.  
Pruebe que si $x_n$ y $x_{n-1}$ están dentro del intervalo, entonces: $$|f[x_n, x_{n-1}]| \geq M$$

f) Sea $L$ la cota superior de $|f''|$ en el intervalo $[x - \epsilon_1, x + \epsilon_1]$.   Pruebe que si $x_n$ y $x_{n-1}$ están dentro del intervalo, entonces: $$|f[x_n, x_{n-1}, x]| \leq \frac{L}{2}$$

g) Pruebe que: $$|x_{n+1} - x| \leq \frac{L}{2M} |x_n - x| \cdot |x_{n-1} - x|$$

h) Tome: $$ r_n := \frac{L}{2M} |x_n - x|, \quad \text{y} \quad \varepsilon < \min\left( \epsilon_1, \frac{2M}{L} \right)$$ usando (a), concluya que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$, entonces $x_n$ está dentro del intervalo para todo $n$, y $|x_n - x| \to 0$ cuando $n \to \infty$.

:::

## Solución

# Ejercicio 23

::: {.callout-note}
## Instrucción
Pruebe los Teoremas 7, 10, 11 y 12 de la presentación de la clase.
:::

## Solución

::: {.callout-tip}
## Teorema 7

Sea $f$ una función de clase $C^2[a, b]$ con $a < b$.  
Si existe un punto $x$ tal que $f(x) = 0$ y $f'(x) \neq 0$, entonces existe un número $\varepsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$, la sucesión generada por el **método de la secante**
$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

está bien definida dentro del intervalo $[x - \varepsilon, x + \varepsilon]$ y **converge a** $x$, **cero de la ecuación** $f(x) = 0$.

::: {.callout-caution collapse="true"}
### Prueba

Como $f'(\alpha)\ne 0$ y $f'$ es continua, existe $\varepsilon>0$ tal que $f'(x)\neq 0$ para todo $x\in I:=[\alpha-\varepsilon,\alpha+\varepsilon]$. Luego $f$ es estrictamente monótona en $I$.  
Si $x_{n-2},x_{n-1}\in I$ y $x_{n-2}\ne x_{n-1}$, entonces $f(x_{n-2})\ne f(x_{n-1})$; por el Teorema del Valor Medio existe $\xi_n$ entre $x_{n-2}$ y $x_{n-1}$ con
$$
\frac{f(x_{n-2})-f(x_{n-1})}{x_{n-2}-x_{n-1}} = f'(\xi_n)\ne 0,
$$
por lo que $x_n$ queda bien definido. Además, como $f$ es monótona y $f(\alpha)=0$, el punto $x_n$ cae entre $x_{n-2}$ y $x_{n-1}$, de modo que por inducción toda la sucesión permanece en $I$ si $x_0,x_1\in I$.

Sea $e_n:=x_n-\alpha$. La expansión de Taylor alrededor de $\alpha$ da, para $x$ cercano a $\alpha$,
$$
f(x)=f'(\alpha)(x-\alpha) + \tfrac12 f''(\eta_x)(x-\alpha)^2,
$$
donde $\eta_x$ está entre $x$ y $\alpha$. Aplicando en $x_{n-1}$ y $x_{n-2}$ y sustituyendo en la fórmula de la secante, tras simplificar se obtiene
$$
e_n
= \frac{f''(\alpha)}{2f'(\alpha)}\,e_{n-1}e_{n-2}
\;+\; O\!\big(e_{n-1}^2 e_{n-2}\big)
\;+\; O\!\big(e_{n-1} e_{n-2}^2\big).
$$
En particular, existen $C>0$ y (reduciendo $\varepsilon$ si fuera necesario) $\delta\in(0,\varepsilon]$ tales que si $|e_0|,|e_1|<\delta$, entonces
$$
|e_n|\le C\,|e_{n-1}|\,|e_{n-2}| \qquad (n\ge2).
$$

Elija $\delta$ tal que $C\delta<1$. Entonces, por inducción,
$$
|e_2|\le C|e_1||e_0|<\delta,\qquad
|e_3|\le C|e_2||e_1|<\delta,\quad \dots
$$
por lo que todas las iteraciones permanecen en $I$ y la sucesión $\{e_n\}$ es decreciente a $0$.  
Además, de la relación asintótica
$$
e_n \sim \frac{f''(\alpha)}{2f'(\alpha)}\,e_{n-1}e_{n-2},
$$
se deduce la **convergencia superlineal** de orden $\varphi=(1+\sqrt5)/2$. Concluimos que, para $x_0,x_1$ suficientemente cercanos a $\alpha$, la secuencia generada por la secante está bien definida en $I$ y **converge a $\alpha$**.

$\blacksquare$


:::
:::

::: {.callout-tip}
## Teorema 10

Sea $f$ una función de clase $C^2[a, b]$.  
Si existe un punto $\tilde{x}$ tal que $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$, entonces existe un número $\varepsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$, la sucesión generada por el **método de la secante**
$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$
converge a $\tilde{x}$ **con orden de al menos** $\dfrac{1 + \sqrt{5}}{2} \approx 1.618$.

::: {.callout-caution collapse="true"}
### Prueba
Denotemos el error $e_n:=x_n-\tilde x$. Por Taylor alrededor de $\tilde x$, para $x$ cercano a $\tilde x$,
$$
f(x)=f’(\tilde x)(x-\tilde x) + \tfrac12 f’’(\tilde x)(x-\tilde x)^2 + r(x),
\qquad r(x)=o\big((x-\tilde x)^2\big).
$$
Aplicando esto a $x_{n-1}$ y $x_{n-2}$ y sustituyendo en la fórmula de la secante, usando además el Teorema del Valor Medio para el denominador
$$
\frac{f(x_{n-2})-f(x_{n-1})}{x_{n-2}-x_{n-1}}=f’(\xi_n)\quad(\xi_n\ \text{entre }x_{n-2},x_{n-1}),
$$
se obtiene, tras álgebra directa (y absorbiendo los restos en $o(\cdot)$),
$$
e_{n}
= \frac{f’’(\tilde x)}{2,f’(\tilde x)},e_{n-1}e_{n-2};+; o\big(e_{n-1}e_{n-2}\big).
\tag{1}
$$
En particular, existe un $C>0$ tal que (para $n$ grande)
$$
|e_{n+1}| ;=; C,|e_{n}|,|e_{n-1}| ;+; o\big(|e_{n}|,|e_{n-1}|\big),
\qquad
C=\left|\frac{f’’(\tilde x)}{2,f’(\tilde x)}\right|.
\tag{2}
$$
De aquí se deduce que la secuencia es bien definida y converge localmente (si $|e_0|,|e_1|$ son suficientemente pequeños, la relación (2) hace decrecer los errores a cero).
Buscamos un modelo asintótico de la forma
$$
|e_n|\sim \lambda,|e_{n-1}|^{\alpha}, \qquad \lambda>0,\ \alpha>1.
\tag{3}
$$
Sustituyendo (3) en (2) y despreciando términos de orden menor,
$|e_{n+1}|\sim C\,|e_n|\,|e_{n-1}| \sim C\,\big(\lambda |e_{n-1}|^{\alpha}\big)\,|e_{n-1}| = C\lambda\,|e_{n-1}|^{\alpha+1}.$
Pero por (3) también
$|e_{n+1}|\sim \lambda\,|e_n|^{\alpha} \sim \lambda\,(\lambda |e_{n-1}|^{\alpha})^{\alpha} = \lambda^{\alpha+1}\,|e_{n-1}|^{\alpha^2}$
Igualando exponentes y coeficientes dominantes se obtiene el sistema
$$
\alpha^2=\alpha+1,\qquad \lambda^{\alpha}=C.
$$
La ecuación cuadrática da
$$
\boxed{\ \alpha=\frac{1+\sqrt5}{2}\ } \quad(\text{la otra raíz } <1 \text{ se descarta}).
$$
Y el factor asintótico es $\lambda=C^{1/\alpha}$.

De (2) y del cálculo precedente se concluye que la secante, cerca de una raíz simple con $f’’(\tilde x)\neq 0$, verifica
$$
|e_{n+1}|= \lambda,|e_n|^{\alpha}+o\big(|e_n|^{\alpha}\big),
\qquad
\alpha=\frac{1+\sqrt5}{2}>1,
$$
por lo que converge localmente y su orden de convergencia es al menos $\alpha$.
$\blacksquare$

:::
:::

::: {.callout-tip}
## Teorema 11

Sea $(x_n)$ una sucesión que converge a $x$ con orden lineal con constante asintótica $\lambda < 1$, además se asume que $e_n = x_n - x \ne 0$ para todo $n$. Entonces la sucesión $(\tilde{x}_n)$, definida en (1.14), converge a $x$ más rápido que $(x_n)$.

::: {.callout-caution collapse="true"}
### Prueba

Como la convergencia de $(x_n)$ a $x$ es lineal con constante $\lambda<1$, existe $c\neq 0$ tal que
$$
e_n = x_n-x = c\,\lambda^n + r_n,\qquad \frac{r_n}{\lambda^n}\to 0\quad (n\to\infty).
\tag{2}
$$

Expresamos las diferencias progresivas usando (2):

\begin{align*}
\triangle x_n &= x_{n+1}-x_n = e_{n+1}-e_n = c(\lambda^{n+1}-\lambda^n) + o(\lambda^n)
= c(\lambda-1)\lambda^n + o(\lambda^n),\\[2mm]
\triangle^2 x_n &= \triangle(\triangle x_n)
= c(\lambda-1)^2 \lambda^n + o(\lambda^n)\tag{3}
\end{align*}


Inserte (3) en la fórmula de Aitken (1.14):

\begin{align*}
\tilde{x}_{n+3}-x
&= (x_n-x) - \frac{(\triangle x_n)^2}{\triangle^2 x_n} \\
&= \Big[c\lambda^n + o(\lambda^n)\Big]
 - \frac{\Big[c(\lambda-1)\lambda^n + o(\lambda^n)\Big]^2}
        {c(\lambda-1)^2\lambda^n + o(\lambda^n)}.
\end{align*}

Factorizando $c\lambda^n$ arriba y abajo y usando Notación de Landau,
$$
\frac{(\triangle x_n)^2}{\triangle^2 x_n}
= c\lambda^n\;\frac{(1+o(1))}{(1+o(1))}=c\lambda^n+o(\lambda^n).
$$
Por tanto,
$$
\tilde{e}_{n+3}:=\tilde x_{n+3}-x
= \big[c\lambda^n + o(\lambda^n)\big] - \big[c\lambda^n + o(\lambda^n)\big]
= o(\lambda^n).
\tag{4}
$$

De (2)–(4) se tiene
$$\frac{|\tilde e_{n+3}|}{|e_n|} \;\longrightarrow\; 0$$
es decir, $|\tilde x_{n+3}-x|=o(|x_n-x|)$; por lo tanto, $(\tilde x_n)$ **converge más rápido** que $(x_n)$.\quad$\blacksquare$


:::
:::

::: {.callout-tip}

## Teorema 12
Si el método de punto fijo $x_{n+1} = g(x_n)$ converge linealmente, entonces el orden de convergencia del método de Steffensen es al menos dos.

::: {.callout-caution collapse="true"}
### Prueba

Sea $e_n:=x_n-\tilde x$. Como $g$ es $C^2$ en un entorno de $\tilde x$ con $g(\tilde x)=\tilde x$,
por Taylor:
$$
g(\tilde x+e)=\tilde x+a e + b e^2+O(e^3),\qquad
a:=g'(\tilde x),\; b:=\tfrac12 g''(\tilde x).
$$
Entonces
$$
e_{n+1}=a e_n + b e_n^2 + O(e_n^3). \tag{1}
$$

Escribamos las diferencias progresivas de la sucesión de punto fijo:
$$
\Delta x_n:=x_{n+1}-x_n=e_{n+1}-e_n
=(a-1)e_n + b e_n^2+O(e_n^3), \tag{2}
$$
y
$$
\Delta^2 x_n:=x_{n+2}-2x_{n+1}+x_n
=e_{n+2}-2e_{n+1}+e_n. \tag{3}
$$
Usando (1) en $n$ y $n+1$,

\begin{align*}
e_{n+2}
&= a e_{n+1}+b e_{n+1}^2 + O(e_{n+1}^3) \\
&= a(a e_n+b e_n^2)+b (a e_n)^2 + O(e_n^3) \\
&= a^2 e_n + b(a+a^2)e_n^2 + O(e_n^3)
\end{align*}

Sustituyendo en (3) y simplificando,
$$
\Delta^2 x_n
=(a-1)^2 e_n + b\,(a^2+a-2)\,e_n^2 + O(e_n^3). \tag{4}
$$

El paso de **Steffensen** es precisamente la **aceleración $\Delta^2$ de Aitken**:
$$
x^{(S)}_{n+1}
= x_n - \frac{(\Delta x_n)^2}{\Delta^2 x_n},\qquad
\tilde e_{n+1}:=x^{(S)}_{n+1}-\tilde x
= e_n - \frac{(\Delta x_n)^2}{\Delta^2 x_n}.
$$
Con (2)–(4),
\begin{align*}
(\Delta x_n)^2 &= (a-1)^2 e_n^2 + 2(a-1)b e_n^3 + O(e_n^4),\\
\Delta^2 x_n &= (a-1)^2 e_n \Big[1 + \tfrac{b(a^2+a-2)}{(a-1)^2} e_n + O(e_n^2)\Big].
\end{align*}
Dividiendo,
$$
\frac{(\Delta x_n)^2}{\Delta^2 x_n}
= e_n \Big[1 + \tfrac{2b}{(a-1)} e_n - \tfrac{b(a^2+a-2)}{(a-1)^2} e_n + O(e_n^2)\Big]
= e_n + \frac{ab}{1-a}\,e_n^2 + O(e_n^3).
$$
Por lo tanto,
$$
\tilde e_{n+1}
= e_n - \left(e_n + \frac{ab}{1-a}\,e_n^2 + O(e_n^3)\right)
= \frac{ab}{a-1}\,e_n^2 + O(e_n^3).
$$
Finalmente,
$$
\lim_{n\to\infty}\frac{|\tilde e_{n+1}|}{|\tilde e_n|^2}
=\left|\frac{ab}{1-a}\right|
=\frac12\,\left|\frac{g'(\tilde x)\,g''(\tilde x)}{\,1-g'(\tilde x)\,}\right|
=:\lambda\neq 0,
$$
lo que muestra **convergencia cuadrática** (orden $\ge 2$). \hfill$\blacksquare$

:::

:::
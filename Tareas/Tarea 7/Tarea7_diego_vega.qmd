---
title: "MA0501 – Tarea 7"
author: 
  - name: "Diego Alberto Vega Víquez - C38367" 
    email: "diegovv13@gmail.com"
  - name: "José Carlos Quintero Cedeño - C26152" 
    email: "jose.quinterocedeno@ucr.ac.cr"
  - name: "Gabriel Valverde Guzmán - C38060"
    email: "GABRIEL.VALVERDEGUZMAN@ucr.ac.cr"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 1
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    include-in-header: 
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    toc: true
    toc-depth: 1
    toc-location: left
    toc_float: yes
    html-math-method: katex
    css: styles.css
    df_print: paged
    theme: flatly
    highlight: tango
    embed-resources: true
    page-layout: full
editor: 
  markdown: 
    wrap: 72
---

\newpage

# Ejercicio 1

::: {.callout-note title="Instrucción del ejercicio 1"}

Programe en **R** los algoritmos de **Punto Fijo** y del **método de Newton–Raphson** para sistemas de ecuaciones no lineales de $n$ variables vistos en clase.

:::

**Solución**

**Punto fijo**
```{r}
#| code-fold: true

punto.fijo.vv <- function(p0, F, tol = 1e-8, N = 100){
  x.prev <- as.numeric(p0)
  m <- length(p0)
  x.next <- x.prev
  for (i in 1:N) {
    for (j in 1:m) {
      x.next[j] <- F[[j]](x.prev)
    }
    if(abs(max(x.next - x.prev)) < tol){
      return(x.next)
    }
    x.prev <- x.next
  }
  return(paste0("Numero maximo de iteraciones excedido: ", N))
}

#Prueba
f1 <- function(x) (1/3)*cos(x[2]*x[3]) + 1/6
f2 <- function(x) (1/9)*sqrt(x[1]^2 + sin(x[3]) + 1.06) - 0.1
f3 <- function(x) -(1/20)*exp(-x[1]*x[2]) - (10*pi - 3)/60

F <- list(f1, f2, f3)

p0 <- c(0, 0, 0)
sol <- punto.fijo.vv(p0, F)
sol
```
**Newton Raphson**
```{r}
#| code-fold: true

#funciones auxiliares
evaluar.componente <- function(gj, x){
  return(gj(x))
}

evaluar.funcion.vectorial <- function(G, x){
  vapply(G, evaluar.componente, numeric(1), x = x)
}

calcular.jacobiano <- function(G, x, h = sqrt(.Machine$double.eps)){
  x <- as.numeric(x)
  m <- length(x)
  J <- matrix(0, nrow = m, ncol = m)
  for (j in 1:m) {
    ej <- rep(0, m)
    ej[j] <- 1
    fp <- evaluar.funcion.vectorial(G, x + h * ej)
    fm <- evaluar.funcion.vectorial(G, x - h * ej)
    J[, j] <- (fp - fm)/(2*h)
  }
  return(J)
}

#funcion
newton.raphson.vv <- function(p0, F, tol = 1e-8, N = 50){
  x.prev <- as.numeric(p0)
  for (i in 1:N) {
    J <- calcular.jacobiano(F, x.prev)
    Fx <- evaluar.funcion.vectorial(F, x.prev)
    delta <- try(solve(J, Fx), silent = TRUE)
    if(inherits(delta, "try-error") || any(!is.finite(delta))) {
      stop("Falla al resolver el sistema lineal: jacobiana singular o mal condicionada")
    }
    x.next <- x.prev - as.numeric(delta)
    
    if(abs(max(x.next - x.prev)) < tol){
      return(x.next)
    }
    x.prev <- x.next
  }
  return("Numero maximo de iteraciones excedido")
}

#prueba
r1 <- function(x) x[1]^2 + x[2]^2 + 0.6*x[2] - 0.16
r2 <- function(x) x[1]^2 - x[2]^2 + x[1] -1.6*x[2] - 0.14

R <- list(r1, r2)
p0 <- c(0.6, 0.25)

sol <- newton.raphson.vv(p0, R)
sol
```

# Ejercicio 2

::: {.callout-note title="Instrucción del ejercicio 2"}

El **Método Simplificado de Newton** o de **Newton–Kantorovich** tiene el siguiente esquema de iteración:

$$
x_{n+1} = x_n - [df(x_0)]^{-1} f(x_n), \qquad n = 0, 1, 2, \dots
$$

donde $df(x)$ es la matriz Jacobiana de $f$.

a) Escriba una función en **R** para este método.

b) Sea $C \subset \mathbb{R}^n$ un abierto y sea $f : \Omega \to \mathbb{R}^n$ una aplicación diferenciable, sea $C \subset \Omega$ convexo y cerrado y sea $x_0$ en el interior de $C$.  
Se supone que $df$ verifica la **condición de Lipschitz** dentro de $C$ con parámetro $\lambda > 0$ y que $df(x_0)$ es invertible. Se denota:

$$
A_0 = [df(x_0)]^{-1}, 
\quad 
\alpha = \|A_0\|, 
\quad 
\beta = \|A_0 f(x_0)\|,
\quad 
\gamma = \alpha \beta \lambda.
$$

Se supone además que $\beta \ne 0$ y que $\gamma < \tfrac{1}{4}$.  
Se denota por $t_0 < 1$ las raíces de la ecuación $\gamma t^2 - t + 1 = 0$  
y se supone que la bola cerrada $B_0$ de centro $x_0$ y radio $\beta t_0$ está contenida en $C$.

Finalmente, denotamos por $g$ la aplicación definida en $\Omega$ por:

$$
g(x) := x - A_0 f(x).
$$

1. Muestre que $\forall x, y \in C$:

   $$
   \|f(x) - f(y) - df(y)(x - y)\| \le \frac{\lambda}{2} \|x - y\|^2.
   $$

   [Sugerencia: Considere $\varphi : [0,1] \to \mathbb{R}^n$ definida por $\varphi(t) := f(y + t(x - y))$ y use el hecho de que $df$ verifica la condición de Lipschitz.]

2. Muestre que $g(B_0) \subset B_0$.  
   [Sugerencia: Use el inciso anterior y la definición de $t_0$.]

3. Muestre que $\forall x, y \in B_0$:

   $$
   \|g(x) - g(y)\| \le \frac{1 - \sqrt{1 - 4\gamma}}{2} \|x - y\|.
   $$

   [Sugerencia: Pruebe que $\|dg(x)\| \le t_0 = \tfrac{1 - \sqrt{1 - 4\gamma}}{2}$.]

4. Muestre que la sucesión $(x_k)$ definida en $B_0$ por:

   $$
   x_{k+1} := x_k - A_0 f(x_k), \qquad k = 0, 1, 2, \dots
   $$

   converge a $x$, la **solución única** de la ecuación $f(x) = 0$ con $x \in B_0$.

5. Muestre que se tiene la siguiente **cota para el error absoluto**:

   $$
   \|x_k - x\| \le \frac{\beta t_0}{1 - \delta} \, \delta^k, 
   \qquad k = 0, 1, 2, \dots
   $$

   donde:
   $$
   \delta := \frac{1 - \sqrt{1 - 4\gamma}}{2}.
   $$

:::

**Solución**

a) La diferencia entre este metodo y el de Newton-Raphson es que el jacobiano se toma como constante (a partir del punto de partida que se tome), de modo que la funcion quedaria de la siguiente manera:

```{r}
#| code-fold: true

newton.kantorovich.vv <- function(p0, F, tol = 1e-8, N = 50){
  x.prev <- as.numeric(p0)
  J <- calcular.jacobiano(F, p0)
  for (i in 1:N) {
    Fx <- evaluar.funcion.vectorial(F, x.prev)
    delta <- try(solve(J, Fx), silent = TRUE)
    if(inherits(delta, "try-error") || any(!is.finite(delta))) {
      stop("Falla al resolver el sistema lineal: jacobiana singular o mal condicionada")
    }
    x.next <- x.prev - as.numeric(delta)
    
    if(abs(max(x.next - x.prev)) < tol){
      return(x.next)
    }
    x.prev <- x.next
  }
  return("Numero maximo de iteraciones excedido")
}

#prueba
r1 <- function(x) x[1]^2 + x[2]^2 + 0.6*x[2] - 0.16
r2 <- function(x) x[1]^2 - x[2]^2 + x[1] -1.6*x[2] - 0.14

R <- list(r1, r2)
p0 <- c(0.6, 0.25)

sol <- newton.kantorovich.vv(p0, R)
sol
```

b)

  1. Defina $\varphi:[0,1]\to\mathbb{R}^n$ por $\varphi(t)=f\big(y+t(x-y)\big)$. Entonces
$\varphi'(t)=df\big(y+t(x-y)\big)(x-y)$ y, por el teorema fundamental del cálculo,

$$
f(x)-f(y)=\int_0^1 df\big(y+t(x-y)\big)(x-y)\,dt.
$$
    Restando $df(y)(x-y)$ y usando Lipschitz para $df$,

\begin{align*}
\lVert f(x)-f(y)-df(y)(x-y)\rVert
&=\left\lVert\int_0^1 \big(df(y+t(x-y))-df(y)\big)(x-y)\,dt\right\rVert \\
&\le \int_0^1 \lambda t \lVert x-y\rVert^2\,dt
= \frac{\lambda}{2}\lVert x-y\rVert^2.
\end{align*}
    $\blacksquare$

  2. Sea $x\in B_0$ y ponga $\Delta=x-x_0$. Por el punto anterior con $y=x_0$,
  $$
  f(x)-f(x_0)-df(x_0)\Delta=:r(x), \qquad \lVert r(x)\rVert\le \frac{\lambda}{2}\lVert\Delta\rVert^2.
  $$
    Entonces
  
  \begin{align*}
  g(x)-x_0
  &= (x-x_0)-A_0\big(f(x)-f(x_0)\big)-A_0f(x_0) \\
  &= \underbrace{\big(I-A_0df(x_0)\big)}_{=\,0}\Delta - A_0 r(x) - A_0 f(x_0),
  \end{align*}
  
    de donde
  $$
  \lVert g(x)-x_0\rVert \le \alpha\,\frac{\lambda}{2}\lVert\Delta\rVert^2 + \beta
  \le \alpha\,\frac{\lambda}{2}(\beta t_0)^2 + \beta
  = \beta\!\left(1+\frac{\gamma}{2}t_0^2\right).
  $$
    Como $t_0$ satisface $\gamma t_0^2-t_0+1=0$, se tiene $t_0=1+\gamma t_0^2$, y por tanto
  $1+\frac{\gamma}{2}t_0^2\le t_0$. Luego
  $$
  \lVert g(x)-x_0\rVert \le \beta t_0,
  $$
    es decir, $g(x)\in B_0$. Por arbitrariedad de $x$, $g(B_0)\subset B_0$. 

    $\blacksquare$

  3. $g$ es una contracción en $B_0$

    Note que $dg(x)=I-A_0df(x)$. Para $x\in B_0$,
$$
\lVert dg(x)\rVert \le \lVert A_0\rVert \,\lVert df(x)-df(x_0)\rVert
\le \alpha\lambda \lVert x-x_0\rVert
\le \alpha\lambda\,\beta t_0=\gamma t_0.
$$
    Con $t_0=\dfrac{1-\sqrt{1-4\gamma}}{2\gamma}$ se obtiene
$$
\sup_{x\in B_0}\lVert dg(x)\rVert \le \gamma t_0
= \frac{1-\sqrt{1-4\gamma}}{2} =: \delta \in (0,1).
$$
    Por el teorema del valor medio en espacios de Banach,
$$
\lVert g(x)-g(y)\rVert \le \delta\,\lVert x-y\rVert
\qquad \forall\,x,y\in B_0,
$$
    que es la cota solicitada:
$$
\lVert g(x)-g(y)\rVert \le \frac{1-\sqrt{1-4\gamma}}{2}\,\lVert x-y\rVert.
$$
    $\blacksquare$

  4. Existencia y unicidad de solución en $B_0$ y convergencia de $x_{k+1}=g(x_k)$

    De (2.) y (3.), $g:B_0\to B_0$ es una contracción con constante $\delta\in(0,1)$. 

    Por el **teorema del punto fijo de Banach**, existe un único $x\in B_0$ tal que $g(x)=x$. 

    Como $A_0$ es invertible, $g(x)=x$ implica $A_0 f(x)=0$, luego $f(x)=0$.

    Si definimos $x_{k+1}=g(x_k)$ con $x_0\in B_0$, entonces $(x_k)$ converge a dicho punto fijo $x$, que es la solución única de $f(x)=0$ en $B_0$. 
    
    $\blacksquare$

  5. Cota para el error absoluto

    Para una contracción con constante $\delta$, vale (estimación a posteriori de Banach)
$$
\lVert x_k-x\rVert \le \frac{\delta^k}{1-\delta}\,\lVert x_1-x_0\rVert
\qquad (k=0,1,2,\dots).
$$
    Aquí $x_1-x_0=g(x_0)-x_0=-A_0 f(x_0)$, por lo que $\lVert x_1-x_0\rVert=\beta$. Además, como $t_0\ge 1$, tenemos $\beta\le \beta t_0$, y por lo tanto
$$
\boxed{\;\lVert x_k-x\rVert \le \frac{\beta t_0}{1-\delta}\,\delta^k,\qquad
\delta=\frac{1-\sqrt{1-4\gamma}}{2}\;}
$$
    que es exactamente la cota pedida. 
    
    $\blacksquare$





# Ejercicio 3

::: {.callout-note title="Instrucción del ejercicio 3"}

Resuelva el siguiente sistema de ecuaciones usando el **método de punto fijo** (pruebe antes que se cumplen las hipótesis del teorema):

$$
\begin{cases}
F_1(x, y) = 2x^2 - xy - 5x + 1 = 0, \\
F_2(x, y) = x + 3\log_{10}x - y^2 = 0.
\end{cases}
$$

:::

**Solución**

Verificación de la hipótesis de convergencia para el método de punto fijo

Sea el sistema de ecuaciones dado por:

$$
\begin{cases}
F_1(x,y) = 2x^2 - xy - 5x + 1 = 0, \\[4pt]
F_2(x,y) = x + 3\log_{10}x - y^2 = 0,
\end{cases}
$$

que puede escribirse en forma de punto fijo mediante las funciones:

$$
\begin{cases}
f_1(x,y) = \sqrt{\dfrac{xy + 5x - 1}{2}}, \\[8pt]
f_2(x,y) = \sqrt{x + 3\log_{10}(x)}.
\end{cases}
$$

De este modo, el mapeo de punto fijo es **G(x,y) = (f₁(x,y), f₂(x,y))**, y su jacobiano viene dado por:

$$
J_G(x,y) =
\begin{pmatrix}
\dfrac{\partial f_1}{\partial x} & \dfrac{\partial f_1}{\partial y} \\[8pt]
\dfrac{\partial f_2}{\partial x} & \dfrac{\partial f_2}{\partial y}
\end{pmatrix}
=
\begin{pmatrix}
\dfrac{y + 5}{4 f_1} & \dfrac{x}{4 f_1} \\[8pt]
\dfrac{1 + \dfrac{3}{x\ln 10}}{2\sqrt{x + 3\log_{10}x}} & 0
\end{pmatrix}.
$$



Región de análisis

Consideremos un rectángulo **D = [3.2, 3.8] × [2.0, 2.6]** que contiene la solución positiva aproximada **(x, y) ≈ (3.487, 2.262)**.

En este dominio, el valor mínimo de f₁ ocurre en (x, y) = (3.2, 2.0):

$$
f_{1,\min} = \sqrt{\dfrac{3.2 \cdot 2.0 + 5 \cdot 3.2 - 1}{2}} = \sqrt{10.7} \approx 3.272.
$$



Cotas de las derivadas parciales

$$
\begin{aligned}
\left|\frac{\partial f_1}{\partial x}\right| &\le \frac{2.6 + 5}{4 \cdot 3.272} = 0.581, \\[4pt]
\left|\frac{\partial f_1}{\partial y}\right| &\le \frac{3.8}{4 \cdot 3.272} = 0.291.
\end{aligned}
$$

Para **∂f₂/∂x**, el máximo ocurre en x = 3.2:

$$
\begin{aligned}
v &= 3.2 + 3\log_{10}(3.2) \approx 4.715, \quad \sqrt{v} \approx 2.172, \\[4pt]
1 + \frac{3}{x \ln 10} &\le 1 + \frac{3}{3.2 \cdot 2.3026} \approx 1.407, \\[4pt]
\Rightarrow \left|\frac{\partial f_2}{\partial x}\right| &\le \frac{1.407}{2 \cdot 2.172} \approx 0.324.
\end{aligned}
$$



Criterio de contracción (Observación 1)

Usando la norma 1 (suma máxima por columnas):

$$
\begin{aligned}
\text{Columna 1: } &\left|\frac{\partial f_1}{\partial x}\right| + \left|\frac{\partial f_2}{\partial x}\right| \le 0.581 + 0.324 = 0.905 < 1, \\[4pt]
\text{Columna 2: } &\left|\frac{\partial f_1}{\partial y}\right| \le 0.291 < 1.
\end{aligned}
$$

También se cumple con la norma infinito (suma máxima por filas):

$$
\begin{aligned}
\text{Fila 1: } &0.581 + 0.291 = 0.872 < 1, \\[4pt]
\text{Fila 2: } &0.324 < 1.
\end{aligned}
$$



Conclusión

En el dominio D se cumple que:

$$
\sup_{(x,y)\in D} \|J_G(x,y)\|_1 < 1 \quad \text{y} \quad \sup_{(x,y)\in D} \|J_G(x,y)\|_\infty < 1.
$$

Por tanto, según la **Observación 1**, el mapeo G(x,y) es una contracción en D, lo cual garantiza que el **método de punto fijo converge localmente** hacia la solución positiva del sistema.


```{r}
#| code-fold: true

f1 <- function(x) sqrt((x[1]*x[2] + 5*x[1] - 1)/(2))
f2 <- function(x) sqrt(x[1] + 3*log10(x[1]))

F <- list(f1, f2)
punto.fijo.vv(c(1,1), F)
```



# Ejercicio 4

::: {.callout-note title="Instrucción del ejercicio 4"}

El sistema no lineal:

$$
\begin{cases}
x_1(1 - x_1) + 4x_2 = 12, \\
(x_1 - 2)^2 + (2x_2 - 3)^2 = 25,
\end{cases}
$$

tiene dos soluciones.

**a)** Aproxime estas dos soluciones gráficamente.

**b)** Use las aproximaciones encontradas en (a) para encontrar las soluciones usando el **método de Newton** y el **método de Newton–Kantorovich**.

:::

**Solución**

a)

```{r}
#| echo: false

# Raíz lineal:
x1_lin <- -1

# Raíces del cúbico con polyroot:
coef_cub <- c(-48, 20, -3, 1)  # -48 + 20 x + (-3) x^2 + 1 x^3
r <- polyroot(coef_cub)

# Quedarse con las reales
x1_cub_real <- Re(r[abs(Im(r)) < 1e-10])

# Juntar raíces reales de todo el polinomio:
x1_solutions <- c(x1_lin, x1_cub_real)

# 2) Recuperar x2 desde la fórmula x2 = 3 + (x1^2 - x1)/4
x2_from_x1 <- function(x1) 3 + (x1^2 - x1)/4
x2_solutions <- x2_from_x1(x1_solutions)

sol <- data.frame(
  x1 = x1_solutions,
  x2 = x2_solutions
)

```


```{r}
#| echo: false
library(ggplot2)

# Curva 1: x2 = 3 + (x1^2 - x1)/4
xgrid <- seq(-3, 6, length.out = 600)
curve1 <- data.frame(
  x1 = xgrid,
  x2 = 3 + (xgrid^2 - xgrid)/4,
  curve = "x2 = 3 + (x1^2 - x1)/4"
)

# Curva 2 (elipse): (x1-2)^2 + 4(x2-1.5)^2 = 25
rhs <- 25 - (xgrid - 2)^2
mask <- rhs >= 0
ell_top <- data.frame(
  x1 = xgrid[mask],
  x2 = 1.5 + 0.5 * sqrt(rhs[mask]),
  curve = "Elipse (arriba)"
)
ell_bot <- data.frame(
  x1 = xgrid[mask],
  x2 = 1.5 - 0.5 * sqrt(rhs[mask]),
  curve = "Elipse (abajo)"
)

# Intersecciones
pts <- data.frame(
  x1 = x1_solutions,
  x2 = x2_solutions
)

ggplot() +
  geom_line(data = curve1, aes(x = x1, y = x2, linetype = curve)) +
  geom_line(data = ell_top, aes(x = x1, y = x2, linetype = curve)) +
  geom_line(data = ell_bot, aes(x = x1, y = x2, linetype = curve)) +
  geom_point(data = pts, aes(x = x1, y = x2), size = 3) +
  geom_text(data = pts,
            aes(x = x1, y = x2, label = sprintf("(%.4f, %.4f)", x1, x2)),
            hjust = -0.1, vjust = -0.8) +
  labs(x = expression(x[1]), y = expression(x[2]),
       title = "Aproximación gráfica de las dos soluciones") +
  theme_minimal()
```


si se grafican ambas ecuaciones se obtiene que las dos posibles soluciones estan en $(-1, 3.5)$ y $(2.5, 4)$

b)

```{r}
#| code-fold: true

f1 <- function(x) x[1]*(1-x[1]) + 4*x[2] - 12
f2 <- function(x) (x[1] - 2)^2 + (2*x[2] - 3)^2 - 25
F <- list(f1, f2)
newton.raphson.vv(c(-1, 3), F)
newton.raphson.vv(c(2.5, 4), F)
newton.kantorovich.vv(c(-1,3), F)
newton.kantorovich.vv(c(2.5, 4), F)
```


# Ejercicio 5

::: {.callout-note title="Instrucción del ejercicio 5"}

Usando el **método de Newton** y el **método de Newton–Kantorovich**, resuelva el sistema:

$$
\begin{cases}
15x_1 + x_2^2 - 4x_3 = 13, \\
x_1^2 + 10x_2 - x_3 = 11, \\
x_2^3 - 25x_3 = -22,
\end{cases}
$$

en 

$$
D := \{(x_1, x_2, x_3)^t \in \mathbb{R}^3 \text{ tal que } 0 \le x_i \le 2 \text{ para } i = 1, 2, 3\}.
$$

:::

**Solución**

```{r}
#| code-fold: true

f1 <- function(x) 15*x[1] + x[2]^2 - 4*x[3] - 13
f2 <- function(x) x[1]^2 + 10*x[2] - x[3] - 11
f3 <- function(x) x[2]^3 -25*x[3] + 22
F <- list(f1, f2, f3)

newton.raphson.vv(c(0, 0, 0), F)
newton.kantorovich.vv(c(0, 0, 0), F)
```

#### 1) Método de Newton 

La iteración es $x^{(k+1)}=x^{(k)}-J(x^{(k)})^{-1}F(x^{(k)})$, es decir, en cada paso se resuelve
$$
J(x^{(k)})\,s^{(k)}=-F(x^{(k)}),\qquad x^{(k+1)}=x^{(k)}+s^{(k)}.
$$

Tomemos $x^{(0)}=(1,1,1)\in D$.

**Paso 0**

- $F(x^{(0)})=(-1,-1,-2)^t$.
- $J(x^{(0)})=
\begin{pmatrix}
15 & 2 &-4\\
2 & 10 & -1\\
0 & 3 & -25 \end{pmatrix}$
- Resolver $J(x^{(0)})s^{(0)}=-F(x^{(0)})=(1,1,2)^t$ (por eliminación de Gauss):
  $$s^{(0)}\approx(0.03664921, 0.08569854, -0.06971618)^t.$$
- Actualizar:
  $$x^{(1)}=x^{(0)}+s^{(0)}\approx(1.03664921, 1.08569854, 0.93028382).$$

**Paso 1**

- $F(x^{(1)})\approx(0.00734424, 0.00134316, 0.02266211)^t$.
- $J(x^{(1)})=
\begin{pmatrix}
15&2x_2^{(1)}&-4\\
2x_1^{(1)}&10&-1\\
0&3(x_2^{(1)})^2&-25
\end{pmatrix}
\approx
\begin{pmatrix}
15&2.17139708&-4\\
2.07329842&10&-1\\
0&3.536928&-25
\end{pmatrix}.
$
- Resolver $J(x^{(1)})s^{(1)}=-F(x^{(1)})$:
  $$s^{(1)}\approx(-2.48745\times10^{-4}, 8.01762\times10^{-6}, 9.07618\times10^{-4}).$$
- Actualizar:
  $$x^{(2)}=x^{(1)}+s^{(1)}\approx(1.03640047, 1.08570656, 0.93119144).$$

Con un paso más, $F(x^{(2)})$ queda del orden de $10^{-8}$ y la iteración ya está en la solución (precisión a $\sim 10^{-8}$).

**Solución (en $D$):**
$$
\boxed{x^*\approx(1.036400470329, 1.085706550742, 0.931191442315)}.
$$



#### 2) Verificación de Newton–Kantoróvich en $D$

Usamos el marco estándar con $A_0=J(x^{(0)})^{-1}$, $\alpha=\lVert A_0\rVert$, $\beta=\lVert A_0F(x^{(0)})\rVert=\lVert s^{(0)}\rVert$ y $\gamma=\alpha\beta\lambda$, donde $\lambda$ es una constante de Lipschitz para $J$ en $D$.

La hessiana (derivadas segundas) no nulas son:
- De $F_1$: $\partial^2F_1/\partial x_2^2=2$.
- De $F_2$: $\partial^2F_2/\partial x_1^2=2$.
- De $F_3$: $\partial^2F_3/\partial x_2^2=6x_2\le 12$ en $D$.

Con la norma de Frobenius, un cota válida es
$$
\lVert J(x)-J(y)\rVert\le \sqrt{(2\lvert x_1-y_1\rvert)^2+(2\lvert x_2-y_2\rvert)^2+(12\lvert x_2-y_2\rvert)^2}
\le \sqrt{148}\,\lVert x-y\rVert.
$$
Tomamos $\lambda=\sqrt{148}\approx 12.165$.

- $A_0=J(x^{(0)})^{-1}$ $\Rightarrow$ $\alpha=\lVert A_0\rVert\approx 0.1078875$.
- $\beta=\lVert s^{(0)}\rVert\approx 0.1163948$.
- $\gamma=\alpha\beta\lambda\approx 0.152769<\tfrac{1}{4}$.

Entonces
$$
t_0=\frac{1-\sqrt{1-4\gamma}}{2\gamma}\approx 1.231802,\qquad
\delta=\frac{1-\sqrt{1-4\gamma}}{2}\approx 0.188181.
$$
El radio garantizado es $\beta t_0\approx 0.143375$.

Como $\lVert x^*-x^{(0)}\rVert\approx 0.115781<0.143375$, la solución pertenece a la bola $B(x^{(0)},\beta t_0)\subset D$ y es **única** allí. Además, Newton con $x^{(0)}$ **converge** y vale la cota de error
$$
\lVert x^{(k)}-x^*\rVert\le \frac{\beta t_0}{1-\delta}\,\delta^k
\approx 0.17661\,\delta^k.
$$

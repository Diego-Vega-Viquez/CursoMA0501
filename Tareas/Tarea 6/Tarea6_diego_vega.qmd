---
title: "MA0501 – Tarea 6"
author: 
  - name: "Diego Alberto Vega Víquez - C38367" 
    email: "diegovv13@gmail.com"
  - name: "José Carlos Quintero Cedeño - C26152" 
    email: "jose.quinterocedeno@ucr.ac.cr"
  - name: "Gabriel Valverde Guzmán - C38060"
    email: "GABRIEL.VALVERDEGUZMAN@ucr.ac.cr"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 1
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    include-in-header: 
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    toc: true
    toc-depth: 1
    toc-location: left
    toc_float: yes
    html-math-method: katex
    css: styles.css
    df_print: paged
    theme: flatly
    highlight: tango
    embed-resources: true
    page-layout: full
editor: 
  markdown: 
    wrap: 72
---

\newpage

# Ejercicio 1

::: {.callout-note title="Instrucción del ejercicio 1"}

Implemente en **R** funciones para todos los algoritmos numéricos de resolución de ecuaciones diferenciales vistos en clase.

:::

**Solución**

*(Implementación en R de los métodos numéricos básicos: Euler, Euler modificado, Heun, Runge–Kutta de orden 4, etc.)*



# Ejercicio 2

::: {.callout-note title="Instrucción del ejercicio 2"}

Complete los detalles de las demostraciones que quedaron pendientes en este capítulo.

:::

**Solución**

::: {.callout-note title="Lema 3"}

Sea $(\xi_j)$ una sucesión en $\mathbb{R}$ con la propiedad

$$
|\xi_{j+1}| \le (1+A)\,|\xi_j| + B, \qquad j=0,1,2,\ldots,
$$

para algunas constantes $A>0$ y $B\ge 0$. Entonces, para todo $j\ge 0$ se cumple

$$
|\xi_j| \le |\xi_0|\,e^{jA} + \frac{B}{A}\big(e^{jA}-1\big).
$$

::: {.callout-caution collapse="true" title="Prueba"}

Sea $a_j := |\xi_j| \ge 0$. La hipótesis dice
$$
a_{j+1} \le (1+A)\,a_j + B, \qquad j\ge 0.
$$

**Paso 1 (desenrollando la recurrencia).**  
Probamos por inducción que para todo $j\ge 0$,
$$
a_j \le (1+A)^j a_0 + B\sum_{k=0}^{j-1}(1+A)^k. \tag{1}
$$

- Para $j=0$, la afirmación es trivial:
  $a_0 \le (1+A)^0 a_0 + 0$.

- Suponga que vale para $j$. Entonces, usando la recurrencia,
  $$
  \begin{aligned}
  a_{j+1}
  &\le (1+A)\,a_j + B \\
  &\le (1+A)\Big[(1+A)^j a_0 + B\sum_{k=0}^{j-1}(1+A)^k\Big] + B \\
  &= (1+A)^{j+1} a_0 + B\sum_{k=1}^{j}(1+A)^k + B \\
  &= (1+A)^{j+1} a_0 + B\sum_{k=0}^{j}(1+A)^k,
  \end{aligned}
  $$
  que es la fórmula (1) con $j$ reemplazado por $j+1$. Queda probado por inducción.

De (1) y la suma geométrica,
$$
\sum_{k=0}^{j-1}(1+A)^k=\frac{(1+A)^j-1}{A},
$$
obtenemos
$$
a_j \le (1+A)^j a_0 + \frac{B}{A}\big((1+A)^j-1\big). \tag{2}
$$

**Paso 2 (paso de $(1+A)^j$ a $e^{Aj}$).**  
Usando que $(1+A)^j \le e^{Aj}$ para $A>0$ (por $1+x\le e^x$), en (2) se concluye
$$
a_j \le a_0\,e^{Aj} + \frac{B}{A}\big(e^{Aj}-1\big).
$$

Recordando que $a_j=|\xi_j|$ y $a_0=|\xi_0|$, queda
$$
|\xi_j| \le |\xi_0|\,e^{jA} + \frac{B}{A}\big(e^{jA}-1\big), \qquad j=0,1,2,\ldots
$$
como se quería. $\blacksquare$

:::

:::


::: {.callout-note title="Teorema 6"}

Si la función $\varphi$ en un método de un paso es continua (con respecto a $h$) y satisface la **condición de Lipschitz**

$$
|\varphi(x,u;h) - \varphi(x,v;h)| \le M\,|u - v|,
$$

para todo $(x,u),(x,v)\in G$ y para $h$ suficientemente pequeño,  
entonces **un método de un paso es convergente si y solo si el método es consistente.**

::: {.callout-caution collapse="true" title="Prueba"}

**1. (Consistencia $\Rightarrow$ Convergencia)**

Sea $e_j = u_j - u(x_j)$ el error global en el nodo $x_j$.  
Se tiene que:

$$
\begin{aligned}
e_{j+1} - e_j
&= [u_{j+1} - u_j] - [u(x_{j+1}) - u(x_j)] \\
&= h\,\varphi(x_j,u_j;h) - [u(x_{j+1}) - u(x_j)] \\
&= h\,[\,\varphi(x_j,u_j;h) - \varphi(x_j,u(x_j);h) - \Delta(x_j,u(x_j);h)\,].
\end{aligned}
$$

Por la condición de Lipschitz, se obtiene:

$$
|e_{j+1} - e_j| \le h\,[\,M\,|u_j - u(x_j)| + c(h)\,], \tag{1.14}
$$

donde
$$
c(h) := \max_{a \le x \le b} |\Delta(x,u(x);h)|.
$$

Nótese que $c(h)\to 0$ cuando $h\to 0$, pues el método es consistente.

**2. (Aplicación del Lema 3)**

Como $e_j = u_j - u(x_j)$, la desigualdad (1.14) implica que:

$$
|e_{j+1}| \le (1 + hM)\,|e_j| + h\,c(h), \qquad j = 0,1,2,\ldots,n.
$$

Aplicando el **Lema 3** con $A = hM$, $B = h\,c(h)$ y observando que $e_0 = 0$, se deduce:

$$
|e_j| \le \frac{h\,c(h)}{hM}\,\big(e^{j hM} - 1\big), \qquad j=0,1,2,\ldots,n.
$$

Como $x_j = x_0 + jh$, esto se puede reescribir como:

$$
|e_j| \le \frac{c(h)}{M}\,\big(e^{M(x_j - x_0)} - 1\big), \qquad j=0,1,2,\ldots,n. \tag{1.15}
$$

Por tanto, el **error global máximo** cumple:

$$
E(h) = \max_j |e_j|
\le \frac{c(h)}{M}\,\big(e^{M(b-a)} - 1\big)
\longrightarrow 0 \quad \text{cuando } h\to 0.
$$

Esto prueba que el método es **convergente**.



**3. (Convergencia $\Rightarrow$ Consistencia)**


Recordemos (Def. 7) que el **error de discretización local** en un punto $(x,u)$ se
define por
$$
\Delta(x,u;h)=\frac{\eta(x+h)-\eta(x)}{h}-\varphi(x,u;h),
$$
donde $\eta$ es la solución del problema auxiliar
$\eta' = f(\xi,\eta)$ con condición inicial $\eta(x)=u$.
Además, por el Teorema 3, un método de un paso es **consistente**
si y sólo si
$$
\lim_{h\to0}\varphi(x,u;h)=f(x,u)
\qquad(\text{uniformemente en }(x,u)\in G).
$$

Sea ahora un método de un paso **convergente** en el sentido de la Def. 10.
Fijemos $(x,u)\in G$ y apliquemos el método sobre el intervalo corto
$[x,x+h]$ partiendo exactamente de $u(x)=u$.
Denotemos por
$$
U_1 := u + h\,\varphi(x,u;h)
$$
la aproximación numérica al valor exacto $\eta(x+h)$ tras un solo paso.
El error en ese único nodo es
$$
e_1 \;=\; U_1-\eta(x+h)
      \;=\; h\Big(\varphi(x,u;h)-\frac{\eta(x+h)-\eta(x)}{h}\Big)
      \;=\; -\,h\,\Delta(x,u;h).                                  \tag{1}
$$

Como el método es convergente, aplicado al problema en $[x,x+h]$
(se trata de un problema del mismo tipo pero en un intervalo de longitud $h$),
se tiene que el error global máximo en ese intervalo, que aquí coincide con
$|e_1|$, satisface
$$
|e_1| \longrightarrow 0 \qquad \text{cuando } h\to0 .
$$

Dividiendo (1) entre $h$ obtenemos
$$
|\Delta(x,u;h)| \;=\; \frac{|e_1|}{h}.
$$

Usando que $\varphi$ es Lipschitz en la segunda variable y continua en $h$
y que $f$ es localmente acotada, el error tras un paso satisface
la cota estándar (obtenida con Grönwall)
$$
|e_1| \;\le\; C\,h \qquad \text{para $h$ suficientemente pequeño,}
$$
con $C$ independiente de $h$ y de $(x,u)$ en compactos de $G$.
Por lo tanto
$$
|\Delta(x,u;h)|
 \;=\; \frac{|e_1|}{h}
 \;\le\; C \qquad\text{y, además,}\qquad
\lim_{h\to0} |\Delta(x,u;h)|=\lim_{h\to0}\frac{|e_1|}{h}=0,
$$
ya que $|e_1|=o(h)$ cuando $h\to0$ (convergencia en un paso).

Con ello se concluye que
$$
\lim_{h\to0}\Delta(x,u;h)=0
\quad\Longleftrightarrow\quad
\lim_{h\to0}\varphi(x,u;h)=\lim_{h\to0}\frac{\eta(x+h)-\eta(x)}{h}
= \eta'(x)=f(x,u),
$$
uniformemente en $(x,u)\in G$. Es decir, el método es **consistente**.

$\blacksquare$

:::

:::


::: {.callout-note title="Corolario 2"}

1. El **método de Euler** es convergente y, si $f$ es continuamente diferenciable, entonces el **orden de convergencia** es $1$.

2. El **método de Euler modificado** (o de Heun) es convergente y, si $f$ es dos veces continuamente diferenciable, entonces el **orden de convergencia** es $2$.

::: {.callout-caution collapse="true" title="Prueba"}

**Para 1. (Euler clásico).**  
El método de Euler tiene
$$
\varphi(x,u;h)=f(x,u).
$$

- **Lipschitz en $u$:** Si $f$ es Lipschitz en su segunda variable con constante $L$ (uniforme en $x$), entonces
$$
|\varphi(x,u;h)-\varphi(x,v;h)|=|f(x,u)-f(x,v)|\le L\,|u-v|.
$$

- **Consistencia y orden:** Por el **Teorema 4** (ya demostrado), Euler es consistente y, si $f$ es $C^1$, su **orden de consistencia es $1$**.

Aplicando el **Teorema 6** (consistencia $\Longleftrightarrow$ convergencia bajo Lipschitz) y el **Teorema 7** (el orden de convergencia coincide con el orden de consistencia), se concluye que Euler **converge con orden $1$**.

**Para 2. (Euler modificado / Heun).**  
En este caso
$$
\varphi(x,u;h)=\frac12\Big[f(x,u)+f\!\big(x+h,\,u+h\,f(x,u)\big)\Big].
$$

- **Lipschitz en $u$:** Suponga que $f$ es Lipschitz en su segunda variable con constante $L$ (uniforme en $x$). Entonces, para $u,v$ cualesquiera,

\begin{align*}
|\varphi(x,u;h)-\varphi(x,v;h)|
&\le \tfrac12\,|f(x,u)-f(x,v)|
   +\tfrac12\,\Big|f\!\big(x+h,\,u+h f(x,u)\big)-f\!\big(x+h,\,v+h f(x,v)\big)\Big| \\
&\le \tfrac12\,L|u-v|
   +\tfrac12\,L\,\big|[u-v]+h\,[f(x,u)-f(x,v)]\big| \\
&\le \tfrac12\,L|u-v|
   +\tfrac12\,L\,(|u-v|+h\,L|u-v|) \\
&= L\Big(1+\tfrac12\,hL\Big)\,|u-v|.
\end{align*}

Así, $\varphi$ es Lipschitz en $u$ con constante $M=L(1+\tfrac12 hL)$ (válida para $h$ pequeño).

- **Consistencia y orden:** Por el resultado demostrado para Euler mejorado, si $f\in C^2$ entonces el método es **consistente de orden $2$**.

Aplicando de nuevo el **Teorema 6** (bajo Lipschitz hay equivalencia entre consistencia y convergencia) y el **Teorema 7** (el orden de convergencia coincide con el de consistencia), se concluye que el método de Euler modificado **converge con orden $2$**.

$\blacksquare$

:::

:::


::: {.callout-note title="Definición 11"}

El método de **Runge–Kutta de cuarto orden (RK4)** para resolver el problema de valor inicial:

$$
\begin{cases}
u' = f(x, u), \\
u(x_0) = u_0,
\end{cases}
$$

construye una aproximación $u_j$ de $u(x_j)$ en la malla de puntos equidistantes  
$x_j := x_0 + jh$ para $j = 1, 2, \ldots$, usando las siguientes ecuaciones:

\begin{align*}
k_1 &= f(x_j, u_j), \\
k_2 &= f\!\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_1\right), \\
k_3 &= f\!\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_2\right), \\
k_4 &= f(x_j + h,\, u_j + h k_3), \\
u_{j+1} &= u_j + \frac{h}{6}\,(k_1 + 2k_2 + 2k_3 + k_4).
\end{align*}

::: {.callout-important collapse="true" title="Observaciones"}

1. El método fue introducido por **Runge** en 1895 y extendido por **Kutta** en 1901 para sistemas de ecuaciones diferenciales.

2. Si $u' = f(x)$, el método de Runge–Kutta y la **regla de Simpson** son equivalentes (pruébelo).


::: {.callout-caution collapse="true" title="Prueba"}

Sea $u' = f(x)$, es decir, $u(x)$ es una primitiva de $f(x)$. Entonces, la solución exacta cumple:
$$
u(x_{j+1}) = u(x_j) + \int_{x_j}^{x_{j+1}} f(x) \, dx.
$$

El método de Runge–Kutta de orden 4 estima $u_{j+1}$ mediante:
\begin{align*}
k_1 &= f(x_j), \\
k_2 &= f\left(x_j + \frac{h}{2} \right), \\
k_3 &= f\left(x_j + \frac{h}{2} \right), \\
k_4 &= f(x_j + h), \\
u_{j+1} &= u_j + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).
\end{align*}

Observamos que como $u' = f(x)$, la evaluación de $f$ no depende de $u_j$, por lo tanto:
$$
k_2 = k_3 = f\left(x_j + \frac{h}{2} \right).
$$

Sustituyendo:
\begin{align*}
u_{j+1} &= u_j + \frac{h}{6} \left[ f(x_j) + 2f\left(x_j + \frac{h}{2} \right) + 2f\left(x_j + \frac{h}{2} \right) + f(x_j + h) \right] \\
&= u_j + \frac{h}{6} \left[ f(x_j) + 4f\left(x_j + \frac{h}{2} \right) + f(x_j + h) \right].
\end{align*}

Esto coincide exactamente con la **regla de Simpson** para la integral de $f$ en el intervalo $[x_j, x_{j+1}]$, que aproxima:
$$
\int_{x_j}^{x_{j+1}} f(x)\,dx \approx \frac{h}{6}\left[f(x_j) + 4f\left(x_j + \frac{h}{2}\right) + f(x_j + h)\right].
$$

Luego:
$$
u_{j+1} = u_j + \int_{x_j}^{x_{j+1}} f(x)\,dx \approx u_j + \text{(Simpson)} = \text{RK4}.
$$

:::

3. Resumiendo, el método de **Runge–Kutta** se puede escribir como:

$$
\begin{cases}
w_0 = \alpha, \\
w_{i+1} = w_i + \dfrac{1}{6}\,(k_1 + 2k_2 + 2k_3 + k_4),
\end{cases}
$$

donde:

\begin{align*}
k_1 &= h f(t_i, w_i), \\[0.5em]
k_2 &= h f\!\left(t_i + \frac{h}{2},\, w_i + \frac{k_1}{2}\right), \\[0.5em]
k_3 &= h f\!\left(t_i + \frac{h}{2},\, w_i + \frac{k_2}{2}\right), \\[0.5em]
k_4 &= h f(t_i + h,\, w_i + k_3).
\end{align*}

:::

:::


# Ejercicio 3

::: {.callout-note title="Instrucción del ejercicio 3"}

Para los problemas de valor inicial:

$$
y' = -2ty^2, \quad y(0) = 1, \quad 0 \le t \le 1.
$$

$$
y' - y = \cos(t), \quad y(0) = \tfrac{1}{2}, \quad 0 \le t \le 1.
$$

$$
x\sqrt{1 - y^2}\,dx + y\sqrt{1 - x^2}\,dy = 0, \quad y(0) = 1, \quad 0 \le x \le 1.
$$

(a) Genere una tabla con cada uno de los métodos del ejercicio anterior con $N = 10$.  

(b) Encuentre la solución exacta usando **R**.  

(c) Mediante alguno de los métodos de interpolación vistos en el curso, interpole cada una de las soluciones obtenidas. Luego grafique en un mismo plano la solución exacta y el polinomio obtenido.  

(d) ¿Cuál de los métodos permitió obtener una mejor solución aproximada?

:::

**Solución**

*(Implementación de los tres problemas en R, aplicación de los métodos numéricos de Euler, Heun y Runge–Kutta, comparación con la solución exacta e interpolación de resultados.)*



# Ejercicio 4

::: {.callout-note title="Instrucción del ejercicio 4"}

Muestre que el método de **Euler** falla al aproximar la solución

$$
u(x) = \left(\frac{2}{3}x\right)^{3/2}
$$

para el problema de valor inicial

$$
u' = u^{1/3}, \quad u(0) = 0.
$$

Explique por qué falla.

:::

**Solución**

*(Análisis de la discontinuidad del término $u^{1/3}$ en $u=0$, que causa que el método de Euler no satisfaga la hipótesis de Lipschitz y por tanto diverja.)*



# Ejercicio 5

::: {.callout-note title="Instrucción del ejercicio 5"}

Demuestre que el método de un paso dado por:

$$
u_{j+1} = u_j + h f\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2} f(x_j, u_j)\right)
$$

es consistente y que, si $f$ es dos veces continuamente diferenciable, entonces tiene **orden dos**  
(este método se conoce como el **método de Euler Modificado**).  

Implemente este método en **R** y resuelva las ecuaciones diferenciales del problema 2. Luego grafique.

:::

**Solución**

*(Demostración de la consistencia mediante expansión en serie de Taylor y verificación del error local de orden $O(h^3)$. Implementación en R y comparación gráfica con otros métodos.)*



# Ejercicio 6

::: {.callout-note title="Instrucción del ejercicio 6"}

Demuestre que el método de un paso dado por:

$$
k_1 = f(x_j, u_j),
$$
$$
k_2 = f\left(x_j + \frac{h}{3},\, u_j + \frac{h}{3} k_1\right),
$$
$$
k_3 = f\left(x_j + \frac{2h}{3},\, u_j + \frac{2h}{3} k_2\right),
$$
$$
u_{j+1} = u_j + \frac{h}{4}(k_1 + 3k_3),
$$

es consistente y que, si $f$ es tres veces continuamente diferenciable, entonces tiene **orden tres**  
(este método se conoce como el **método de Tercer Orden de Heun**).  

Implemente este método en **R** y resuelva las ecuaciones diferenciales del problema 2. Luego grafique.

:::

**Solución**

*(Demostración de la consistencia mediante comparación con la expansión de Taylor hasta términos de $O(h^4)$, y verificación del orden 3. Implementación en R y análisis de error global con respecto a métodos de orden inferior.)*

# Ejercicio 7

::: {.callout-note title="Instrucción del ejercicio 7"}

Demuestre que el método de un paso dado por:

$$
k_1 = f(x_j, u_j),
$$
$$
k_2 = f\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_1\right),
$$
$$
k_3 = f\left(x_j + h,\, u_j - hk_1 + 2hk_2\right),
$$
$$
u_{j+1} = u_j + \frac{h}{6}(k_1 + 4k_2 + k_3),
$$

es consistente y que si $f$ es tres veces continuamente diferenciable entonces tiene **orden tres**  
(este método se conoce como el **método de Tercer Orden de Runge–Kutta**).  

Implemente este método en **R** y resuelva las ecuaciones diferenciales del problema 2, luego grafique.

:::

**Solución**

*(Demostración del orden del método mediante expansión en series de Taylor, comparación con la solución exacta y verificación de la cancelación de términos hasta $O(h^4)$. Implementación en R y análisis gráfico de la convergencia frente a métodos de orden menor.)*



# Ejercicio 8

::: {.callout-note title="Instrucción del ejercicio 8"}

La idea de este ejercicio es introducir la **matriz exponencial** $e^A$ para resolver sistemas de ecuaciones diferenciales.

**Definición:**  
Dada una sucesión $\{C_k\}$ de matrices $m \times n$ cuyos elementos son reales o complejos, se denota por $c_{ij}^k$ la entrada $ij$ de $C_k$.  
Entonces, si todas las $mn$ series

$$
\sum\limits_{k=1}^{\infty} c_{ij}^k \quad \text{con } i = 1, 2, \ldots, m; \; j = 1, 2, \ldots, n
$$

son convergentes, se dice que la serie de matrices $\sum\limits_{k=1}^{\infty} C_k$ es convergente y su suma es la matriz cuya entrada $ij$ es:

$$
\sum\limits_{k=1}^{\infty} c_{ij}^k.
$$

(a) Pruebe que si $\{C_k\}$ es una sucesión de matrices $m \times n$ tales que  
$\sum\limits_{k=1}^{\infty} \|C_k\|$ converge, entonces la serie de matrices $\sum\limits_{k=1}^{\infty} C_k$ también es convergente.

(b) Pruebe que la serie $\sum\limits_{k=0}^{\infty} \frac{A^k}{k!}$ es convergente.

::: {.callout-note title="Definición"}
Dada una matriz $A$, $n \times n$ con elementos reales o complejos, se define la **matriz exponencial** como:

$$
e^A = \sum\limits_{k=0}^{\infty} \frac{A^k}{k!}.
$$
:::

(c) Verifique que si 

$$
A = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 3
\end{pmatrix}
$$

  entonces 

$$
e^A =
\begin{pmatrix}
e^t & 0 & 0 \\
0 & e^{2t} & 0 \\
0 & 0 & e^{3t}
\end{pmatrix}.
$$

(e) Verifique que si 

$$
A = 
\begin{pmatrix}
a & 1 \\
0 & a
\end{pmatrix}
$$

  entonces 

$$
e^{tA} =
\begin{pmatrix}
e^{at} & te^{at} \\
0 & e^{at}
\end{pmatrix}.
$$

(f) Pruebe que para todo $t \in \mathbb{R}$ la función matricial $E(t) = e^{tA}$ satisface la ecuación diferencial matricial:

$$
E'(t) = E(t)A = AE(t).
$$

(g) Dada una matriz $A$, $n \times n$ con elementos reales o complejos, pruebe que para todo $t \in \mathbb{R}$ se cumple:

$$
e^{tA} e^{-tA} = I
$$

  o sea que $e^{tA}$ es no singular y su inversa es $e^{-tA}$.


(h) Pruebe que si $A$ y $B$ son dos matrices $n \times n$ con elementos reales o complejos tales que $AB = BA$,  
entonces:

$$
e^{A+B} = e^A e^B.
$$

(i) Pruebe que, dada una matriz $A$, $n \times n$ con elementos reales o complejos y un vector $B$ de $n$ entradas, entonces el problema de valor inicial:

$$
Y'(t) = AY(t)
$$
$$
Y(0) = B
$$

  tiene solución única con $t \in \mathbb{R}$ y que esta solución está dada por:

$$
Y(t) = e^{tA} B.
$$

(j) Pruebe que si una matriz cuadrada $A$ es diagonalizable, es decir, que existe una matriz $C$ no singular tal que la matriz $D = C^{-1}AC$ es diagonal, entonces:

$$
e^{tA} = C e^{tD} C^{-1}.
$$

(k) Escriba una función en **R** que reciba una matriz $A$ diagonalizable y retorne $e^{tA}$.  
Para esto puede usar la función `eigen(...)` de R.

(l) Verifique que si

$$
A =
\begin{pmatrix}
3 & -1 \\
-2 & 2
\end{pmatrix}
$$

  entonces

$$
e^{tA} = -\frac{1}{3}
\begin{pmatrix}
-e^t - 2e^{4t} & -e^t + e^{4t} \\
-2e^t + 2e^{4t} & -2e^t - e^{4t}
\end{pmatrix}.
$$

(m) Resuelva el siguiente sistema de ecuaciones diferenciales:

$$
\begin{cases}
x_1'(t) = 3x_1(t) - x_2(t) \\
x_2'(t) = -2x_1(t) + 2x_2(t)
\end{cases}
$$

  sujeto a $x_1(0) = 90$ y $x_2(0) = 150$.

(n) Escriba una función en **R** que reciba una matriz $A$ con los coeficientes de un sistema de ecuaciones diferenciales, las condiciones iniciales en una lista de pares, y retorne una lista con las soluciones del sistema de ecuaciones diferenciales.

(o) Usando el programa, resuelva el siguiente sistema de ecuaciones diferenciales:

$$
\begin{cases}
x_1'(t) = 3x_1(t) - 2x_2(t) \\
x_2'(t) = -2x_1(t) + 3x_2(t) \\
x_3'(t) = 5x_3(t)
\end{cases}
$$

  sujeto a $x_1(0) = 2$, $x_2(0) = 1$ y $x_3(0) = 3$.

:::

**Solución**

*(Prueba de la propiedad de conmutatividad para exponentiales de matrices mediante expansión de series de potencias; demostración de la existencia y unicidad de la solución $Y(t) = e^{tA}B$ aplicando el teorema de Picard–Lindelöf; derivación de la forma general de $e^{tA}$ para matrices diagonalizables; implementación en R usando `eigen()` y validación con ejemplos concretos. Finalmente, resolución de los sistemas de ecuaciones diferenciales con $e^{tA}$ y graficación de las trayectorias de las soluciones.)*

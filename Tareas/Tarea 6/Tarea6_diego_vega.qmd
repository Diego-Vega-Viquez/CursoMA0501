---
title: "MA0501 – Tarea 6"
author: 
  - name: "Diego Alberto Vega Víquez - C38367" 
    email: "diegovv13@gmail.com"
  - name: "José Carlos Quintero Cedeño - C26152" 
    email: "jose.quinterocedeno@ucr.ac.cr"
  - name: "Gabriel Valverde Guzmán - C38060"
    email: "GABRIEL.VALVERDEGUZMAN@ucr.ac.cr"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 1
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    include-in-header: 
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    toc: true
    toc-depth: 1
    toc-location: left
    toc_float: yes
    html-math-method: katex
    css: styles.css
    df_print: paged
    theme: flatly
    highlight: tango
    embed-resources: true
    page-layout: full
editor: 
  markdown: 
    wrap: 72
---

\newpage

```{r}
#| include: false

library(tidyverse)
neville <- function(nodos, valores, x) {
  stopifnot(is.numeric(nodos),
            is.numeric(valores),
            length(nodos) == length(valores))
  n <- length(nodos)
  Q <- matrix(NA_real_, nrow = n, ncol = n)
  
  # Columna inicial con valores de Y
  Q[, 1] <-  valores
  
  # Construccion de la tabla de Neville
  for (i in 2:n) {
    for (j in 2:i) {
      numerador <- ((x - nodos[i - j + 1]) * Q[i, j - 1] - (x - nodos[i]) * Q[i -
                                                                                1, j - 1])
      denominador <- nodos[i] - nodos[i - j + 1]
      Q[i, j] <-  numerador / denominador
    }
  }
  return(list(valor = Q[n, n], tabla = Q))
}


graficar.polinomio <- function(nodos1,
                               a,
                               b,
                               metodo1,
                               f1 = NULL,
                               valores1 = NULL,
                               nodos2 = NULL,
                               metodo2 = metodo1,
                               f2 = NULL,
                               valores2 = NULL,
                               f.referencia = NULL) {
  stopifnot(is.numeric(nodos1), length(nodos1) >= 2)

  if (is.null(f1) == is.null(valores1)) {
    stop("Para el primer conjunto debe proveer exactamente uno: 'f1' o 'valores1'.")
  }

  tiene.segundo <- !is.null(nodos2)

  if (tiene.segundo) {
    stopifnot(is.numeric(nodos2), length(nodos2) >= 2)
    if (is.null(f2) == is.null(valores2)) {
      stop("Para el segundo conjunto debe proveer exactamente uno: 'f2' o 'valores2'.")
    }
  }

  if (!is.null(f1)) {
    stopifnot(is.function(f1))
    valores_nodos_1 <- f1(nodos1)
  } else {
    stopifnot(is.numeric(valores1), length(valores1) == length(nodos1))
    valores_nodos_1 <- valores1
  }

  if (tiene.segundo) {
    if (!is.null(f2)) {
      stopifnot(is.function(f2))
      valores_nodos_2 <- f2(nodos2)
    } else {
      stopifnot(is.numeric(valores2), length(valores2) == length(nodos2))
      valores_nodos_2 <- valores2
    }
  } else {
    valores_nodos_2 <- NULL
  }

  H1 <- function(x) {
    vapply(
      x,
      function(xx)
        metodo1(nodos1, valores_nodos_1, xx)$valor,
      numeric(1)
    )
  }

  if (tiene.segundo) {
    H2 <- function(x) {
      vapply(
        x,
        function(xx)
          metodo2(nodos2, valores_nodos_2, xx)$valor,
        numeric(1)
      )
    }
  } else {
    H2 <- NULL
  }

  xi <- seq(a, b, length.out = 400)

  fx1.col <- if (!is.null(f1)) f1(xi) else NA_real_

  if (tiene.segundo) {
    fx2.col <- if (!is.null(f2)) f2(xi) else NA_real_
  } else {
    fx2.col <- NA_real_
  }

  fx.ref.col <- if (!is.null(f.referencia)) {
    stopifnot(is.function(f.referencia))
    f.referencia(xi)
  } else {
    NA_real_
  }

  df_plot <- data.frame(
    x        = xi,
    H1x      = H1(xi),
    H2x      = if (tiene.segundo) H2(xi) else NA_real_,
    fx1      = fx1.col,
    fx2      = fx2.col,
    fx.ref   = fx.ref.col
  )

  df_nodos_1 <- data.frame(x = nodos1, y = valores_nodos_1)

  if (tiene.segundo) {
    df_nodos_2 <- data.frame(x = nodos2, y = valores_nodos_2)
  }

  p <- ggplot(df_plot, aes(x = x))
  
  if (!is.null(f.referencia)) {
    p <- p +
      geom_line(
        aes(y = fx.ref,
            color = "Referencia",
            linetype = "Referencia"),
        linewidth = 1
      )
  }

  if (!is.null(f1)) {
    p <- p +
      geom_line(
        aes(y = fx1,
            color = "Original 1",
            linetype = "Original 1"),
        linewidth = 1
      )
  }

  if (tiene.segundo && !is.null(f2)) {
    p <- p +
      geom_line(
        aes(y = fx2,
            color = "Original 2",
            linetype = "Original 2"),
        linewidth = 1
      )
  }

  p <- p +
    geom_line(
      aes(y = H1x,
          color = "Interpolación 1",
          linetype = "Interpolación 1"),
      linewidth = 1
    )

  if (tiene.segundo) {
    p <- p +
      geom_line(
        aes(y = H2x,
            color = "Interpolación 2",
            linetype = "Interpolación 2"),
        linewidth = 1
      )
  }

  p <- p +
    geom_point(
      data = df_nodos_1,
      aes(x = x, y = y, fill = "Nodos 1"),
      shape = 21,
      size = 3,
      stroke = .6,
      color = "black"
    )

  if (tiene.segundo) {
    p <- p +
      geom_point(
        data = df_nodos_2,
        aes(x = x, y = y, fill = "Nodos 2"),
        shape = 21,
        size = 3,
        stroke = .6,
        color = "black"
      )
  }

  p <- p +
    scale_color_manual(
      values = c(
        "Original 1"        = "blue",
        "Original 2"        = "darkgreen",
        "Interpolación 1"   = "red",
        "Interpolación 2"   = "purple",
        "Referencia"        = "orange"
      ),
      name = "Serie"
    ) +
    scale_linetype_manual(
      values = c(
        "Original 1"        = "solid",
        "Original 2"        = "solid",
        "Interpolación 1"   = "dashed",
        "Interpolación 2"   = "dashed",
        "Referencia"        = "solid"
      ),
      guide = "none"
    ) +
    scale_fill_manual(
      values = c(
        "Nodos 1" = "white",
        "Nodos 2" = "grey70"
      ),
      name = "Nodos"
    ) +
    labs(
      title = if (tiene.segundo) {
        paste0(
          "Interpolación 1: ",
          deparse(substitute(metodo1)),
          " | Interpolación 2: ",
          deparse(substitute(metodo2))
        )
      } else {
        paste0(
          "Interpolación por ",
          deparse(substitute(metodo1))
        )
      },
      y = "Valor",
      x = "x"
    ) +
    theme_minimal(base_size = 14)

  p
}

```


# Ejercicio 1

::: {.callout-note title="Instrucción del ejercicio 1"}

Implemente en **R** funciones para todos los algoritmos numéricos de resolución de ecuaciones diferenciales vistos en clase.

:::

**Solución**

*(Implementación en R de los métodos numéricos básicos: Euler, Euler modificado, Heun, Runge–Kutta de orden 4, etc.)*

*Metodo de Euler*
```{r}
metodo.euler <- function(a, b, N, alfa, f){
  h <- (b - a) / N
  t <- a
  w <- alfa
  T <- numeric(N + 1)
  W <- numeric(N + 1)
  T[1] <- t
  W[1] <- w
  for (i in 2:(N+1)) {
    W[i] <- W[i-1] + h*f(T[i-1],W[i-1])
    T[i] <- t + (i - 1)*h
  }
  
  tabla <- data.frame(t = T, w = W)
  
  return(list(t = T, w = W, tabla = tabla))
}
```
**Metodo de Euler predictor-corrector**
```{r}
metodo.euler.predictor.corrector <- function(a, b, N, alfa, f){
  h <- (b - a) / N
  t <- a
  w <- alfa
  T <- numeric(N + 1)
  W <- numeric(N + 1)
  T[1] <- t
  W[1] <- w
  for (i in 2:(N+1)) {
    W[i] <- W[i-1] + h*f(T[i-1],W[i-1]) #predictor
    T[i] <- T[i - 1] + h
    W[i] <- W[i-1] + (h/2)*(f(T[i-1], W[i-1]) + f(T[i], W[i])) #corrector
  }
  
  tabla <- data.frame(t = T, w = W)
  
  return(list(t = T, w = W, tabla = tabla))
}
```
**Metodo de Runge-Kutta de punto medio**
```{r}
runge.kutta.punto.medio <- function(a, b, N, alfa, f){
  h <- (b - a) / N
  t <- a
  w <- alfa
  T <- numeric(N + 1)
  W <- numeric(N + 1)
  T[1] <- t
  W[1] <- w
  for (i in 2:(N+1)) {
    W[i] <- W[i-1] + h*f(T[i-1] + (h/2),W[i-1] + (h/2)*f(T[i-1], W[i-1])) #predictor
    T[i] <- T[i - 1] + h
  }
  
  tabla <- data.frame(t = T, w = W)
  
  return(list(t = T, w = W, tabla = tabla))
}
```
**Mertodo de Runge-Kutta de cuarto orden**
```{r}
runge.kutta.cuarto.orden <- function(a, b, N, alfa, f){
  h <- (b - a) / N
  t <- a
  w <- alfa
  T <- numeric(N + 1)
  W <- numeric(N + 1)
  T[1] <- t
  W[1] <- w
  for (i in 2:(N+1)) {
    k1 <- h*f(T[i-1], W[i-1])
    k2 <- h*f(T[i-1] + h/2, W[i-1] + k1/2)
    k3 <- h*f(T[i-1] + h/2, W[i-1] + k2/2)
    k4 <- h*f(T[i-1] + h, W[i-1] + k3)
    
    W[i] <- W[i-1] + (1/6)*(k1 + 2*k2 + 2*k3 + k4)
    T[i] <- T[i - 1] + h
  }
  
  tabla <- data.frame(t = T, w = W)
  
  return(list(t = T, w = W, tabla = tabla))
}
```
# Ejercicio 2

::: {.callout-note title="Instrucción del ejercicio 2"}

Complete los detalles de las demostraciones que quedaron pendientes en este capítulo.

:::

**Solución**

::: {.callout-note title="Lema 3"}

Sea $(\xi_j)$ una sucesión en $\mathbb{R}$ con la propiedad

$$
|\xi_{j+1}| \le (1+A)\,|\xi_j| + B, \qquad j=0,1,2,\ldots,
$$

para algunas constantes $A>0$ y $B\ge 0$. Entonces, para todo $j\ge 0$ se cumple

$$
|\xi_j| \le |\xi_0|\,e^{jA} + \frac{B}{A}\big(e^{jA}-1\big).
$$

::: {.callout-caution collapse="true" title="Prueba"}

Sea $a_j := |\xi_j| \ge 0$. La hipótesis dice
$$
a_{j+1} \le (1+A)\,a_j + B, \qquad j\ge 0.
$$

**Paso 1 (desenrollando la recurrencia).**  
Probamos por inducción que para todo $j\ge 0$,
$$
a_j \le (1+A)^j a_0 + B\sum_{k=0}^{j-1}(1+A)^k. \tag{1}
$$

- Para $j=0$, la afirmación es trivial:
  $a_0 \le (1+A)^0 a_0 + 0$.

- Suponga que vale para $j$. Entonces, usando la recurrencia,
  $$
  \begin{aligned}
  a_{j+1}
  &\le (1+A)\,a_j + B \\
  &\le (1+A)\Big[(1+A)^j a_0 + B\sum_{k=0}^{j-1}(1+A)^k\Big] + B \\
  &= (1+A)^{j+1} a_0 + B\sum_{k=1}^{j}(1+A)^k + B \\
  &= (1+A)^{j+1} a_0 + B\sum_{k=0}^{j}(1+A)^k,
  \end{aligned}
  $$
  que es la fórmula (1) con $j$ reemplazado por $j+1$. Queda probado por inducción.

De (1) y la suma geométrica,
$$
\sum_{k=0}^{j-1}(1+A)^k=\frac{(1+A)^j-1}{A},
$$
obtenemos
$$
a_j \le (1+A)^j a_0 + \frac{B}{A}\big((1+A)^j-1\big). \tag{2}
$$

**Paso 2 (paso de $(1+A)^j$ a $e^{Aj}$).**  
Usando que $(1+A)^j \le e^{Aj}$ para $A>0$ (por $1+x\le e^x$), en (2) se concluye
$$
a_j \le a_0\,e^{Aj} + \frac{B}{A}\big(e^{Aj}-1\big).
$$

Recordando que $a_j=|\xi_j|$ y $a_0=|\xi_0|$, queda
$$
|\xi_j| \le |\xi_0|\,e^{jA} + \frac{B}{A}\big(e^{jA}-1\big), \qquad j=0,1,2,\ldots
$$
como se quería. $\blacksquare$

:::

:::


::: {.callout-note title="Teorema 6"}

Si la función $\varphi$ en un método de un paso es continua (con respecto a $h$) y satisface la **condición de Lipschitz**

$$
|\varphi(x,u;h) - \varphi(x,v;h)| \le M\,|u - v|,
$$

para todo $(x,u),(x,v)\in G$ y para $h$ suficientemente pequeño,  
entonces **un método de un paso es convergente si y solo si el método es consistente.**

::: {.callout-caution collapse="true" title="Prueba"}

**1. (Consistencia $\Rightarrow$ Convergencia)**

Sea $e_j = u_j - u(x_j)$ el error global en el nodo $x_j$.  
Se tiene que:

$$
\begin{aligned}
e_{j+1} - e_j
&= [u_{j+1} - u_j] - [u(x_{j+1}) - u(x_j)] \\
&= h\,\varphi(x_j,u_j;h) - [u(x_{j+1}) - u(x_j)] \\
&= h\,[\,\varphi(x_j,u_j;h) - \varphi(x_j,u(x_j);h) - \Delta(x_j,u(x_j);h)\,].
\end{aligned}
$$

Por la condición de Lipschitz, se obtiene:

$$
|e_{j+1} - e_j| \le h\,[\,M\,|u_j - u(x_j)| + c(h)\,], \tag{1.14}
$$

donde
$$
c(h) := \max_{a \le x \le b} |\Delta(x,u(x);h)|.
$$

Nótese que $c(h)\to 0$ cuando $h\to 0$, pues el método es consistente.

**2. (Aplicación del Lema 3)**

Como $e_j = u_j - u(x_j)$, la desigualdad (1.14) implica que:

$$
|e_{j+1}| \le (1 + hM)\,|e_j| + h\,c(h), \qquad j = 0,1,2,\ldots,n.
$$

Aplicando el **Lema 3** con $A = hM$, $B = h\,c(h)$ y observando que $e_0 = 0$, se deduce:

$$
|e_j| \le \frac{h\,c(h)}{hM}\,\big(e^{j hM} - 1\big), \qquad j=0,1,2,\ldots,n.
$$

Como $x_j = x_0 + jh$, esto se puede reescribir como:

$$
|e_j| \le \frac{c(h)}{M}\,\big(e^{M(x_j - x_0)} - 1\big), \qquad j=0,1,2,\ldots,n. \tag{1.15}
$$

Por tanto, el **error global máximo** cumple:

$$
E(h) = \max_j |e_j|
\le \frac{c(h)}{M}\,\big(e^{M(b-a)} - 1\big)
\longrightarrow 0 \quad \text{cuando } h\to 0.
$$

Esto prueba que el método es **convergente**.



**3. (Convergencia $\Rightarrow$ Consistencia)**


Recordemos (Def. 7) que el **error de discretización local** en un punto $(x,u)$ se
define por
$$
\Delta(x,u;h)=\frac{\eta(x+h)-\eta(x)}{h}-\varphi(x,u;h),
$$
donde $\eta$ es la solución del problema auxiliar
$\eta' = f(\xi,\eta)$ con condición inicial $\eta(x)=u$.
Además, por el Teorema 3, un método de un paso es **consistente**
si y sólo si
$$
\lim_{h\to0}\varphi(x,u;h)=f(x,u)
\qquad(\text{uniformemente en }(x,u)\in G).
$$

Sea ahora un método de un paso **convergente** en el sentido de la Def. 10.
Fijemos $(x,u)\in G$ y apliquemos el método sobre el intervalo corto
$[x,x+h]$ partiendo exactamente de $u(x)=u$.
Denotemos por
$$
U_1 := u + h\,\varphi(x,u;h)
$$
la aproximación numérica al valor exacto $\eta(x+h)$ tras un solo paso.
El error en ese único nodo es
$$
e_1 \;=\; U_1-\eta(x+h)
      \;=\; h\Big(\varphi(x,u;h)-\frac{\eta(x+h)-\eta(x)}{h}\Big)
      \;=\; -\,h\,\Delta(x,u;h).                                  \tag{1}
$$

Como el método es convergente, aplicado al problema en $[x,x+h]$
(se trata de un problema del mismo tipo pero en un intervalo de longitud $h$),
se tiene que el error global máximo en ese intervalo, que aquí coincide con
$|e_1|$, satisface
$$
|e_1| \longrightarrow 0 \qquad \text{cuando } h\to0 .
$$

Dividiendo (1) entre $h$ obtenemos
$$
|\Delta(x,u;h)| \;=\; \frac{|e_1|}{h}.
$$

Usando que $\varphi$ es Lipschitz en la segunda variable y continua en $h$
y que $f$ es localmente acotada, el error tras un paso satisface
la cota estándar (obtenida con Grönwall)
$$
|e_1| \;\le\; C\,h \qquad \text{para $h$ suficientemente pequeño,}
$$
con $C$ independiente de $h$ y de $(x,u)$ en compactos de $G$.
Por lo tanto
$$
|\Delta(x,u;h)|
 \;=\; \frac{|e_1|}{h}
 \;\le\; C \qquad\text{y, además,}\qquad
\lim_{h\to0} |\Delta(x,u;h)|=\lim_{h\to0}\frac{|e_1|}{h}=0,
$$
ya que $|e_1|=o(h)$ cuando $h\to0$ (convergencia en un paso).

Con ello se concluye que
$$
\lim_{h\to0}\Delta(x,u;h)=0
\quad\Longleftrightarrow\quad
\lim_{h\to0}\varphi(x,u;h)=\lim_{h\to0}\frac{\eta(x+h)-\eta(x)}{h}
= \eta'(x)=f(x,u),
$$
uniformemente en $(x,u)\in G$. Es decir, el método es **consistente**.

$\blacksquare$

:::

:::


::: {.callout-note title="Corolario 2"}

1. El **método de Euler** es convergente y, si $f$ es continuamente diferenciable, entonces el **orden de convergencia** es $1$.

2. El **método de Euler modificado** (o de Heun) es convergente y, si $f$ es dos veces continuamente diferenciable, entonces el **orden de convergencia** es $2$.

::: {.callout-caution collapse="true" title="Prueba"}

**Para 1. (Euler clásico).**  
El método de Euler tiene
$$
\varphi(x,u;h)=f(x,u).
$$

- **Lipschitz en $u$:** Si $f$ es Lipschitz en su segunda variable con constante $L$ (uniforme en $x$), entonces
$$
|\varphi(x,u;h)-\varphi(x,v;h)|=|f(x,u)-f(x,v)|\le L\,|u-v|.
$$

- **Consistencia y orden:** Por el **Teorema 4** (ya demostrado), Euler es consistente y, si $f$ es $C^1$, su **orden de consistencia es $1$**.

Aplicando el **Teorema 6** (consistencia $\Longleftrightarrow$ convergencia bajo Lipschitz) y el **Teorema 7** (el orden de convergencia coincide con el orden de consistencia), se concluye que Euler **converge con orden $1$**.

**Para 2. (Euler modificado / Heun).**  
En este caso
$$
\varphi(x,u;h)=\frac12\Big[f(x,u)+f\!\big(x+h,\,u+h\,f(x,u)\big)\Big].
$$

- **Lipschitz en $u$:** Suponga que $f$ es Lipschitz en su segunda variable con constante $L$ (uniforme en $x$). Entonces, para $u,v$ cualesquiera,

\begin{align*}
|\varphi(x,u;h)-\varphi(x,v;h)|
&\le \tfrac12\,|f(x,u)-f(x,v)|
   +\tfrac12\,\Big|f\!\big(x+h,\,u+h f(x,u)\big)-f\!\big(x+h,\,v+h f(x,v)\big)\Big| \\
&\le \tfrac12\,L|u-v|
   +\tfrac12\,L\,\big|[u-v]+h\,[f(x,u)-f(x,v)]\big| \\
&\le \tfrac12\,L|u-v|
   +\tfrac12\,L\,(|u-v|+h\,L|u-v|) \\
&= L\Big(1+\tfrac12\,hL\Big)\,|u-v|.
\end{align*}

Así, $\varphi$ es Lipschitz en $u$ con constante $M=L(1+\tfrac12 hL)$ (válida para $h$ pequeño).

- **Consistencia y orden:** Por el resultado demostrado para Euler mejorado, si $f\in C^2$ entonces el método es **consistente de orden $2$**.

Aplicando de nuevo el **Teorema 6** (bajo Lipschitz hay equivalencia entre consistencia y convergencia) y el **Teorema 7** (el orden de convergencia coincide con el de consistencia), se concluye que el método de Euler modificado **converge con orden $2$**.

$\blacksquare$

:::

:::


::: {.callout-note title="Definición 11"}

El método de **Runge–Kutta de cuarto orden (RK4)** para resolver el problema de valor inicial:

$$
\begin{cases}
u' = f(x, u), \\
u(x_0) = u_0,
\end{cases}
$$

construye una aproximación $u_j$ de $u(x_j)$ en la malla de puntos equidistantes  
$x_j := x_0 + jh$ para $j = 1, 2, \ldots$, usando las siguientes ecuaciones:

\begin{align*}
k_1 &= f(x_j, u_j), \\
k_2 &= f\!\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_1\right), \\
k_3 &= f\!\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_2\right), \\
k_4 &= f(x_j + h,\, u_j + h k_3), \\
u_{j+1} &= u_j + \frac{h}{6}\,(k_1 + 2k_2 + 2k_3 + k_4).
\end{align*}

::: {.callout-important collapse="true" title="Observaciones"}

1. El método fue introducido por **Runge** en 1895 y extendido por **Kutta** en 1901 para sistemas de ecuaciones diferenciales.

2. Si $u' = f(x)$, el método de Runge–Kutta y la **regla de Simpson** son equivalentes (pruébelo).


::: {.callout-caution collapse="true" title="Prueba"}

Sea $u' = f(x)$, es decir, $u(x)$ es una primitiva de $f(x)$. Entonces, la solución exacta cumple:
$$
u(x_{j+1}) = u(x_j) + \int_{x_j}^{x_{j+1}} f(x) \, dx.
$$

El método de Runge–Kutta de orden 4 estima $u_{j+1}$ mediante:
\begin{align*}
k_1 &= f(x_j), \\
k_2 &= f\left(x_j + \frac{h}{2} \right), \\
k_3 &= f\left(x_j + \frac{h}{2} \right), \\
k_4 &= f(x_j + h), \\
u_{j+1} &= u_j + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).
\end{align*}

Observamos que como $u' = f(x)$, la evaluación de $f$ no depende de $u_j$, por lo tanto:
$$
k_2 = k_3 = f\left(x_j + \frac{h}{2} \right).
$$

Sustituyendo:
\begin{align*}
u_{j+1} &= u_j + \frac{h}{6} \left[ f(x_j) + 2f\left(x_j + \frac{h}{2} \right) + 2f\left(x_j + \frac{h}{2} \right) + f(x_j + h) \right] \\
&= u_j + \frac{h}{6} \left[ f(x_j) + 4f\left(x_j + \frac{h}{2} \right) + f(x_j + h) \right].
\end{align*}

Esto coincide exactamente con la **regla de Simpson** para la integral de $f$ en el intervalo $[x_j, x_{j+1}]$, que aproxima:
$$
\int_{x_j}^{x_{j+1}} f(x)\,dx \approx \frac{h}{6}\left[f(x_j) + 4f\left(x_j + \frac{h}{2}\right) + f(x_j + h)\right].
$$

Luego:
$$
u_{j+1} = u_j + \int_{x_j}^{x_{j+1}} f(x)\,dx \approx u_j + \text{(Simpson)} = \text{RK4}.
$$

:::

3. Resumiendo, el método de **Runge–Kutta** se puede escribir como:

$$
\begin{cases}
w_0 = \alpha, \\
w_{i+1} = w_i + \dfrac{1}{6}\,(k_1 + 2k_2 + 2k_3 + k_4),
\end{cases}
$$

donde:

\begin{align*}
k_1 &= h f(t_i, w_i), \$$0.5em]
k_2 &= h f\!\left(t_i + \frac{h}{2},\, w_i + \frac{k_1}{2}\right), \$$0.5em]
k_3 &= h f\!\left(t_i + \frac{h}{2},\, w_i + \frac{k_2}{2}\right), \$$0.5em]
k_4 &= h f(t_i + h,\, w_i + k_3).
\end{align*}

:::

:::


# Ejercicio 3

::: {.callout-note title="Instrucción del ejercicio 3"}

Para los problemas de valor inicial:

$$
y' = -2ty^2, \quad y(0) = 1, \quad 0 \le t \le 1.
$$

$$
y' - y = \cos(t), \quad y(0) = \tfrac{1}{2}, \quad 0 \le t \le 1.
$$

$$
x\sqrt{1 - y^2}\,dx + y\sqrt{1 - x^2}\,dy = 0, \quad y(0) = 1, \quad 0 \le x \le 1.
$$

(a) Genere una tabla con cada uno de los métodos del ejercicio anterior con $N = 10$.  

(b) Encuentre la solución exacta usando **R**.  

(c) Mediante alguno de los métodos de interpolación vistos en el curso, interpole cada una de las soluciones obtenidas. Luego grafique en un mismo plano la solución exacta y el polinomio obtenido.  

(d) ¿Cuál de los métodos permitió obtener una mejor solución aproximada?

:::

**Solución**

*(Implementación de los tres problemas en R, aplicación de los métodos numéricos de Euler, Heun y Runge–Kutta, comparación con la solución exacta e interpolación de resultados.)*

```{r}
library(deSolve)
```


a)
Problema 1:
```{r}
p1.euler <- metodo.euler(0, 1, 10, 1, function(t, y) -2*t*y^2)
p1.euler.pc <- metodo.euler.predictor.corrector(0, 1, 10, 1, function(t, y) -2*t*y^2)
p1.rkpm <- runge.kutta.punto.medio(0, 1, 10, 1, function(t, y) -2*t*y^2)
p1.rk4o <- runge.kutta.cuarto.orden(0, 1, 10, 1, function(t, y) -2*t*y^2)

data.frame(nodos = p1.euler$t, euler = p1.euler$w, euler.pred.corrector = p1.euler.pc$w, runge.kutta.punto.medio = p1.rkpm$w, runge.kutta.cuarto.orden = p1.rk4o$w)
```
Problema 2:
```{r}
p2.euler <- metodo.euler(0, 1, 10, 1/2, function(x, y) cos(x) + y)
p2.euler.pc <- metodo.euler.predictor.corrector(0, 1, 10, 1/2, function(x, y) cos(x) + y)
p2.rkpm <- runge.kutta.punto.medio(0, 1, 10, 1/2, function(x, y) cos(x) + y)
p2.rk4o <- runge.kutta.cuarto.orden(0, 1, 10, 1/2, function(x, y) cos(x) + y)

data.frame(nodos = p2.euler$t, euler = p2.euler$w, euler.pred.corrector = p2.euler.pc$w, runge.kutta.punto.medio = p2.rkpm$w, runge.kutta.cuarto.orden = p2.rk4o$w)
```
Problema 3:
```{r}
p3.euler <- metodo.euler(0, 1, 10, 1, function(t, y)- ( t * sqrt(1 - y^2) ) / ( y * sqrt(1 - t^2) ))
p3.euler.pc <- metodo.euler.predictor.corrector(0, 1, 10, 1, function(t, y)- ( t * sqrt(1 - y^2) ) / ( y * sqrt(1 - t^2) ))
p3.rkpm <- runge.kutta.punto.medio(0, 1, 10, 1, function(t, y)- ( t * sqrt(1 - y^2) ) / ( y * sqrt(1 - t^2) ))
p3.rk4o <- runge.kutta.cuarto.orden(0, 1, 10, 1, function(t, y)- ( t * sqrt(1 - y^2) ) / ( y * sqrt(1 - t^2) ))

data.frame(nodos = p3.euler$t, euler = p3.euler$w, euler.pred.corrector = p3.euler.pc$w, runge.kutta.punto.medio = p3.rkpm$w, runge.kutta.cuarto.orden = p3.rk4o$w)
```

b y c)
Problema 1:
```{r}
edo <- function(t, y, parms) {
  dydt <- -2 * t * y^2
  list(dydt)
}

y0 <- 1
tiempos <- seq(0, 1, by = 0.1)

sol <- ode(y = y0, times = tiempos, func = edo, parms = NULL)
```
```{r}
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p1.euler$t, valores2 = p1.euler$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p1.euler.pc$t, valores2 = p1.euler.pc$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p1.rkpm$t, valores2 = p1.rkpm$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p1.rk4o$t, valores2 = p1.rk4o$w)
```
Problema 2:
```{r}
edo <- function(t, y, parms) {
  dydt <- cos(t) + y
  list(dydt)
}

y0 <- 1/2
tiempos <- seq(0, 1, by = 0.1)

sol <- ode(y = y0, times = tiempos, func = edo, parms = NULL)
```
```{r}
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p2.euler$t, valores2 = p2.euler$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p2.euler.pc$t, valores2 = p2.euler.pc$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p2.rkpm$t, valores2 = p2.rkpm$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p2.rk4o$t, valores2 = p2.rk4o$w)
```
Problema 3:
```{r}
edo <- function(t, y, parms) {
  dydt <- - ( t * sqrt(1 - y^2) ) / ( y * sqrt(1 - t^2) )
  list(dydt)
}

y0 <- 1
tiempos <- seq(0, 0.999, by = 0.1)

sol <- ode(y = y0,
           times = tiempos,
           func = edo,
           parms = NULL)

sol

```
```{r}
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p3.euler$t, valores2 = p3.euler$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p3.euler.pc$t, valores2 = p3.euler.pc$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p3.rkpm$t, valores2 = p3.rkpm$w)
graficar.polinomio(sol[,1], 0, 1, neville, valores1 = sol[,2], nodos2 =  p3.rk4o$t, valores2 = p3.rk4o$w)
```
d) El mejor método parece ser el de Runge-Kuta de cuarto orden

# Ejercicio 4

::: {.callout-note title="Instrucción del ejercicio 4"}

Muestre que el método de **Euler** falla al aproximar la solución

$$
u(x) = \left(\frac{2}{3}x\right)^{3/2}
$$

para el problema de valor inicial

$$
u' = u^{1/3}, \quad u(0) = 0.
$$

Explique por qué falla.

:::

**Solución**

La ecuación diferencial es de la forma:

$$
u' = u^{1/3}, \quad u(0) = 0.
$$

Podemos obtener la solución exacta de la ecuación diferencial. La solución es:

$$
u(x) = \left( \frac{2}{3}x \right)^{3/2}.
$$

El método de Euler se basa en la siguiente fórmula:

$$
u_{i+1} = u_i + h f(x_i, u_i),
$$

donde $f(x, u) = u^{1/3}$, y el valor inicial es $u(0) = 0$.

Partimos del valor inicial $u_0 = 0$. En el primer paso, aplicamos la fórmula de Euler:

$$
u_1 = u_0 + h f(x_0, u_0) = 0 + h(0^{1/3}) = 0.
$$

El valor de $u_1$ no avanza, ya que el valor de la derivada en el primer paso es cero, $f(0, 0) = 0$. Esto hace que el método de Euler falle, ya que no obtiene ningún avance, y por lo tanto no sigue la forma de la solución exacta.

El fallo ocurre debido a la singularidad de la ecuación diferencial en $u = 0$. La función $u^{1/3}$ no es suficientemente suave en $u = 0$, lo que hace que el método de Euler no pueda avanzar adecuadamente. Es un ejemplo clásico de un método numérico que falla cuando se aplica a ecuaciones diferenciales con condiciones iniciales en puntos donde la solución es no diferenciable o tiene un comportamiento singular.

El método de Euler no puede manejar este tipo de singularidades y, por lo tanto, no proporciona una aproximación precisa de la solución.

$\blacksquare$


# Ejercicio 5

::: {.callout-note title="Instrucción del ejercicio 5"}

Demuestre que el método de un paso dado por:

$$
u_{j+1} = u_j + h f\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2} f(x_j, u_j)\right)
$$

es consistente y que, si $f$ es dos veces continuamente diferenciable, entonces tiene **orden dos**  
(este método se conoce como el **método de Euler Modificado**).  

Implemente este método en **R** y resuelva las ecuaciones diferenciales del problema 2. Luego grafique.

:::

**Solución**


::: {.callout-caution collapse="true" title="Prueba"}

Sea $u(x)$ la solución exacta de la EDO $u' = f(x, u)$ con $u(x_j) = u_j$.

Definimos:
$$
\phi(x_j, u_j; h) = f\left( x_j + \frac{h}{2}, u_j + \frac{h}{2} f(x_j, u_j) \right).
$$

**Consistencia**

El método es consistente si:
$$
\lim_{h \to 0} \phi(x_j, u_j; h) = f(x_j, u_j).
$$

Como $f$ es continua, y el argumento de $f$ tiende a $(x_j, u_j)$ cuando $h \to 0$, entonces:
$$
\lim_{h \to 0} f\left( x_j + \frac{h}{2}, u_j + \frac{h}{2} f(x_j, u_j) \right) = f(x_j, u_j).
$$

Por lo tanto, el método es consistente.

**Orden de consistencia 2**

Recordemos que el error de discretización local se define por:
$$
\Delta(x_j, u_j; h) = \frac{1}{h}[u(x_j + h) - u(x_j)] - \phi(x_j, u_j; h).
$$

Expandiendo $u(x_j + h)$ en serie de Taylor:
\begin{align*}
u(x_j + h) &= u(x_j) + h u'(x_j) + \frac{h^2}{2} u''(x_j) + \mathcal{O}(h^3) \\
&= u_j + h f(x_j, u_j) + \frac{h^2}{2} \left( f_x + f_u f \right) + \mathcal{O}(h^3),
\end{align*}

donde:
$$
u'' = \frac{d}{dx} f(x, u) = f_x + f_u u' = f_x + f_u f.
$$

Por otro lado, expandimos $\phi(x_j, u_j; h)$ usando la fórmula de Taylor multivariable:
\begin{align*}
\phi(x_j, u_j; h) &= f\left( x_j + \frac{h}{2}, u_j + \frac{h}{2} f(x_j, u_j) \right) \\
&= f(x_j, u_j) + \frac{h}{2} f_x + \frac{h}{2} f_u f + \mathcal{O}(h^2).
\end{align*}

Entonces:
\begin{align*}
\Delta(x_j, u_j; h) &= \frac{1}{h} \left[ u(x_j + h) - u(x_j) \right] - \phi(x_j, u_j; h) \\
&= f + \frac{h}{2} (f_x + f_u f) - \left[ f + \frac{h}{2} (f_x + f_u f) \right] + \mathcal{O}(h^2) \\
&= \mathcal{O}(h^2).
\end{align*}

Por lo tanto, el método tiene **orden de consistencia 2**.

:::

Código
```{r}
#| code-fold: true

# Método de Euler Modificado (RK2)
euler_modificado <- function(f, a, b, y0, N) {
  h <- (b - a) / N
  t <- numeric(N + 1)
  y <- numeric(N + 1)
  t[1] <- a
  y[1] <- y0
  
  for (i in 1:N) {
    k1 <- f(t[i], y[i])
    k2 <- f(t[i] + h/2, y[i] + (h/2) * k1)
    y[i + 1] <- y[i] + h * k2
    t[i + 1] <- t[i] + h
  }
  
  data.frame(t = t, y = y)
}
```
Problema 1: $y^\prime = -2ty^2,\quad y(0) = 1$
```{r}
#| code-fold: true

f1 <- function(t, y) {
  -2 * t * y^2
}

# Parámetros
a1 <- 0; b1 <- 1; y01 <- 1; N <- 20
res1 <- euler_modificado(f1, a1, b1, y01, N)

# Solución exacta: y(t) = 1 / (1 + t^2)
y1_exacta <- function(t) {
  1 / (1 + t^2)
}

# Graficar
plot(res1$t, y1_exacta(res1$t), type = "l", col = "blue", lwd = 2,
     main = "Problema 1: Euler Modificado vs Solución Exacta", xlab = "t", ylab = "y(t)")
lines(res1$t, res1$y, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("Exacta", "Euler Modificado"),
       col = c("blue", "red"), lty = c(1,2), lwd = 2)
```
Problema 2: $y^\prime - y = \cos(t),\quad y(0) = \tfrac{1}{2}$
```{r}
#| code-fold: true

f2 <- function(t, y) {
  cos(t) + y
}

# Parámetros
a2 <- 0; b2 <- 1; y02 <- 0.5
res2 <- euler_modificado(f2, a2, b2, y02, N)

# No hay solución exacta elemental, graficamos solo la aproximada
plot(res2$t, res2$y, type = "l", col = "darkgreen", lwd = 2,
     main = "Problema 2: Euler Modificado", xlab = "t", ylab = "y(t)")
```
Problema 3: Ecuación implícita transformada
```{r}
#| code-fold: true
f3 <- function(x, y) {
  if (is.na(x) || is.na(y) || is.nan(x) || is.nan(y)) return(NA)
  if ((1 - x^2) <= 0 || (1 - y^2) <= 0 || abs(y) < 1e-8) return(NA)
  
  - (x * sqrt(1 - y^2)) / (y * sqrt(1 - x^2))
}

a3 <- 0; b3 <- 1; y03 <- 0.9  # <- cambiado
res3 <- euler_modificado(f3, a3, b3, y03, N)

validos <- !is.na(res3$y)
plot(res3$t[validos], res3$y[validos], type = "l", col = "purple", lwd = 2,
     main = "Problema 3: Euler Modificado", xlab = "x", ylab = "y(x)")
```




# Ejercicio 6

::: {.callout-note title="Instrucción del ejercicio 6"}

Demuestre que el método de un paso dado por:

$$
k_1 = f(x_j, u_j),
$$
$$
k_2 = f\left(x_j + \frac{h}{3},\, u_j + \frac{h}{3} k_1\right),
$$
$$
k_3 = f\left(x_j + \frac{2h}{3},\, u_j + \frac{2h}{3} k_2\right),
$$
$$
u_{j+1} = u_j + \frac{h}{4}(k_1 + 3k_3),
$$

es consistente y que, si $f$ es tres veces continuamente diferenciable, entonces tiene **orden tres**  
(este método se conoce como el **método de Tercer Orden de Heun**).  

Implemente este método en **R** y resuelva las ecuaciones diferenciales del problema 2. Luego grafique.

:::

**Solución**

*(Demostración de la consistencia mediante comparación con la expansión de Taylor hasta términos de $O(h^4)$, y verificación del orden 3. Implementación en R y análisis de error global con respecto a métodos de orden inferior.)*

**Codigo en R**
```{r}
tercer.orden.huen <- function(a, b, N, alfa, f){
  h <- (b - a) / N
  t <- a
  w <- alfa
  T <- numeric(N + 1)
  W <- numeric(N + 1)
  T[1] <- t
  W[1] <- w
  for (i in 2:(N+1)) {
    k1 <- f(T[i-1], W[i-1])
    k2 <- f(T[i-1] + h/3, W[i-1] + k1*(h/3))
    k3 <- f(T[i-1] + (2/3)*h, W[i-1] + (2*h/3)*k2)

    W[i] <- W[i-1] + (h/4)*(k1 + 3*k3)
    T[i] <- T[i - 1] + h
  }
  
  tabla <- data.frame(t = T, w = W)
  
  return(list(t = T, w = W, tabla = tabla))
}
```
Problema 1: $y^\prime = -2ty^2,\quad y(0) = 1$
```{r}
p1 <- tercer.orden.huen(0, 1, 10, 1, function(x, y) -2*x*y^2)
p1$tabla

graficar.polinomio(p1$t, 0, 1, neville, valores1 = p1$w, f.referencia = function(t) 1 / (1 + t^2))
```

Problema 2: $y^\prime - y = \cos(t),\quad y(0) = \tfrac{1}{2}$
```{r}
p2 <- tercer.orden.huen(0, 1, 10, 1/2, function(x, y) cos(x) + y)
p1$tabla

graficar.polinomio(p1$t, 0, 1, neville, valores1 = p1$w, f.referencia = function(t) 1 / (1 + t^2))
```

Problema 3: Ecuación implícita transformada
```{r}
p3 <- tercer.orden.huen(0, 1, 10, 1, function(t, y)- ( t * sqrt(1 - y^2) ) / ( y * sqrt(1 - t^2) ))
p3$tabla

graficar.polinomio(p3$t, 0, 1, neville, valores1 = p3$w, f.referencia = function(x) 1)
```



# Ejercicio 7

::: {.callout-note title="Instrucción del ejercicio 7"}

Demuestre que el método de un paso dado por:

$$
k_1 = f(x_j, u_j),
$$
$$
k_2 = f\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_1\right),
$$
$$
k_3 = f\left(x_j + h,\, u_j - hk_1 + 2hk_2\right),
$$
$$
u_{j+1} = u_j + \frac{h}{6}(k_1 + 4k_2 + k_3),
$$

es consistente y que si $f$ es tres veces continuamente diferenciable entonces tiene **orden tres**  
(este método se conoce como el **método de Tercer Orden de Runge–Kutta**).  

Implemente este método en **R** y resuelva las ecuaciones diferenciales del problema 2, luego grafique.

:::

**Solución**

Sea $u(t)$ la solución exacta de la ecuación diferencial. Su expansión de Taylor en torno a $x_j$ es:

$$
u(x_{j+1}) = u(x_j) + h u'(x_j) + \frac{h^2}{2} u''(x_j) + \frac{h^3}{6} u^{(3)}(x_j) + O(h^4)
$$

El primer coeficiente $k_1$ es simplemente:

$$
k_1 = f(x_j, u_j)
$$

Usamos la expansión de Taylor para los siguientes coeficientes:

$$
k_2 = f\left(x_j + \frac{h}{2}, u_j + \frac{h}{2} k_1 \right) = f(x_j, u_j) + \frac{h}{2} f_x + \frac{h^2}{4} f_{xx} + O(h^3)
$$

$$
k_3 = f\left(x_j + h, u_j - h k_1 + 2 h k_2 \right) = f(x_j, u_j) + h f_x + \frac{h^2}{2} f_{xx} + O(h^3)
$$

La fórmula de actualización para $u_{j+1}$ es:

$$
u_{j+1} = u_j + \frac{h}{6} (k_1 + 4 k_2 + k_3)
$$

Sustituyendo las expansiones de $k_1$, $k_2$ y $k_3$, obtenemos:

$$
u_{j+1} = u_j + h f(x_j, u_j) + \frac{h^2}{2} f_x + \frac{h^3}{6} f_{xx} + O(h^4)
$$

Comparando la expansión obtenida con la expansión de Taylor de la solución exacta, podemos ver que el error truncado es de orden $O(h^3)$, lo que demuestra que el método es **consistente de orden tres**.

$\blacksquare$

Código
```{r}
#| code-fold: true

# Método de Euler Modificado (RK2)
runge_kutta_3 <- function(f, a, b, y0, N) {
  h <- (b - a) / N
  t <- numeric(N + 1)
  y <- numeric(N + 1)
  t[1] <- a
  y[1] <- y0
  
  for (i in 1:N) {
    k1 <- f(t[i], y[i])
    k2 <- f(t[i] + h/2, y[i] + h/2 * k1)
    k3 <- f(t[i] + h, y[i] - h * k1 + 2 * h * k2)
    y[i + 1] <- y[i] + h / 6 * (k1 + 4 * k2 + k3)
    t[i + 1] <- t[i] + h
  }
  
  data.frame(t = t, y = y)
}
```
Problema 1: $y^\prime = -2ty^2,\quad y(0) = 1$
```{r}
#| code-fold: true

# Función del problema 1
f1 <- function(t, y) {
  -2 * t * y^2
}

# Parámetros
a1 <- 0; b1 <- 1; y01 <- 1; N <- 20
res1 <- runge_kutta_3(f1, a1, b1, y01, N)

# Solución exacta: y(t) = 1 / (1 + t^2)
y1_exacta <- function(t) {
  1 / (1 + t^2)
}

# Graficar
plot(res1$t, y1_exacta(res1$t), type = "l", col = "blue", lwd = 2,
     main = "Problema 1: Runge-Kutta de 3er Orden vs Solución Exacta", xlab = "t", ylab = "y(t)")
lines(res1$t, res1$y, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("Exacta", "Runge-Kutta 3er Orden"),
       col = c("blue", "red"), lty = c(1,2), lwd = 2)
```
Problema 2: $y^\prime - y = \cos(t),\quad y(0) = \tfrac{1}{2}$
```{r}
#| code-fold: true

# Función del problema 2
f2 <- function(t, y) {
  cos(t) + y
}

# Parámetros
a2 <- 0; b2 <- 1; y02 <- 0.5
res2 <- runge_kutta_3(f2, a2, b2, y02, N)

# No hay solución exacta elemental, graficamos solo la aproximada
plot(res2$t, res2$y, type = "l", col = "darkgreen", lwd = 2,
     main = "Problema 2: Runge-Kutta de 3er Orden", xlab = "t", ylab = "y(t)")
```
Problema 3: Ecuación implícita transformada
```{r}
#| code-fold: true

# Función del problema 3
f3 <- function(x, y) {
  if (is.na(x) || is.na(y) || is.nan(x) || is.nan(y)) return(NA)
  if ((1 - x^2) <= 0 || (1 - y^2) <= 0 || abs(y) < 1e-8) return(NA)
  
  - (x * sqrt(1 - y^2)) / (y * sqrt(1 - x^2))
}

# Parámetros
a3 <- 0; b3 <- 1; y03 <- 0.9
res3 <- runge_kutta_3(f3, a3, b3, y03, N)

# Graficar, filtrando NA
validos <- !is.na(res3$y)
plot(res3$t[validos], res3$y[validos], type = "l", col = "purple", lwd = 2,
     main = "Problema 3: Runge-Kutta de 3er Orden", xlab = "x", ylab = "y(x)")
```






# Ejercicio 8

::: {.callout-note title="Instrucción del ejercicio 8"}

La idea de este ejercicio es introducir la **matriz exponencial** $e^A$ para resolver sistemas de ecuaciones diferenciales.

**Definición:**  
Dada una sucesión $\{C_k\}$ de matrices $m \times n$ cuyos elementos son reales o complejos, se denota por $c_{ij}^k$ la entrada $ij$ de $C_k$.  
Entonces, si todas las $mn$ series

$$
\sum\limits_{k=1}^{\infty} c_{ij}^k \quad \text{con } i = 1, 2, \ldots, m; \; j = 1, 2, \ldots, n
$$

son convergentes, se dice que la serie de matrices $\sum\limits_{k=1}^{\infty} C_k$ es convergente y su suma es la matriz cuya entrada $ij$ es:

$$
\sum\limits_{k=1}^{\infty} c_{ij}^k.
$$

(a) Pruebe que si $\{C_k\}$ es una sucesión de matrices $m \times n$ tales que  
$\sum\limits_{k=1}^{\infty} \|C_k\|$ converge, entonces la serie de matrices $\sum\limits_{k=1}^{\infty} C_k$ también es convergente.

(b) Pruebe que la serie $\sum\limits_{k=0}^{\infty} \frac{A^k}{k!}$ es convergente.

::: {.callout-note title="Definición"}
Dada una matriz $A$, $n \times n$ con elementos reales o complejos, se define la **matriz exponencial** como:

$$
e^A = \sum\limits_{k=0}^{\infty} \frac{A^k}{k!}.
$$
:::

(c) Verifique que si 

$$
A = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 3
\end{pmatrix}
$$

  entonces 

$$
e^A =
\begin{pmatrix}
e^t & 0 & 0 \\
0 & e^{2t} & 0 \\
0 & 0 & e^{3t}
\end{pmatrix}.
$$

(e) Verifique que si 

$$
A = 
\begin{pmatrix}
a & 1 \\
0 & a
\end{pmatrix}
$$

  entonces 

$$
e^{tA} =
\begin{pmatrix}
e^{at} & te^{at} \\
0 & e^{at}
\end{pmatrix}.
$$

(f) Pruebe que para todo $t \in \mathbb{R}$ la función matricial $E(t) = e^{tA}$ satisface la ecuación diferencial matricial:

$$
E'(t) = E(t)A = AE(t).
$$

(g) Dada una matriz $A$, $n \times n$ con elementos reales o complejos, pruebe que para todo $t \in \mathbb{R}$ se cumple:

$$
e^{tA} e^{-tA} = I
$$

  o sea que $e^{tA}$ es no singular y su inversa es $e^{-tA}$.


(h) Pruebe que si $A$ y $B$ son dos matrices $n \times n$ con elementos reales o complejos tales que $AB = BA$,  
entonces:

$$
e^{A+B} = e^A e^B.
$$

(i) Pruebe que, dada una matriz $A$, $n \times n$ con elementos reales o complejos y un vector $B$ de $n$ entradas, entonces el problema de valor inicial:

$$
Y'(t) = AY(t)
$$
$$
Y(0) = B
$$

  tiene solución única con $t \in \mathbb{R}$ y que esta solución está dada por:

$$
Y(t) = e^{tA} B.
$$

(j) Pruebe que si una matriz cuadrada $A$ es diagonalizable, es decir, que existe una matriz $C$ no singular tal que la matriz $D = C^{-1}AC$ es diagonal, entonces:

$$
e^{tA} = C e^{tD} C^{-1}.
$$

(k) Escriba una función en **R** que reciba una matriz $A$ diagonalizable y retorne $e^{tA}$.  
Para esto puede usar la función `eigen(...)` de R.

(l) Verifique que si

$$
A =
\begin{pmatrix}
3 & -1 \\
-2 & 2
\end{pmatrix}
$$

  entonces

$$
e^{tA} = -\frac{1}{3}
\begin{pmatrix}
-e^t - 2e^{4t} & -e^t + e^{4t} \\
-2e^t + 2e^{4t} & -2e^t - e^{4t}
\end{pmatrix}.
$$

(m) Resuelva el siguiente sistema de ecuaciones diferenciales:

$$
\begin{cases}
x_1'(t) = 3x_1(t) - x_2(t) \\
x_2'(t) = -2x_1(t) + 2x_2(t)
\end{cases}
$$

  sujeto a $x_1(0) = 90$ y $x_2(0) = 150$.

(n) Escriba una función en **R** que reciba una matriz $A$ con los coeficientes de un sistema de ecuaciones diferenciales, las condiciones iniciales en una lista de pares, y retorne una lista con las soluciones del sistema de ecuaciones diferenciales.

(o) Usando el programa, resuelva el siguiente sistema de ecuaciones diferenciales:

$$
\begin{cases}
x_1'(t) = 3x_1(t) - 2x_2(t) \\
x_2'(t) = -2x_1(t) + 3x_2(t) \\
x_3'(t) = 5x_3(t)
\end{cases}
$$

  sujeto a $x_1(0) = 2$, $x_2(0) = 1$ y $x_3(0) = 3$.

:::

**Solución**

*(Prueba de la propiedad de conmutatividad para exponentiales de matrices mediante expansión de series de potencias; demostración de la existencia y unicidad de la solución $Y(t) = e^{tA}B$ aplicando el teorema de Picard–Lindelöf; derivación de la forma general de $e^{tA}$ para matrices diagonalizables; implementación en R usando `eigen()` y validación con ejemplos concretos. Finalmente, resolución de los sistemas de ecuaciones diferenciales con $e^{tA}$ y graficación de las trayectorias de las soluciones.)*

a)

Queremos ver que $\sum_{k=1}^{\infty}c_{ij}^k$ converge

Tome la suma parcial entre dos indices $p$ y $q$ ($p < q$). Note, por desigualdad triangular, que:

$$|\sum_{k=p}^{q}c_{ij}^k|\leq \sum_{k=p}^{q}|c_{ij}^k|$$

Ahora nos apoyamos en el hecho de que cada entrada $|c_{ij}|$ de la matriz está acotada por la norma de la matriz total: $c_{ij} \leq ||C||$ $\forall i, j$

$$\Rightarrow \sum_{k=p}^{q}|c_{ij}^k| \leq \sum_{k=p}^{q}||C_k||$$

Dado que $\sum_{k=p}^{q}||C_k||$ converge, entonces es de Cauchy, por lo cual:

$\forall \epsilon>0 \exists N_0\in \mathbb{N}:N_0<p<q \Rightarrow \sum_{k=p}^{q}||C_k|| < \epsilon$

Juntando todo esto, se concluye que $\sum_{k=1}^{n}c_{ij}^k$ es una sucesion de Cauchy, y dado que "vive" en un espacio completo ($\mathbb{R}$ o $\mathbb{C}$), entonces converge.

Asi pues, dado que hay convergencia en cada entrada de la matriz, podemos ver que $\sum_{k=1}^{\infty}C_k$ converge.

b)

Basta con demostrar que $\sum_{k=0}^{\infty}||\dfrac{A^k}{k!}||$ converge por lo visto en el inciso anterior.

Note que, en el caso de una norma matricial, se tiene que $||\alpha \cdot A|| = |\alpha|\cdot||A||$ y $||AB||\leq||A||\cdot||B||$

$$\Rightarrow ||\dfrac{A^k}{k!}||\leq\dfrac{||A^k||}{k!}$$
$$\Rightarrow \sum_{k = 0}^{\infty}||\dfrac{A^k}{k!}||\leq \sum_{k = 0}^{\infty}\dfrac{||A^k||}{k!} = e^{||A||} < \infty$$
Por lo tanto, $\sum_{k = 0}^{\infty}||\dfrac{A^k}{k!}||$ converge, lo que significa que $\sum_{k = 0}^{\infty}\dfrac{A^k}{k!}$ converge

c)

$$
e^{tA}
= \sum_{k=0}^{\infty} \frac{(tA)^k}{k!}
= \sum_{k=0}^{\infty} \frac{1}{k!}
\begin{pmatrix}
1 & 0 & 0\\
0 & 2t & 0\\
0 & 0 & 3t
\end{pmatrix}^{k}
= \sum_{k=0}^{\infty}
\begin{pmatrix}
\frac{1^k}{k!} & 0 & 0\\
0 & \frac{(2t)^k}{k!} & 0\\
0 & 0 & \frac{(3t)^k}{k!}
\end{pmatrix}.
$$
Esta ultima igualdad es valida al estar trabajando con una matriz diagonal (de lo contrario no seria tan facil como "solo meter el exponente a cada entrada")
$$
= 
\begin{pmatrix}
\displaystyle \sum_{k=0}^{\infty} \frac{1^k}{k!} & 0 & 0\\[8pt]
0 & \displaystyle \sum_{k=0}^{\infty} \frac{(2t)^k}{k!} & 0\\[8pt]
0 & 0 & \displaystyle \sum_{k=0}^{\infty} \frac{(3t)^k}{k!}
\end{pmatrix}
=
\begin{pmatrix}
e^t & 0 & 0\\
0 & e^{2t} & 0\\
0 & 0 & e^{3t}
\end{pmatrix}.
$$
e)

**Proposición.** Para todo $n \in \mathbb{N}$,
$$
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}^n
=
\begin{pmatrix}
a^n & n\,a^{\,n-1}\\
0 & a^n
\end{pmatrix}.
$$

**Demostración**

**Caso base (n = 1(.**
$$
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}^1
=
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}
=
\begin{pmatrix}
a^1 & 1\cdot a^0\\
0 & a^1
\end{pmatrix}.
$$

**Hipótesis inductiva.** Suponga que para algún $n \ge 1$,
$$
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}^n
=
\begin{pmatrix}
a^n & n\,a^{\,n-1}\\
0 & a^n
\end{pmatrix}.
$$

**Paso inductivo.**
$$
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}^{n+1}
=
\left(
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}^n
\right)
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}
=
\begin{pmatrix}
a^n & n\,a^{\,n-1}\\
0 & a^n
\end{pmatrix}
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}.
$$

La multiplicación da:
$$
\begin{pmatrix}
a^{n+1} & a^n + n\,a^n\\
0 & a^{n+1}
\end{pmatrix}
=
\begin{pmatrix}
a^{n+1} & (n+1)\,a^{\,n}\\
0 & a^{n+1}
\end{pmatrix}.
$$

**Conclusión.** Por el principio de inducción, la fórmula es válida para todo $n \in \mathbb{N}$

Ahora, procedemos con la demostracion solicitada:

Partimos de la definición de la exponencial de matrices:
$$
e^{tA}=\sum_{k=0}^{\infty}\frac{(tA)^k}{k!}.
$$

Sea
$$
A = \begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}.
$$

Entonces
$$
(tA)^k
=
t^k
\begin{pmatrix}
a & 1\\
0 & a
\end{pmatrix}^k
=
t^k
\begin{pmatrix}
a^k & k\,a^{k-1}\\
0 & a^k
\end{pmatrix}
$$
(donde usamos el resultado probado por inducción).

Sustituimos en la serie:
$$
e^{tA}
=
\sum_{k=0}^{\infty}
\frac{t^k}{k!}
\begin{pmatrix}
a^k & k\,a^{k-1}\\
0 & a^k
\end{pmatrix}.
$$

Esto equivale a sumar componente a componente:
$$
e^{tA}
=
\begin{pmatrix}
\displaystyle \sum_{k=0}^{\infty}\frac{(ta)^k}{k!}
&
\displaystyle \sum_{k=0}^{\infty}\frac{k\,t^k\,a^{k-1}}{k!}
\\[10pt]
0
&
\displaystyle \sum_{k=0}^{\infty}\frac{(ta)^k}{k!}
\end{pmatrix}.
$$

Reconocemos series conocidas:
- La serie de la diagonal es (e^{ta}).
- Para la serie del elemento fuera de la diagonal, usamos:
  $$\sum_{k=1}^{\infty}\frac{k\, t^k \, a^{k-1}}{k!}
  = t \sum_{k=1}^{\infty}\frac{(ta)^{k-1}}{(k-1)!}
  = t e^{ta}.$$

Por tanto,
$$
e^{tA}
=
\begin{pmatrix}
e^{ta} & t\, e^{ta}\\[4pt]
0 & e^{ta}
\end{pmatrix}.
$$

f)
Definimos la función exponencial de matriz:
$$
E(t) = e^{tA} = \sum_{k=0}^{\infty} \frac{t^k A^k}{k!}.
$$

Esto es,
$$
E(t) = I + tA + \frac{t^2 A^2}{2!} + \frac{t^3 A^3}{3!} + \cdots.
$$

Note que la serie asociada a la función exponencial es uniformemente convergente, 
lo que permite derivar término por término.

Entonces:
$$
E'(t) = \sum_{k=0}^{\infty} \frac{d}{dt}\left(\frac{t^k A^k}{k!}\right)
= \sum_{k=1}^{\infty} \frac{k\, t^{k-1} A^k}{k!}.
$$

Reescribimos el coeficiente:
$$
\frac{k}{k!} = \frac{1}{(k-1)!},
$$
por lo que:
$$
E'(t) = \sum_{k=1}^{\infty} \frac{t^{k-1} A^k}{(k-1)!}.
$$

Cambiamos índice usando (m = k-1):
$$
E'(t) = \sum_{m=0}^{\infty} \frac{t^{m} A^{m+1}}{m!}.
$$

Factorizamos (A):
$$
E'(t) = A \sum_{m=0}^{\infty} \frac{t^{m} A^{m}}{m!}.
$$

La serie restante es nuevamente (E(t)), por definición. Por tanto:
$$
E'(t) = A\,E(t).
$$

Del mismo modo, como (A) conmuta con sus potencias,
$$
E'(t) = E(t)\,A.
$$

**Conclusión.**
$$
E'(t) = A\,E(t) = E(t)\,A.
$$

g)

Tomemos
$$
F(t) = e^{tA} \, e^{-tA}.
$$
Nótese que
$$
F(t) = I \quad \text{para todo } t \in \mathbb{R}.
$$

Observemos que
$$
F'(t) = A e^{tA} e^{-tA} + e^{tA}(-A)e^{-tA}
= A F(t) - F(t) A.
$$

En particular, para (t=0):
$$
F(0) = e^{0 \cdot A} \, e^{0 \cdot (-A)} = I \cdot I = I.
$$

Por lo tanto:
$$
F(0)=I.
$$

Ahora definamos
$$
G(t) = F(t) - I.
$$
Entonces
$$
G(0)=0.
$$

Además, como (I) conmuta con toda matriz, se tiene:
$$
F'(t)= A F(t) - F(t) A.
$$

Sustituyendo (F(t)=I+G(t)):
$$
F'(t)= A(I+G(t)) - (I+G(t))A.
$$

Esto se simplifica a:
$$
A + A G(t) - A - G(t)A = A G(t) - G(t)A.
$$

Por lo tanto:
$$
G'(t) = A G(t) - G(t) A,
\quad
G(0)=0.
$$

Es decir, (G) satisface una ecuación diferencial matricial homogénea con condición inicial nula.

Notemos que (G(t) = 0) es solución de dicha ecuación diferencial.  
Por el **teorema básico de existencia y unicidad de soluciones de EDO**, se concluye que la solución es única.

Entonces:
$$
G(t)=0 \quad \text{para todo } t.
$$

En consecuencia:
$$
F(t) = I \quad \text{para todo } t.
$$

h)

**Demostración**

Asumimos:
$$
A,B \in \mathbb{C}^{n\times n}\quad\text{y}\quad AB=BA.
$$

Definimos la exponencial por serie:
$$
e^{X}=\sum_{k=0}^{\infty}\frac{X^{k}}{k!}.
$$

Como A y B conmutan, aplica el binomio matricial:
$$
(A+B)^k=\sum_{r=0}^{k}\binom{k}{r}A^{\,r}B^{\,k-r}\quad (k\ge 0).
$$

Entonces:
$$
\begin{aligned}
e^{A+B}
&=\sum_{k=0}^{\infty}\frac{(A+B)^k}{k!}
=\sum_{k=0}^{\infty}\frac{1}{k!}\sum_{r=0}^{k}\binom{k}{r}A^{\,r}B^{\,k-r} \\
&=\sum_{k=0}^{\infty}\sum_{r=0}^{k}\frac{A^{\,r}}{r!}\,\frac{B^{\,k-r}}{(k-r)!}.
\end{aligned}
$$

Cambio de índices p=r y q=k−r (p,q≥0, k=p+q):
$$
e^{A+B}
=\sum_{p=0}^{\infty}\sum_{q=0}^{\infty}\frac{A^{\,p}}{p!}\,\frac{B^{\,q}}{q!}.
$$

Por convergencia absoluta, reordenamos y factorizamos:
$$
e^{A+B}
=\left(\sum_{p=0}^{\infty}\frac{A^{\,p}}{p!}\right)
 \left(\sum_{q=0}^{\infty}\frac{B^{\,q}}{q!}\right)
= e^{A}\,e^{B}.
$$

Asi pues, acabamos de demostrar que:
$$
e^{A+B}=e^{A}e^{B}.
$$

i)

Sea
$$
Y(t)=e^{tA}B.
$$

Calculamos su derivada (aqui nos apoyamos en lo encontrado en el inciso f):
$$
Y'(t)=\frac{d}{dt}(e^{tA})B = A\,e^{tA}B = A\,Y(t).
$$

Además:
$$
Y(0)=e^{0\cdot A}B = I\,B = B.
$$
Por lo tanto, $Y(t)=e^{tA}B$ cumple el problema planteado.

Para probar unicidad, supongamos por contradicción que existe otra solución Z(t) tal que:
$$
Z'(t)=A\,Z(t),\quad Z(0)=B.
$$

Definimos:
$$
W(t)=Z(t)-Y(t).
$$

Entonces:
$$
W'(t)=Z'(t)-Y'(t)=A\,Z(t)-A\,Y(t)=A(Z(t)-Y(t))=A\,W(t).
$$

La condición inicial:
$$
W(0)=Z(0)-Y(0)=B-B=0.
$$

Así, W(t) satisface:
$$
W'(t)=A\,W(t),\quad W(0)=0.
$$

La única solución de este sistema homogéneo con condición inicial cero es:
$$
W(t)=0\quad \text{para todo } t\in\mathbb{R}.
$$

Por lo tanto:
$$
Z(t)=Y(t)\quad\text{para todo }t.
$$

**Conclusión.** La solución es única y está dada por:
$$
Y(t)=e^{tA}B.
$$
j)

**Demostración**

Usamos la definición por serie:
$$
e^{tA}=\sum_{k=0}^{\infty}\frac{t^{k}}{k!}\,A^{k}.
$$

Primero probamos por inducción que
$$
A^{k}=C\,D^{k}\,C^{-1}\quad \text{para todo } k\ge 1.
$$

Caso base k=1:
$$
A^{1}=A=C\,D\,C^{-1}.
$$

Paso inductivo. Suponga cierto para k, entonces
$$
A^{k+1}=A^{k}A=(C\,D^{k}\,C^{-1})(C\,D\,C^{-1})
=C\,D^{k}\,(C^{-1}C)\,D\,C^{-1}=C\,D^{k+1}\,C^{-1}.
$$
Con ello queda probado por inducción.

Sustituyendo en la serie:
$$
\begin{aligned}
e^{tA}
&=\sum_{k=0}^{\infty}\frac{t^{k}}{k!}\,A^{k}
=\sum_{k=0}^{\infty}\frac{t^{k}}{k!}\,C\,D^{k}\,C^{-1} \\[4pt]
&= C\left(\sum_{k=0}^{\infty}\frac{t^{k}}{k!}\,D^{k}\right)C^{-1}
= C\,e^{tD}\,C^{-1}.
\end{aligned}
$$

Como D es diagonal, $e^{tD}$ es la diagonal con entradas $e^{t d_i}$, pero la identidad anterior vale en general por la linealidad de la serie.

**Conclusión.**
$$
e^{tA}=C\,e^{tD}\,C^{-1}.
$$
k)
```{r}
exp_matriz <- function(A, t) {
  eig <- eigen(A)
  C <- eig$vectors
  D <- diag(exp(t * eig$values))
  C %*% D %*% solve(C)
}

A <- matrix(c(3, -1, -2, 2), nrow = 2, byrow = TRUE)
t <- 1
exp_matriz(A, t)
```
l)
**Paso 1. Valores y vectores propios.**
$$
\chi_A(\lambda)=\det\!\begin{pmatrix}3-\lambda&-1\\-2&2-\lambda\end{pmatrix}
=(\lambda-1)(\lambda-4).
$$
Por tanto, los autovalores son (1) y (4).

Para ($\lambda=4$):
$$
A-4I=\begin{pmatrix}-1&-1\\-2&-2\end{pmatrix}\;\Rightarrow\;
v_1=\begin{pmatrix}1\\-1\end{pmatrix}.
$$
Para ($\lambda=1$):
$$
A-I=\begin{pmatrix}2&-1\\-2&1\end{pmatrix}\;\Rightarrow\;
v_2=\begin{pmatrix}1\\2\end{pmatrix}.
$$

**Paso 2. Diagonalización.**
$$
C=\begin{pmatrix}1&1\\-1&2\end{pmatrix},\qquad
D=\begin{pmatrix}4&0\\0&1\end{pmatrix},\qquad
C^{-1}=\frac13\begin{pmatrix}2&-1\\[2pt]1&1\end{pmatrix}.
$$

**Paso 3. Exponencial por semejanza.**
$$
e^{tA}=C\,e^{tD}\,C^{-1},\qquad
e^{tD}=\begin{pmatrix}e^{4t}&0\\0&e^{t}\end{pmatrix}.
$$

**Paso 4. Producto final.**
$$
\begin{aligned}
e^{tA}
&=
\begin{pmatrix}1&1\\-1&2\end{pmatrix}
\begin{pmatrix}e^{4t}&0\\0&e^{t}\end{pmatrix}
\frac13\begin{pmatrix}2&-1\\1&1\end{pmatrix}\\[6pt]
&=\frac13
\begin{pmatrix}
2e^{4t}+e^{t} & -e^{4t}+e^{t}\\
-2e^{4t}+2e^{t} & e^{4t}+2e^{t}
\end{pmatrix}.
\end{aligned}
$$

Esto es equivalente a
$$
e^{tA}
=\,-\frac13
\begin{pmatrix}
-\,e^{t}-2e^{4t} & -\,e^{t}+e^{4t}\\
-\,2e^{t}+2e^{4t} & -\,2e^{t}-e^{4t}
\end{pmatrix},
$$
que coincide con lo pedido.

ll)

**Sistema**
$$
\begin{cases}
x_1'(t)=3x_1(t)-x_2(t),\\
x_2'(t)=-2x_1(t)+2x_2(t),
\end{cases}
\qquad
x_1(0)=90,\; x_2(0)=150.
$$

**Forma matricial**
$$
X'(t)=A\,X(t),\qquad
A=\begin{pmatrix}3&-1\\-2&2\end{pmatrix},\quad
X(0)=\begin{pmatrix}90\\150\end{pmatrix}.
$$

**Exponencial de A**  (del cálculo previo)
$$
e^{tA}=\frac{1}{3}
\begin{pmatrix}
2e^{4t}+e^{t} & -e^{4t}+e^{t}\\
-2e^{4t}+2e^{t} & e^{4t}+2e^{t}
\end{pmatrix}.
$$

**Solución**
$$
X(t)=e^{tA}X(0)
=\frac{1}{3}
\begin{pmatrix}
2e^{4t}+e^{t} & -e^{4t}+e^{t}\\
-2e^{4t}+2e^{t} & e^{4t}+2e^{t}
\end{pmatrix}
\begin{pmatrix}90\\150\end{pmatrix}.
$$

**Componentes**
$$
\begin{aligned}
x_1(t)&=10e^{4t}+80e^{t},\\
x_2(t)&=-10e^{4t}+160e^{t}.
\end{aligned}
$$

**Chequeo inicial**
$$
x_1(0)=10+80=90,\qquad x_2(0)=-10+160=150.
$$

m)
```{r}
resolver.sistema.diferencial <- function(A, condiciones.iniciales, t.puntos) {
  nombres <- sapply(condiciones.iniciales, function(x) as.character(x[[1]]))
  B <- matrix(as.numeric(sapply(condiciones.iniciales, function(x) x[[2]])), ncol = 1)
  sol.mat <- sapply(t.puntos, function(t) exp_matriz(A, t) %*% B)
  if (is.vector(sol.mat)) sol.mat <- matrix(sol.mat, nrow = nrow(A))
  rownames(sol.mat) <- nombres
  soluciones <- lapply(seq_len(nrow(sol.mat)), function(i) as.numeric(sol.mat[i, ]))
  names(soluciones) <- nombres
  list(tiempos = t.puntos, soluciones = soluciones)
}

A <- matrix(c(3, -1, -2, 2), nrow = 2, byrow = TRUE)
condiciones <- list(list("x1", 90), list("x2", 150))
t.grid <- seq(0, 1, by = 0.1)
res <- resolver.sistema.diferencial(A, condiciones, t.grid)
data.frame(t = res$tiempos, do.call(cbind, res$soluciones))

```
n)
```{r}
A <- matrix(c(3,-2,0,-2,3,0,0,0,5), 3, 3, byrow = TRUE)
cond <- list(list("x1",2), list("x2",1), list("x3",3))
ts <- seq(0, 1, by = 0.1)
res <- resolver.sistema.diferencial(A, cond, ts)
data.frame(t = res$tiempos, do.call(cbind, res$soluciones))
```



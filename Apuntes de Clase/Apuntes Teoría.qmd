---
title: "Apuntes Teoría MA0501"
author: 
  - name: "Diego Alberto Vega Víquez"
    email: "diegovv13@gmail.com"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 3
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    pdf-engine: xelatex
    include-in-header:
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    execute:
      code-fold: true
      message: false
      warning: false
    theme: flatly 
    toc: true
    toc-depth: 3
    toc-location: left
    html-math-method: katex
    css: styles.css
    embed-resources: true
---
\newpage
# Fundamentos de la Programación en Lenguaje R

## Funciones

A continuación un ejemplo de una función en **R**

```{r}
#| code-fold: true
area.triangulo <- function(base,altura) {   # <1>
  area <- (base*altura)/2
  return(area)
}

area.triangulo(2,5)
```
1. Comunmente usamos el punto para separar las palabras en el nombre de las variables

### Usando For, If y While

```{r}
#| code-fold: true
encuentra.cero.f <- function(v) {
  s <- -1
  for(i in 1:length(v)) {
    if((v[i]==0) && (s == -1)) {
      s<-i
    }
  }
  if (s != -1) {
    return(s)
  }
  else {
    return("¡El vector no tiene ningún cero!")
  }
}

vec<-c(4,-7,2,1,9)
encuentra.cero.f(vec)
```

La siguiente función calcula la suma del valor absoluto de las entradas de un vector, es de decir, la norma 1 del vector dada por

$$|v[1]|+|v[2]|+\ldots+|v[n]|$$

```{r}
#| code-fold: true
norma1 <- function(v) {
   suma <- 0
   i = 1
   while (i <= length(v)) {
      suma <- suma + abs(v[i])
      i <- i + 1
   }
   return(suma)
}

vec0<-c(4,-7,2,1)
norma1(vec0)
```


# Algoritmos, aproximaciones y error

## Aproximaciones y Error

### Aritmética punto flotante


#### Punto flotante

::: {.callout-note title="Definición: Forma Punto Flotante"}

Un número real $x$ está en **forma punto flotante** si se escribe de la forma

$$
0.d_1 d_2 \cdots d_k \times 10^{n},
$$

donde $0 \le d_i \le 9$, $d_1 \ne 0$, $i = 1,2,\ldots,k$.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

$$
\pi = 0.31415 \times 10^{1}.
$$
:::

::: {.callout-important collapse="true" title="Observación"}

- En análisis numérico todas las respuestas deben darse en **notación de punto flotante**.
- Si $x \in \mathbb{R}$, entonces se escribe como  
  $$x = 0.d_1 d_2 d_3 \cdots \times 10^{n},$$
  pero el computador solo puede **almacenar una cantidad finita de dígitos** por limitaciones de memoria.

Por lo tanto, quedan **dos posibilidades**:

1. **Cortar**
   - $$x \approx {fl}(x) = 0.d_1 d_2 \cdots d_k \times 10^{n},$$
     es decir, **cortar a partir del dígito $k+1$**.

2. **Redondear y cortar**
   - Si $d_{k+1} \ge 5$ entonces **sume 1 a $d_k$** y corte.
   - Si no, **solamente corte**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

$$
\pi = 3.14159265\cdots, \quad \text{con } k = 5 \text{ tenemos:}
$$

$$
fl(\pi) = 0.31415 \times 10^{1} \quad (\textbf{Cortando})
$$

$$
fl(\pi) = 0.31416 \times 10^{1} \quad (\textbf{Redondeando})
$$
:::

::: {.callout-note title="Definición: Error de Redondeo"}

El error que resulta de reemplazar $x$ por $fl(x)$ se denomina **Error de Redondeo.**
:::

#### ¿Cómo medir, controlar, la propagación del error?

::: {.callout-note title="Definición: Error absoluto & Error relativo"}

Si $P^*$ es una aproximación de $P$ se llama:

$$
\text{Error absoluto} \;=\; |P - P^*|
$$

$$
\text{Error relativo} \;=\; \frac{|P - P^*|}{|P|}, \quad \text{para } P \neq 0
$$
:::

::: {.callout-note title="Teorema"}

Sea $x \in \mathbb{R}$, $x \neq 0$, entonces:

1. Si $fl(x)$ se obtiene usando $k$-dígitos de $x$ **cortando**, entonces:

$$
\left| \frac{x - fl(x)}{x} \right| \leq 10^{-k+1}.
$$

2. Si $fl(x)$ se obtiene usando $k$-dígitos de $x$ **redondeado**, entonces:

$$
\left| \frac{x - fl(x)}{x} \right| \leq 0.5 \times 10^{-k+1} \;=\; 5 \times 10^{-k}.
$$
:::
::: {.callout-tip collapse="true" title="Ejemplo"}

Si $P = 0.3000 \times 10^{1}$ y $P^* = 0.3100 \times 10^{1}$ entonces:

$$
|P - P^*| = 0.1 \times 10^{0}, 
\qquad \frac{|P - P^*|}{|P|} = 0.33 \times 10^{-1}
$$

Pero si $P = 0.3000 \times 10^{4}$ y $P^* = 0.3100 \times 10^{4}$ entonces:

$$
|P - P^*| = 0.1 \times 10^{3}, 
\qquad \frac{|P - P^*|}{|P|} = 0.33 \times 10^{-1}
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}

**1.**

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_1 d_2 \cdots \times 10^n - 0.d_1 d_2 \cdots d_k \times 10^n}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots \times 10^{n-k}}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
&\le \frac{1}{0.1} \times 10^{-k} \\[1ex]
&= 10^{-k+1}.
\end{align*}

---

**2.** 


Ya que ${fl}(x)$ es una aproximación de $x$ con redondeo a $k$ dígitos eso significa que podemos escribir ${fl}(x)$ de la siguiente forma

$${fl}(x) = 0.d_1 d_2 \cdots d_k \times 10^{n}$$

Sea $x\in\mathbb{R}$ que escribiremos como 

$$x=0.d_1 d_2 \cdots \times 10^{n}$$

De esta forma

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_1 d_2 \cdots \times 10^n - 0.d_1 d_2 \cdots d_k \times 10^n}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots \times 10^{n-k}}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
\end{align*}

Aquí hay que analizar por casos:

- Suponga que $d_{k+1} < 5$

En este caso basta con cortar en $d_k$ así:

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
&\leq 0.5\cdot\left| \frac{1}{0.1} \right| \times 10^{-k} \\[1ex]
&= 0.5\times 10^{-k+1}
\end{align*}

- Suponga que $d_{k+1} \ge 5$.

Recuerde que para este caso en ${fl}(x)$ pasa que $d_k$ es una unidad mayor que el $d_k$ de $x$. De esta forma se va a cumplir que $d_{j_\text{real}}$$=d_{j_\text{aproximado}}$ para $j=\{1,2,\ldots,k-1\}$ así se tiene que 

$$
|x - fl(x)| = 10^{1-k}\cdot(1-0.d_{k+1}\cdots)
$$
$$
\left| \frac{x - fl(x)}{x} \right| = \frac{10^{1-k}\cdot(1-0.d_{k+1}\cdots)}{|0.d_1 d_2 \cdots\times 10^{n}|}
$$
Vea que 
\begin{align*}
d_{k+1} \ge 5 &\implies 0.d_{k+1}\cdots\ge\frac{1}{2}\\
&\implies 1-0.d_{k+1}\cdots\le\frac{1}{2}\\
&\implies 1-0.d_{k+1}\cdots\le\frac{1}{2}
\end{align*}
Así
$$
\left| \frac{x - fl(x)}{x} \right| \le \frac{10^{1-k} \cdot 0.5}{|0.d_1 d_2 \cdots\times 10^{n}|} \le 10^{1-k} \cdot 0.5
$$
Luego, concluya que 
$$
\left| \frac{x - {fl}(x)}{x} \right| \leq 0.5 \times 10^{-k+1} \qquad \blacksquare
$$
:::

::: {.callout-note title="Definición: Dígitos significativos"}

Se dice que un número $P^*$ aproxima a $P$ con $t$ **dígitos significativos** si $t \in \mathbb{N}$ es el número más grande tal que:

$$
\frac{|P - P^*|}{|P|} < 5 \times 10^{-t} \;=\; 0.5 \times 10^{-t+1}.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


$$
x \;=\; \pi \;=\; 0.\underbrace{3141}_{\text{4 dígitos}}59265 \cdots \times 10^{1}.
$$

$$
x^* \;=\; \frac{22}{7} \;=\; 0.\underbrace{3142}_{\text{dígitos distintos}}8517 \times 10^{1}.
$$

$$
\frac{|x - x^*|}{|x|} = 0.402 \times 10^{-3} < 0.5 \times 10^{-4+1}.
$$

Por lo que $x^*$ aproxima $x$ con **4 dígitos significativos**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


¿Qué valores puede tomar $x^*$ para aproximar 1000 con **4 dígitos significativos**?  

$$
x = 1000 = 0.1000 \times 10^{4}, \quad \text{luego tenemos que:}
$$

$$
\left| \frac{x^* - 1000}{1000} \right|
< 5 \times 10^{-4}
\;\;\;\;\;\;\;\; \Rightarrow \;\;\;\;\;\;\;\;
-5 \times 10^{-4}
< \frac{x^* - 1000}{1000}
< 5 \times 10^{-4},
$$

$$
\Rightarrow \;\; 999.5 < x^* < 1000.5.
$$

Note que 
$$x^* = 0.9996 \times 10^{3}$$ 
aproxima a $x$ con **4 dígitos significativos**,  

pero 
$$y = 0.1001 \times 10^{4}$$ 
**no** aproxima a $x$ con 4 dígitos significativos.
:::

### Problemas con la aritmética punto flotante


- **División:**  
  Si $x \cong x + \varepsilon$ y se divide entre $\delta$ muy pequeño se tiene:

  $$
  \frac{x}{\delta} \cong \frac{x + \varepsilon}{\delta} = \frac{x}{\delta} + \underbrace{\frac{\varepsilon}{\delta}}_{(*)}
  $$
$(*)$: Nuevo error enorme


- **Resta de dos números casi iguales:**

\begin{align*}
  fl(x) \; &= \; 0.d_1 d_2 \cdots d_p \alpha_{p+1} \alpha_{p+2} \cdots \alpha_k \times 10^n\\
  fl(y) \; &= \; 0.d_1 d_2 \cdots d_p \beta_{p+1} \beta_{p+2} \cdots \beta_k \times 10^n\\
  x - y \; &\cong \; fl(x) - fl(y) \;=\; 0.\underbrace{\gamma_{p+1}\gamma_{p+2}\cdots\gamma_k}_{(**)} \times 10^{n-p}
\end{align*}

$(**)$: Podría ser basura.

::: {.callout-note title="Notación"}

\begin{align*}
\Delta_x \;&=\; |x - x^*|.\\
\delta_x \;&=\; \frac{|x - x^*|}{|x|} \;=\; \frac{\Delta_x}{|x|}.
\end{align*}
:::

---

::: {.callout-note title="Teorema"}


Si $x = x_1 + x_2 + \cdots + x_n$ y $x^* = x_1^* + x_2^* + \cdots + x_n^*$ con $x_i \geq 0$ entonces:

\begin{align*}
\Delta_x \;&\leq\; \sum_{i=1}^n \Delta_{x_i}. \\
\delta_x \;&\leq\; \max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\}.
\end{align*}
:::

::: {.callout-caution collapse="true" title="Prueba"}

\begin{align*}
\Delta_x &= |x - x^*|
= \left| \sum_{i=1}^n x_i - \sum_{i=1}^n x_i^* \right|
\leq \sum_{i=1}^n |x_i - x_i^*|
= \sum_{i=1}^n \Delta_{x_i}.
\end{align*}


\begin{align*}
\text{Sabemos que } \quad
\delta_x &= \frac{\Delta_x}{|x|}
\leq \frac{\Delta_{x_1} + \Delta_{x_2} + \cdots + \Delta_{x_n}}
{|x_1 + x_2 + \cdots + x_n|},\\[1ex]
\delta_{x_i} &= \frac{\Delta_{x_i}}{|x_i|}
\;\Rightarrow\; \Delta_{x_i} = \delta_{x_i}\,|x_i|,\\[2ex]
\Rightarrow \quad
\delta_x &\leq \frac{|x_1|\delta_{x_1} + |x_2|\delta_{x_2} + \cdots + |x_n|\delta_{x_n}}
{|x_1 + x_2 + \cdots + x_n|}\\[2ex]
&\leq
\frac{\max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\}\, (|x_1| + |x_2| + \cdots + |x_n|)}
{|x_1 + x_2 + \cdots + x_n|}\\[2ex]
&\leq \max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\},
\qquad \text{pues } x_i \ge 0 \ \forall i .
\end{align*}

:::

::: {.callout-important collapse="true" title="Observación"}


Se debe evitar la pérdida de dígitos significativos:

Por ejemplo, al evaluar:  

$$
f(x) = 1 - \cos(x),
$$  

con $x$ cercano a $0$ se producirá una pérdida de dígitos significativos.  
Esto se puede evitar racionalizando, como sigue:

$$
f(x) = 1 - \cos(x) \;=\; \frac{\sin^{2}(x)}{1 + \cos(x)}.
$$

:::

### Algoritmos y convergencia

#### ¿Qué es un algoritmo?

::: {.callout-note title="Definición: Algoritmo"}

Un algoritmo es un procedimiento que describe, sin ninguna ambigüedad, una sucesión finita de pasos a realizar en orden específico, con el propósito de resolver un problema.

:::

**Características:**

- Finito.  
- Definido (no ambiguo).  
- Entrada.  
- Salida.  
- Efectivo.  
- Eficiente.  

Para representar las instrucciones utilizaremos pseudocódigo.

::: {.callout-tip collapse="true" title="Ejemplo"}


Para calcular  
$$
\sum_{k=a}^{\infty} f(x,k)
$$
tenemos:

**Entrada:** $\varepsilon, f, a, x$.  

**Salida:** Valor aproximado de  
$$
\sum_{k=a}^{\infty} f(x,k).
$$

---

```{python}
#| eval: false
#| message: false
k <- a
s <- 0
T <- f(x,k)
while |T| < ε do
    s <- s + t
    t <- t * (f(x,k+1) / f(x,k))
    k <- k + 1
end while
return s
```
:::

::: {.callout-note title="Definición: Algoritmos Estables"}

Un algoritmo se dice **estable** si pequeños cambios en la entrada producen pequeños cambios en la salida.  
En caso contrario, es decir, pequeños cambios en la entrada producen grandes cambios en la salida, entonces el algoritmo se dice **inestable (caótico)**.
:::

::: {.callout-note title="Notación"}

\begin{align*}
E \;&=\; \text{Error inicial.}\\
E_n \;&=\; \text{Error luego de }n\text{ pasos.}
\end{align*}
:::

::: {.callout-note title="Definición: Crecimiento lineal del error "}

Si  
$$|E_n| = CnE,$$  
con $C$ constante, entonces el crecimiento del error es **lineal**.  
:::

::: {.callout-note title="Definición: Crecimiento exponencial del error  "}

Si  
$$|E_n| = K^n E, \quad K > 1,$$  
entonces el crecimiento del error es **exponencial**.  
:::

::: {.callout-important collapse="true" title="Observación"}
- Crecimiento del error lineal $\;\Leftrightarrow\;$ estable.  
- Crecimiento del error exponencial $\;\Leftrightarrow\;$ inestable.  
:::

::: {.callout-note title="Definición: Rapidez de convergencia"}

Sea $\{ \alpha_n \}_{n \in \mathbb{N}}$ una sucesión que converge a $\alpha$, se dice que $\{ \alpha_n \}_{n \in \mathbb{N}}$ converge con una rapidez $\mathcal{O}(\beta_n)$, donde $\{ \beta_n \}_{n \in \mathbb{N}}$ es otra sucesión ($\beta_n \neq 0 \ \forall n$) si:  

$$
\frac{|\alpha_n - \alpha|}{|\beta_n|} < K
$$  

para $n$ suficientemente grande y $K$ constante que no depende de $n$.  
:::

::: {.callout-note title="Notación"}

$$
\alpha_n = \alpha + \mathcal{O}(\beta_n).
$$  

$$
\alpha_n \;\to\; \alpha \;\;\; \text{con rapidez } \mathcal{O}(\beta_n).
$$  
:::

::: {.callout-note title="Observación"}
$$
\frac{|\alpha_n - \alpha|}{|\beta_n|} < K 
\;\;\;\Longleftrightarrow\;\;\;
- K \beta_n + \alpha < \alpha_n < K \beta_n + \alpha.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Si $\alpha_n = \dfrac{n+3}{n^3}$, entonces  

$$
\alpha_n = 0 + \mathcal{O}\!\left(\frac{1}{n^2}\right)
$$  

pues:  

$$
\left| \frac{\alpha_n - \alpha}{\beta_n} \right|
= \left| \frac{\tfrac{n+3}{n^3} - 0}{\tfrac{1}{n^2}} \right|
= \left| \frac{n^3 + 3n^2}{n^3} \right|
= \left| 1 + \frac{3}{n} \right|
\leq 4, \quad \text{si } n \to \infty.
$$  

Es decir, $\dfrac{n+3}{n^3}$ converge a $0$ tan rápido como $\dfrac{1}{n^2}$ converge a $0$.  
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Si 
$$
\alpha_n = \frac{\sin(n)}{n},
$$  
entonces  
$$
\alpha_n = 0 + \mathcal{O}\!\left(\frac{1}{n}\right), \quad \text{cuando } n \to \infty.
$$
:::

::: {.callout-note title="Observación: Velocidad de convergencia de los ciclos  "}

1\. $\texttt{For[i=1, i++, i<=N, \ldots]}$

tiene una velocidad de convergencia $\mathcal{O}(N)$.  

2\. $\texttt{For[i=1, i++, i<=N,} \quad$

  $\quad\texttt{For[j=1, j++, j<=M, \ldots]]}$
   
tiene una velocidad de convergencia $\mathcal{O}(N^2)$.  

3\. $\texttt{For[i=1, i++, i<=N,} \quad$

  $\qquad\texttt{For[j=1, j++, j<=M,} \quad$
   
  $\qquad\qquad\texttt{For[k=1, k++, k<=L, \ldots]]]}$
      
tiene una velocidad de convergencia $\mathcal{O}(N^3)$.  
:::

### Origen del Error

#### Tipos de Error

Existen dos tipos de error: El error en los datos y el error computacional

![Tipos de Error](Imagenes/Grafico Error.png){width=400px fig-align="center"}

::: {.callout-note title="Definición: Propagación del error"}

Sea $a \in \mathbb{R}$ y sea $\hat{a}$ una aproximación de $a$ y $f$ una función o procedimiento,  
entonces 

$$
f(\hat{a}) - f(a)
$$  

se llama **propagación del error** o **error propagado**.
:::
Esto se ilustra en el siguiente gráfico

AGRAGAR GRAFICO ANDREY

::: {.callout-tip collapse="true" title="Ejemplo"}


Suponga que se desea calcular $c$ en el siguiente triángulo:

![](Imagenes/Triangulo.png){width=300px fig-align="center"}

Supongamos que $a$ tiene un error y usamos $100.1$.  
Entonces el error se propaga como se muestra en la siguiente tabla:

| Expresión | Exacto | Aproximado | Error relativo |
|-----------|--------|-------------|----------------|
| $a$ | $100$ | $100.1$ | $0.1 \,\%$ |
| $b-a$ | $1$ | $0.9$ | $-10 \,\%$ |
| $(b-a)^2$ | $1$ | $0.81$ | $-19 \,\%$ |
| $4ab \sin^2\!\left(\tfrac{\gamma}{2}\right)$ | $3.0765\ldots$ | $3.0796\ldots$ | $0.1 \,\%$ |
| $(b-a)^2 + 4ab \sin^2\!\left(\tfrac{\gamma}{2}\right)$ | $4.0765\ldots$ | $3.8896\ldots$ | $-4.6 \,\%$ |
| $c$ | $2.0190\ldots$ | $1.9722\ldots$ | $-2.3 \,\%$ |
| **Error relativo inicial:** | $0.1 \,\%$  | **Error relativo final:** | $-2.3 \,\%$ |

: {tbl-colwidths="[35,20,25,20]"}

:::

::: {.callout-tip collapse="true" title="Ejemplo: Inestabilidad Numérica"}


Suponga que se desea calcular  

$$
z = 1.000 - \frac{1.208}{x}
$$  

en una computadora que solamente utiliza 4 dígitos, con $x = 1.209$.

**Algoritmo 1.**  
Calcule primero $y := \dfrac{1.208}{x}$ y luego $z := 1.000 - y$  

![](Imagenes/1 graph.png){width=350px fig-align="center"}

**Algoritmo 2.**  
Calcule primero $y := x - 1.208$ y luego $z := \dfrac{y}{x}$  

![](Imagenes/2 graph.png){width=350px fig-align="center"}
:::

#### Principio Básico:

Si es posible, evite operaciones sensibles con operandos contaminados por la propagación del error.

::: {.callout-tip collapse="true" title="Ejemplo: Raíces cuadráticas y propagación del error"}

Usualmente, para calcular las raíces de una ecuación cuadrática se usa:

$$
x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},
$$

pero si $b > 0$ es mejor usar:

$$
x_1 = \frac{-2c}{-b - \sqrt{b^2 - 4ac}},
$$

y si $b < 0$ es mejor usar:

$$
x_2 = \frac{2c}{-b + \sqrt{b^2 - 4ac}}.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo: Evitar cancelación numérica"}

Otros ejemplos que evitan problemas de cancelación cuando $x \approx y$ son:

\begin{align*}
x^2 - y^2 \;\;&\longrightarrow\;\; (x-y)(x+y),\\
\cos(x) - 1 \;\;&\longrightarrow\;\; 2 \sin^2\!\left(\tfrac{x}{2}\right), \quad \text{para } x \to 0,\\
\ln(x) - \ln(y) = \ln\!\left(\tfrac{x}{y}\right)&\longrightarrow\; 2 \tanh^{-1}\!\left(\tfrac{x-y}{x+y}\right),\\
e^x - e^y \;\;&\longrightarrow\;\; 2 \sinh\!\left(\tfrac{x-y}{2}\right) e^{\tfrac{x+y}{2}}.
\end{align*}
:::  

# Elementos de análisis funcional

## Elementos de Análisis Funcional para Análisis Numérico

### Espacios Normados  

::: {.callout-note title="Definición: Espacio normado"}

Sea $X$ un espacio vectorial complejo (o real).  
Una función $\|\cdot\| : X \to \mathbb{R}$ con las siguientes propiedades:

1. $\|x\| \geq 0,$  
2. $\|x\| = 0$ si y solamente si $x = 0,$  
3. $\|\alpha x\| = |\alpha| \, \|x\|,$  
4. $\|x + y\| \leq \|x\| + \|y\|,$  

para todo $x, y \in X$ y para todo $\alpha \in \mathbb{C}$ (o $\mathbb{R}$),  
se llama **norma** en $X$.  

El espacio vectorial $X$ provisto de una norma se llama **espacio normado**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Algunas normas de $\mathbb{R}^n$ y $\mathbb{C}^n$ son:

$$
\|x\|_1 := \sum_{i=1}^n |x_i|, 
\qquad
\|x\|_2 := \left( \sum_{i=1}^n |x_i|^2 \right)^{1/2}, 
\qquad
\|x\|_\infty := \max_{i=1,2,\ldots,n} |x_i|,
$$

para $x = (x_1, x_2, \ldots, x_n)^t$.

Las normas anteriores son conocidas como las normas $\ell_1$, $\ell_2$ y $\ell_\infty$ respectivamente.  
Las tres son casos particulares de la **norma $\ell_p$**:

$$
\|x\|_p := \left( \sum_{i=1}^n |x_i|^p \right)^{1/p},
\qquad p \geq 1.
$$

La norma $\ell_\infty$ es el límite de $\ell_p$ cuando $p \to \infty$.
:::

::: {.callout-note title="Proposición"}

Para toda norma se tiene la **segunda desigualdad triangular**:

$$
\big| \|x\| - \|y\| \big| \leq \|x - y\|, 
$$

para todo $x, y \in X$.
:::

::: {.callout-caution collapse="true" collapsed="true" title="Prueba"}

De la desigualdad triangular se tiene que

$$
\|x\| = \|x - y + y\| \leq \|x - y\| + \|y\|,
$$

por lo tanto

$$
\|x\| - \|y\| \leq \|x - y\|.
$$

Análogamente, cambiando el rol de $x$ y $y$, se obtiene

$$
\|y\| - \|x\| \leq \|y - x\|,
$$

y por lo tanto se cumple la desigualdad.
:::

::: {.callout-note title="Definición: Norma"}

Para dos elementos $x,y$ en un espacio normado $X$ la norma $\|x-y\|$ se llama **distancia** entre $x$ y $y$.
:::

::: {.callout-note title="Definición: Sucesión Convergente"}

Una sucesión $(x_n)$ de elementos en un espacio normado $X$ se llama **convergente** si existe un elemento $x \in X$ tal que:

$$
\lim_{n \to \infty} \|x_n - x\| = 0,
$$

es decir, para todo $\varepsilon > 0$ existe un entero $N(\varepsilon)$ tal que $\|x_n - x\| < \varepsilon$ para todo $n > N(\varepsilon)$.  

El elemento $x$ es llamado el **límite** de la sucesión $(x_n)$ y se escribe:

$$
\lim_{n \to \infty} x_n = x.
$$

Si una sucesión no converge se llama **divergente**.
:::

::: {.callout-note title="Proposición"}

El límite de una sucesión convergente es **único**.
:::

::: {.callout-caution collapse="true" title="Prueba"}

*(Ejercicio)* $\blacksquare$
:::

::: {.callout-note title="Definición: Normas Equivalentes"}
Dos normas en un espacio vectorial se llaman **equivalentes** si tienen el mismo conjunto de sucesiones convergentes.
:::

::: {.callout-note title="Teorema"}

Dos normas $\|\cdot\|_{a}$ y $\|\cdot\|_{b}$ en un espacio vectorial $X$ son equivalentes si y solamente si existen números positivos $c$ y $C$ tal que:
$$
c \|x\|_{a} \;\leq\; \|x\|_{b} \;\leq\; C \|x\|_{a}, \quad \forall x \in X.
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sean las normas $\|\cdot\|_{a}$ y $\|\cdot\|_{b}$ equivalentes. Suponga que no existe $C>0$ tal que 
$\|x\|_{b} \leq C \|x\|_{a}$ para todo $x \in X$, entonces existe una sucesión $(x_{n})$ con $\|x_{n}\|_{a} = 1$ y $\|x_{n}\|_{b} \geq n^{2}$.  
Luego la sucesión $y_{n} := x_{n}/n$ converge a cero con respecto a $\|\cdot\|_{a}$ pero no con respecto a $\|\cdot\|_{b}$, porque $\|y_{n}\|_{b} \geq n$.  

Recíprocamente, si se tiene (2), entonces si $\|x_{n}-x\|_{a}\to 0$ es claro que $\|x_{n}-x\|_{b}\to 0$ y viceversa.  
$\blacksquare$
:::

::: {.callout-note title="Teorema"}

En un espacio vectorial de dimensión finita todas las normas son equivalentes.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Ejercicio. $\blacksquare$
:::

::: {.callout-note title="Definición: Subconjunto cerrado"}
- Un subconjunto $U$ de un espacio normado se llama **cerrado** si este contiene el límite de todas las sucesiones convergentes de $U$.  

- La **clausura** $\overline{U}$ de un conjunto $U$ de un espacio normado $X$ es el conjunto de todos los límites de sucesiones convergentes de $U$.  

- Un subconjunto $U$ de $X$ se llama **abierto** si su complemento $X \setminus U$ es cerrado.  

- Un conjunto $U$ se llama **denso** en otro conjunto $V$ si $V \subset \overline{U}$, es decir, si cada elemento de $V$ es el límite de una sucesión convergente de $U$.  

- Para cada $x_{0} \in X$ y $r > 0$ el conjunto  
  $$
  B[x_{0}, r] := \{x \in X \;:\; \|x - x_{0}\| \leq r\}
  $$  
  es cerrado y se llama la **bola cerrada** de radio $r$ y centro $x_{0}$.  

  El conjunto  
  $$
  B(x_{0}, r) := \{x \in X \;:\; \|x - x_{0}\| < r\}
  $$  
  es abierto y se llama la **bola abierta** de radio $r$ y centro $x_{0}$.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

- **Cerrado.** $[0,1] \subset \mathbb{R}$ es cerrado: toda sucesión convergente de puntos en $[0,1]$ converge a un punto en $[0,1]$.

- **Clausura.** Si $U = ]0,1[$, entonces $\overline{U} = [0,1]$.  
  Si $U = \mathbb{Q}$, entonces $\overline{U} = \mathbb{R}$.

- **Abierto.** $]0,1[ \subset \mathbb{R}$ es abierto porque su complemento es  
  $\mathbb{R} \setminus ]0,1[ = ]-\infty,0] \cup [1,\infty[$.

- **Denso en $V$.** $\mathbb{Q}$ es denso en $\mathbb{R}$ ($\overline{\mathbb{Q}} = \mathbb{R}$).  
  También $]0,1[$ es denso en $[0,1]$ pues $\overline{]0,1[} = [0,1]$.

- **Bolas (norma usual).**  
  En $\mathbb{R}$, con $x_0 = 2$, $r = 3$:  
  $$
  B[2,3] = \{ x \in \mathbb{R} : |x-2| \leq 3 \} = [-1,5],
  $$
  $$
  B(2,3) = \{ x \in \mathbb{R} : |x-2| < 3 \} = ]-1,5[.
  $$

  En $\mathbb{R}^2$, con $x_0 = (1,2)$, $r = 2$:  
  $$
  B[(1,2),2] = \{ (x,y) \in \mathbb{R}^2 : \sqrt{(x-1)^2+(y-2)^2} \leq 2 \},
  $$
  $$
  B((1,2),2] = \{ (x,y) \in \mathbb{R}^2 : \sqrt{(x-1)^2+(y-2)^2} < 2 \}.
  $$
:::

::: {.callout-note title="Definición"}
Un conjunto $U$ se llama **acotado** si existe un número positivo $C$ tal que  
$$
\|x\| < C \quad \text{para todo } x \in U.
$$
:::

::: {.callout-note title="Teorema"}

Toda sucesión acotada en un espacio normado de dimensión finita $X$ contiene una subsucesión convergente.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $u_{1}, u_{2}, \ldots, u_{n}$ una base de $X$ y sea $(x_{\nu})$ una sucesión acotada. Entonces se puede escribir:
$$
x_{\nu} = \sum_{j=1}^{n} \alpha_{j\nu} u_{j}.
$$

Como $x_{\nu}$ es una sucesión acotada y usando la norma
$$
\|x_{\nu}\|_{\infty B} := \max_{j=1,2,\ldots,n} |\alpha_{j\nu}|,
$$
se tiene que cada una de las sucesiones $(\alpha_{j\nu})$ es acotada en $\mathbb{C}$ para cada $j=1,2,\ldots,n$.  

Por lo tanto, usando el teorema de Bolzano–Weierstrass se puede seleccionar una subsucesión $\alpha_{j\nu(\ell)} \to \alpha_{j}$ cuando $\ell \to \infty$ para cada $j=1,2,\ldots,n$.  

Esto implica que:
$$
x_{\nu(\ell)} \to \sum_{j=1}^{n} \alpha_{j} u_{j} \in X, \quad \text{cuando } \ell \to \infty.
$$
$\blacksquare$
:::

### Productos escalares  

::: {.callout-note title="Definición: Producto interno y espacio pre-Hilbert"}

Sea $X$ un espacio vectorial complejo (o real).  
Una función $\langle \cdot,\cdot \rangle : X \times X \to \mathbb{C}$ (o $\mathbb{R}$) con las siguientes propiedades:

1. $\langle x, x \rangle \ge 0,$
2. $\langle x, x \rangle = 0 \quad \text{si y sólo si } x = 0,$
3. $\langle x, y \rangle = \overline{\langle y, x \rangle},$
4. $\langle \alpha x + \beta y, z \rangle = \alpha \langle x, z \rangle + \beta \langle y, z \rangle, \quad \forall\, x,y,z \in X,\; \alpha,\beta \in \mathbb{C}\; (\text{o } \mathbb{R}),$

se llama **producto interno** en $X$.  
Un espacio vectorial $X$ provisto de un producto interno se llama **espacio pre-Hilbert**.
:::

::: {.callout-important collapse="true" title="Observación"}

Una consecuencia inmediata de 3. y 4. es la **antilinealidad**:
$$
\langle x, \alpha y + \beta z \rangle = \overline{\alpha} \langle x, y \rangle + \overline{\beta} \langle x, z \rangle.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Un ejemplo de producto interno en $\mathbb{C}^n$ (o $\mathbb{R}^n$) está dado por:
$$
\langle x, y \rangle := \sum_{i=1}^{n} x_i \overline{y_i},
$$
donde $x := (x_1, x_2, \ldots, x_n)^t$ y $y := (y_1, y_2, \ldots, y_n)^t$.
:::

::: {.callout-note title="Teorema"}

Para todo producto interno se tiene la desigualdad de Cauchy–Schwarz:
$$
|\langle x, y \rangle|^2 \leq \langle x, x \rangle \langle y, y \rangle,
$$
para todo $x, y \in X$. Además se tiene igualdad si para todo $x, y$ son linealmente dependientes.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Si $x = 0$ la desigualdad es trivial.  
Si $x \neq 0$, tome
$$
z = y - \frac{\langle y, x \rangle}{\|x\|^2}x,
$$
luego es claro que $\langle z, x \rangle = 0$ y que:
$$
0 \leq \|z\|^2
= \left\langle y - \frac{\langle y, x \rangle}{\|x\|^2}x , \; y - \frac{\langle y, x \rangle}{\|x\|^2}x \right\rangle
= \langle y, y \rangle - \frac{\langle y, x \rangle \langle x, y \rangle}{\|x\|^2}
= \|y\|^2 - \frac{|\langle x, y \rangle|^2}{\|x\|^2},
$$
de donde se tiene la desigualdad. Además se tiene igualdad si para todo $x, y$ son linealmente dependientes (ejercicio).
$\blacksquare$
:::

::: {.callout-note title="Teorema"}

Sea $X$ un espacio vectorial complejo (o real).  
Entonces la función:
$$
\|x\| := \langle x, x \rangle^{1/2}
$$
define una norma en $X$, es decir, un espacio pre–Hilbert es siempre un espacio normado.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Ejercicio (use la desigualdad de Cauchy–Schwarz para probar la desigualdad triangular).  
$\blacksquare$
:::

::: {.callout-note title="Definición: Elementos Ortogonales"}

- Dos elementos $x$ y $y$ de un espacio pre–Hilbert se llaman **ortogonales** si:
$$
\langle x, y \rangle = 0.
$$

- Dos subconjuntos $U$ y $V$ se llaman **ortogonales** si $\langle u, v \rangle = 0$ para todo $u \in U$ y $v \in V$.  

- Si dos elementos son ortogonales se denota $x \perp y$ y si dos conjuntos son ortogonales se denota $U \perp V$.  

- Un subconjunto $U \subseteq X$ se llama un **sistema ortogonal** si $\langle x, y \rangle = 0$ para todo $x, y \in U$ con $x \neq y$.  

- Un sistema ortogonal $U$ de $X$ se llama **ortonormal** si $\|x\| = 1$ para todo $x \in U$.
:::

::: {.callout-note title="Teorema"}

Los elementos de un sistema ortogonal son linealmente independientes.
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $\{q_{1},q_{2},\ldots,q_{n}\}$ un sistema ortogonal.  
Si 
$$
\sum_{k=1}^{n} \alpha_{k} q_{k} = 0,
$$
y se multiplica a ambos lados por $q_{j}$, es inmediato que $\alpha_{j} = 0$ para todo $j=1,2,\ldots,n$.  
$\blacksquare$
:::

::: {.callout-note title="Teorema  [Gram–Schmidt]"}

Sea $\{u_{0},u_{1},\ldots\}$ un conjunto finito o numerable de elementos linealmente independientes de un espacio de pre–Hilbert.  
Entonces existe un sistema ortogonal único $\{q_{0},q_{1},\ldots\}$ de la forma:

$$
q_{n} = u_{n} + r_{n}, \quad \text{para } n = 0,1,\ldots,
\tag{3}
$$

con $r_{0}=0$ y $r_{n} \in \operatorname{gen}\{u_{0},u_{1},\ldots,u_{n-1}\}$ para $n=1,2,\ldots$.  
Además se tiene que:

$$
\operatorname{gen}\{u_{0},u_{1},\ldots,u_{n}\} 
= 
\operatorname{gen}\{q_{0},q_{1},\ldots,q_{n}\},
\quad \text{para } n=0,1,\ldots
\tag{4}
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}


Asumamos que hemos construido elementos de la forma (3) con la propiedad (4) hasta $q_{n-1}$.  
Por la propiedad (4) los elementos $\{q_{0},q_{1},\ldots,q_{n-1}\}$ son linealmente independientes y por lo tanto $\|q_{k}\|\neq 0$ para $k=0,1,\ldots,n-1$.  
Por lo tanto:

$$
q_{n} = u_{n} - \sum_{k=0}^{n-1}\frac{\langle u_{n},q_{k}\rangle}{\langle q_{k},q_{k}\rangle}q_{k},
$$

está bien definido, y usando la hipótesis de inducción es fácil notar que  

$$
\langle q_{n},q_{k}\rangle = 0, \quad \text{para } k=0,1,\ldots,n-1.
$$

Es claro que  

$$
r_{n}=\sum_{k=0}^{n-1}\frac{\langle u_{n},q_{k}\rangle}{\langle q_{k},q_{k}\rangle}q_{k}
\;\;\in\;\;
\operatorname{gen}\{q_{0},q_{1},\ldots,q_{n-1}\}
=
\operatorname{gen}\{u_{0},u_{1},\ldots,u_{n-1}\}.
$$

La unicidad queda de ejercicio al lector. $\blacksquare$
:::

### Completitud  


::: {.callout-note title="Definición: Sucesión de Cauchy"}

Una sucesión de elementos en un espacio normado $X$ se llama **sucesión de Cauchy** si para todo $\varepsilon > 0$ existe un $N(\varepsilon) \in \mathbb{N}$ tal que:

$$
\|x_n - x_m\| < \varepsilon,
$$

para todo $n, m \geq N(\varepsilon)$, es decir, si  

$$
\lim_{n \to \infty, \; m \to \infty} \|x_n - x_m\| = 0.
$$
:::

::: {.callout-note title="Teorema"}

Toda sucesión convergente es de Cauchy
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $x_n \to x$ cuando $n \to \infty$. Entonces, para todo $\varepsilon > 0$ existe un $N(\varepsilon) \in \mathbb{N}$ tal que  

$$
\|x_n - x\| < \tfrac{\varepsilon}{2}, \quad \text{para todo } n \geq N(\varepsilon).
$$

Ahora, usando la desigualdad triangular:  

$$
\|x_n - x_m\| 
= \|x_n - x + x - x_m\|
\leq \|x_n - x\| + \|x_m - x\| < \varepsilon,
$$

para todo $n, m \geq N(\varepsilon)$. $\;\blacksquare$
:::

::: {.callout-important collapse="true" title="Observación"}

El recíproco del teorema anterior no es válido en general, por esto tiene sentido dar la siguiente definición.
:::

::: {.callout-note title="Definición: Subconjunto Completo"}
Un subconjunto $U$ de un espacio normado $X$ se llama **completo** si toda sucesión de Cauchy de elementos de $U$ converge a un elemento en $U$.  

Un espacio normado completo se llama **espacio de Banach**.  

Un espacio pre–Hilbert se llama **espacio de Hilbert** si este es un espacio completo.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

El espacio vectorial $C[a,b]$ provisto con la norma:

$$
\|f\|_\infty := \max_{x \in [a,b]} |f(x)|
$$

es un espacio de Banach.

::: {.callout-caution collapse="true" title="Prueba"}

(Ejercicio) $\blacksquare$
:::

:::

::: {.callout-tip collapse="true" title="Ejemplo"}

El espacio vectorial $C[a,b]$ provisto con la norma $L_1$:

$$
\|f\|_1 := \int_a^b |f(x)| \, dx
$$

NO es un espacio de Banach.

::: {.callout-caution collapse="true" title="Prueba"}

Es evidente que $\|f\|_1$ es una norma.  
Sin pérdida de generalidad se toma $[a,b] = [0,2]$ y se escoge:

$$
f_n(x) := 
\begin{cases}
x^n & \text{si } 0 \leq x \leq 1, \\
1   & \text{si } 1 < x \leq 2.
\end{cases}
$$

Para todo $m > n$ se tiene que:

$$
\| f_n - f_m \|_1 
= \int_0^1 \big( x^n - x^m \big) dx 
= \frac{1}{n+1} + \frac{1}{m+1} \to 0
\quad \text{cuando } n,m \to \infty,
$$

por lo tanto $(f_n)$ es una sucesión de Cauchy. 

Ahora, supongamos que la sucesión $(f_n)$ converge a una función continua $f$ con respecto a la norma $L_1$, es decir:

$$
\| f_n - f \|_1 \to 0 \quad \text{cuando } n \to \infty.
$$

Entonces:

$$
\int_0^1 |f(x)| \, dx 
\leq \int_0^1 |f(x) - x^n| \, dx + \int_0^1 x^n \, dx 
\leq \| f - f_n \|_1 + \frac{1}{n+1} \to 0
$$

cuando $n \to \infty$, de donde $f(x) = 0$ para $0 \leq x \leq 1$.

Además se tiene que

$$
\int_1^2 |f(x)-1| \, dx 
= \int_1^2 |f(x)-f_n(x)| \, dx 
\leq \| f - f_n \|_1 \to 0 \quad \text{cuando } n \to \infty,
$$

esto implica que $f(x) = 1$ para $1 \leq x \leq 2$.  
Por lo tanto $f$ no es continua, lo cual es una contradicción. $\blacksquare$
:::

:::

::: {.callout-tip collapse="true" title="Ejemplo"}

El espacio vectorial $C[a,b]$ provisto con la norma $L_2$:

$$
\| f \|_2 := \left( \int_a^b |f(x)|^2 \, dx \right)^{2}
$$

NO es un espacio de Banach.

::: {.callout-caution collapse="true" title="Prueba"}

Hay que probar que el espacio vectorial $C[a,b]$ normado provisto con la norma $L_2$ **NO** es completo. Es decir,
hay que encontrar una sucesión de Cauchy de elementos de $U$ que **NO** converge a un elemento en $U$, siendo $U$ un subconjunto del espacio vectorial $C[a,b]$

Sin pérdida de generalidad se toma $[a,b] = [0,2]$ y se escoge:

$$
f_n(x) := 
\begin{cases}
x^n & \text{si } 0 \leq x \leq 1, \\
1   & \text{si } 1 < x \leq 2.
\end{cases}
$$

Para todo $m > n$ se tiene que:

\begin{align*}
\| f_n - f_m \|_1
&= \left( \int_0^2 |f_n(x) - f_m(x)|^2 \, dx \right)^2\\[1ex]
&= \left( \int_0^1 |x^n - x^m|^2 \, dx \right)^2\\[1ex]
&= \left( \int_0^1 \big(x^{2n} - 2x^{n+m} + x^{2m}\big) \, dx \right)^2\\[1ex]
&= \left( \frac{1}{2n+1} - \frac{2}{n+m+1} + \frac{1}{2m+1} \right)^2 \to 0
\quad \text{cuando } n,m \to \infty,
\end{align*}

por lo tanto $(f_n)$ es una sucesión de Cauchy. 

Ahora, supongamos que la sucesión $(f_n)$ converge a una función continua $f$ con respecto a la norma $L_1$, es decir:

$$
\| f_n - f \|_1 \to 0 \quad \text{cuando } n \to \infty.
$$
Entonces,

\begin{align*}
\|f\|_{1} 
  &= \|(f-x^{n})+x^{n}\|_{1}
\\
  &\le \|f-x^{n}\|_{1} + \|x^{n}\|_{1}
  \qquad\text{(desigualdad triangular)}
\\
  &= \|f-f_{n}\|_{1} + \left(\int_{0}^{1} |x^{n}|^{2}\,dx\right)^{2}
\\
  &= \|f-f_{n}\|_{1} + \left(\int_{0}^{1} x^{2n}\,dx\right)^{2}
\\
  &= \|f-f_{n}\|_{1} + \left(\frac{1}{2n+1}\right)^{2}
  \xrightarrow[n\to\infty]{} 0
\end{align*}
de donde $f(x) = 0$ para $0 \leq x \leq 1$.

Vea además que 

\begin{align*}
\Biggl(\int_{1}^{2}\!\bigl|f(x)-1\bigr|^{2}\,dx\Biggr)^{2} 
&= \Biggl(\int_{1}^{2}\!\bigl|f(x)-f_n(x)\bigr|^{2}\,dx\Biggr)^{2} 
\qquad\text{(pues $f_n(x)=1$ en $(1,2]$)}\\[4ex]
&\le \Biggl(\int_{0}^{2}\!\bigl|f(x)-f_n(x)\bigr|^{2}\,dx\Biggr)^{2} 
= \,\|f-f_n\|_{1}\;\xrightarrow[n\to\infty]{}\;0 .
\end{align*}

esto implica que $f(x) = 1$ para $1 \leq x \leq 2$.  
Por lo tanto $f$ no es continua, lo cual es una contradicción. $\qquad\blacksquare$
:::
:::

::: {.callout-note title="Teorema"}

Todo espacio normado de dimensión finita es un espacio de Banach.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $X$ un espacio normado con base $u_1, u_2, \ldots, u_n$ y sea $(x_\nu)$ una sucesión de Cauchy en $X$.  
Se puede escribir:  

$$
x_\nu = \sum_{j=1}^n \alpha_{j\nu} u_j
$$

usando el Teorema 1 se tiene que existe un $C > 0$ tal que:  

$$
\max_{j=1,2,\ldots,n} |\alpha_{j\nu} - \alpha_{j\mu}| \leq C \, \| x_\nu - x_\mu \|
$$

para todo $\nu, \mu \in \mathbb{N}$.

Por lo tanto $(\alpha_{j\nu})$ es una sucesión de Cauchy en $\mathbb{C}$, entonces existen $\alpha_1, \alpha_2, \ldots, \alpha_n$ tales que $\alpha_{j\nu} \to \alpha_j$ cuando $\nu \to \infty$ para cada $j = 1,2,\ldots,n$.  
Por lo tanto $(x_\nu)$ converge a:  

$$
x_\nu \to x := \sum_{j=1}^n \alpha_j u_j \in X \quad \text{cuando } \nu \to \infty.
$$
$\blacksquare$
:::

### El teorema de punto fijo de Banach  

::: {.callout-note title="Definición: Contracción  "}

Sea $U$ un subconjunto de un espacio normado $X$.  
Un operador (una función) $A: U \to X$ se llama **una contracción** si existe una constante $q \in [0,1[$ tal que:  

$$
\|Ax - Ay\| \leq q \|x - y\|, \quad \forall\, x,y \in U.
$$

:::

::: {.callout-note title="Definición: Continuidad de un operador"}

Sea $U$ un subconjunto de un espacio normado $X$, y sea $Y$ un espacio normado.  

- Una función $A: U \to Y$ se llama **continua en $x \in U$** si para toda sucesión $(x_n)$ de $U$ tal que $\lim\limits_{n \to \infty} x_n = x$, se cumple que  

$$
\lim\limits_{n \to \infty} Ax_n = Ax.
$$

- Un operador $A: U \to Y$ se llama **continuo** si es continuo en $x$ para todo $x \in U$.
:::

::: {.callout-note title="Proposición"}

Toda contracción es un operador continuo.
:::

::: {.callout-caution collapse="true" title="Prueba"}

La prueba es evidente, puesto que si $\|x_n - x\| \to 0$ cuando $n \to \infty$ entonces  
$\|Ax_n - Ax\| \to 0$ cuando $n \to \infty$ ya que  

$$
\|Ax_n - Ax\| \leq q \|x_n - x\| \to 0 \quad \text{cuando } n \to \infty.
$$

$\blacksquare$
:::

::: {.callout-note title="Definición: Operador Lipschitz"}
Un operador $A : U \to X$ se llama *Lipschitz* con constante de Lipschitz $L$ si existe una constante positiva $L$ tal que:  

$$
\|Ax - Ay\| \leq L \|x - y\|
$$

para todo $x, y \in U$.  
Es decir, una contracción es un operador *Lipschitz* con constante menor que uno.
:::

::: {.callout-note title="Definición: Operador lineal"}
Un operador $A : X \to Y$ donde $X$ y $Y$ son espacios normados se llama **lineal** si:  

$$
A(\alpha x + \beta y) = \alpha Ax + \beta Ay
$$  

para todo $x, y \in X$ y $\alpha, \beta \in \mathbb{C}$ (o $\mathbb{R}$).
:::

::: {.callout-note title="Teorema"}

Un operador lineal es continuo si y solo si es continuo en un elemento.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $A : X \to Y$ un operador lineal continuo en $x_0 \in X$.  
Entonces, para todo $x \in X$ y para toda sucesión $(x_n) \to x$ cuando $n \to \infty$, se tiene que:

$$
Ax_n = A(x_n - x + x_0) + A(x - x_0) \;\to\; A(x_0) + A(x - x_0) = Ax,
\quad \text{cuando } n \to \infty.
$$  

::: {.callout-important collapse="true" title="Observación"}

Como $x_n - x + x_0 \to x_0$, entonces se puede aplicar el límite dentro de la expresión.
:::

$\blacksquare$
:::

::: {.callout-note title="Definición: Punto fijo"}

Un elemento $x \in X$ (espacio normado) se llama **punto fijo** de un operador 
$A : U \subseteq X \to X$ si:
$$
Ax = x.
$$
:::

::: {.callout-note title="Teorema"}

Toda contracción tiene a lo más un único punto fijo.
:::

::: {.callout-caution collapse="true" title="Prueba"}
Supongamos que $x$ y $y$ son puntos fijos de una contracción $A$, entonces
$$
0 \neq \|x - y\| = \|Ax - Ay\| \leq q \|x - y\|,
$$
lo que implica que $q \geq 1$, lo cual es una contradicción. $\blacksquare$
:::

::: {.callout-note title="Teorema: (Banach)"}

Sea $U$ un subconjunto completo de un espacio normado $X$ y sea $A : U \to U$ una contracción. Entonces $A$ tiene un punto fijo único.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $x_0 \in U$ entonces definimos recursivamente la siguiente sucesión en $U$:
$$
x_{n+1} := A x_n, \quad \text{para } n = 0,1,2,\ldots
$$

De donde se tiene que:
$$
\|x_{n+1} - x_n\| = \|Ax_n - Ax_{n-1}\| \leq q \|x_n - x_{n-1}\|,
$$

luego, por inducción se deduce que:
$$
\|x_{n+1} - x_n\| \leq q^n \|x_1 - x_0\|, \quad \text{para } n = 0,1,2,\ldots
$$

Por lo tanto para $m > n$ se tiene que:
$$
\|x_n - x_m\| \leq \|x_n - x_{n+1}\| + \|x_{n+1} - x_{n+2}\| + \cdots + \|x_{m-1} - x_m\|
$$
$$
\leq (q^n + q^{n+1} + \cdots + q^{m-1}) \|x_1 - x_0\|
$$
$$
\leq \frac{q^n}{1-q} \|x_1 - x_0\|.
$$

Como $q^n \to 0$ cuando $n \to \infty$, entonces $(x_n)$ es una sucesión de Cauchy y como $U$ es completo entonces existe $x \in U$ tal que $x_n \to x$ cuando $n \to \infty$. Finalmente, por la continuidad de $A$ se tiene que:
$$
x = \lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} A x_n = A x.
$$

La unicidad se tiene por el teorema anterior. $\blacksquare$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Pruebe que la función $f(x) = \dfrac{x^2 - 2x}{6}, \; x \in [-1,1]$ tiene un punto fijo único en $[-1,1]$.

**Solución:** Como $\mathbb{R}$ es un espacio de Banach con la norma valor absoluto, se debe probar que $f(x) \in [-1,1] \quad \forall x \in [-1,1]$.

Como 
$$
f'(x) = \tfrac{1}{6}(2x-2) = \dfrac{x-1}{3} = 0 \iff x=1
$$ 
se tiene que los máximos y mínimos posibles están en $x=-1$ o $x=1$, así el valor máximo es 
$$
f(-1)=\tfrac{1}{2}
$$ 
y el valor mínimo es 
$$
f(1)=-\tfrac{1}{6}.
$$  

Por lo que para todo $x \in [-1,1]$ se tiene que $f(x) \in [-1,1]$, o sea que $f$ tiene un punto fijo en $[-1,1]$.

Para probar la unicidad, por el teorema del valor medio la constante $L$ de Lipschitz está dada por:
$$
L = \max_{x \in [-1,1]} |f'(x)| = \max_{x \in [-1,1]} \left|\frac{x-1}{3}\right| 
= \left|\frac{-1-1}{3}\right| = \frac{2}{3} < 1.
$$

Por lo tanto $f$ es una contracción en $[-1,1]$, luego la unicidad se tiene por el teorema anterior. $\blacksquare$
:::

### El teorema de la mejor aproximación  

::: {.callout-note title="Definición: Mejor aproximación"}

Sea $U$ un subconjunto de un espacio normado $X$ y sea $w \in X$.  
Un elemento $v \in U$ se llama **la mejor aproximación a $w$ con respecto a $U$** si:

$$
\|w - v\| \;\leq\; \inf_{u \in U} \|w - u\|,
$$

es decir, $v$ es el elemento en $U$ más cercano a $w$.
:::

::: {.callout-note title="Teorema"}

Sea $U$ un subespacio de dimensión finita de un espacio normado $X$.  
Entonces, para todo elemento $w \in X$, existe una mejor aproximación con respecto a $U$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $w \in X$ y escojamos una sucesión $(u_n)$ tal que $u_n \in U$ y satisfaga lo siguiente:

$$
\lVert w - u_n \rVert \to d := \inf_{u \in U} \lVert w - u \rVert \quad \text{cuando } n \to \infty.
$$

Como 
$$
\lVert u_n \rVert \leq \lVert w - u_n \rVert + \lVert w \rVert
$$ 
entonces $(u_n)$ es una sucesión acotada.  

Por el Teorema 3 la sucesión $(u_n)$ contiene una subsucesión convergente $(u_{n(\ell)})$ con límite $v \in U$.  

Entonces:

$$
\lVert w - v \rVert = \lim_{\ell \to \infty} \lVert w - u_{n(\ell)} \rVert = d,
$$

con lo que se prueba el teorema. $\blacksquare$
:::

::: {.callout-note title="Teorema"}

Sea $U$ un subespacio vectorial de un espacio de pre-Hilbert $X$.  
Un elemento $v$ es la mejor aproximación a $w \in X$ con respecto a $U$ si y solo si:  

$$
\langle w - v, u \rangle = 0
$$  

para todo $u \in U$.  

Es decir, si y solamente si $w - v \perp U$.  
Además, para cada $w \in X$ existe a lo más una única mejor aproximación con respecto a $U$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


(Ejercicio) $\blacksquare$
:::

::: {.callout-note title="Definición: Operador Acotado"}

Un operador $A : X \to Y$ donde $X$ y $Y$ son espacios normados, se llama **acotado** si existe un número positivo $C$ tal que:  

$$
\lVert Ax \rVert \leq C \lVert x \rVert
$$  

para todo $x \in X$.
:::

::: {.callout-note title="Teorema"}

Un operador lineal $A : X \to Y$ es acotado si y solamente si:  

$$
\lVert A \rVert := \sup_{\lVert x \rVert = 1} \lVert Ax \rVert < \infty .
$$  

El número $\lVert A \rVert$ es la más pequeña cota para $A$ y se llama la **norma de $A$**.
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

Asuma $A$ es acotado con una cota $C$. Entonces  

$$
\sup_{\lVert x \rVert = 1} \lVert Ax \rVert < C < \infty ,
$$  

y entonces $\lVert A \rVert$ es menor o igual que cualquier otra cota para $A$.  

Inversamente, si $\lVert A \rVert < \infty$, entonces usando la linealidad de la norma se tiene que:  

$$
\lVert Ax \rVert 
= \left\lVert A\!\left(\frac{x}{\lVert x \rVert}\right) \right\rVert \lVert x \rVert 
\leq \lVert A \rVert \, \lVert x \rVert ,
$$  

para todo $x \neq 0$, por lo tanto $A$ es acotado con cota $C = \lVert A \rVert$.  

$\blacksquare$
:::

::: {.callout-note title="Teorema"}

Sea $U$ un subespacio vectorial completo de un espacio pre-Hilbert $X$. Entonces para cada elemento $w \in X$ existe una única mejor aproximación con respecto a $U$.  

- El operador $P : X \to U$ que le asigna a $w \in X$ su mejor aproximación es un operador lineal acotado con las siguientes propiedades:  

$$
P^2 = P 
\quad \text{y} \quad 
\lVert P \rVert = 1 .
$$  

- Este operador se conoce como la **proyección ortogonal** de $X$ sobre $U$.
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

(ejercicio) $\blacksquare$
:::

::: {.callout-note title="Corolario"}

Sea $U$ un subespacio vectorial de dimensión finita de un espacio pre-Hilbert $X$ con base $u_{1}, u_{2}, \ldots, u_{n}$.  
Entonces la combinación lineal  

$$
v = \sum_{k=1}^{n} \alpha_{k} u_{k}
$$  

es la mejor aproximación para $w \in X$ con respecto a $U$ si y solamente si los coeficientes $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ satisfacen las *ecuaciones normales*:  

$$
\sum_{k=1}^{n} \alpha_{k} \langle u_{k}, u_{j} \rangle = \langle w, u_{j} \rangle, 
\quad \text{para } j = 1, 2, \ldots, n.
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

Es evidente que la ecuación (8) es equivalente a la ecuación (7).  
$\blacksquare$
:::

::: {.callout-note title="Corolario"}

Sea $U$ un subespacio vectorial de dimensión finita de un espacio pre-Hilbert $X$ con base ortonormal $u_{1}, u_{2}, \ldots, u_{n}$.  
Entonces la proyección ortogonal está dada por:  

$$
Pw = \sum_{k=1}^{n} \langle w, u_{k} \rangle u_{k}, \quad \text{con } w \in X.
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

Es evidente de (8) puesto que  

$$
\langle u_{k}, u_{j} \rangle =
\begin{cases}
0 & \text{si } k \neq j, \\
1 & \text{si } k = j,
\end{cases}
$$  

luego $\alpha_{k} = \langle w, u_{k} \rangle$ para $k = 1, 2, \ldots, n$.  
$\blacksquare$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}
  

Sea $P_{2}[0,1]$ un subespacio de dimensión finita del espacio pre–Hilbert $C[0,1]$ dotado del producto interno  

$$
\langle f,g \rangle := \int_{0}^{1} f(x)g(x)\,dx.
$$  

Es fácil verificar que  
$$
\mathcal{B} = \{\,1, \ \sqrt{3}(2x-1), \ \sqrt{5}(6x^{2}-6x+1)\,\}
$$  
es una base ortonormal de $P_{2}[0,1]$.  

Si tomamos $f(x) = e^{x}$, entonces la mejor aproximación de $f(x)$ en $P_{2}[0,1]$ es:  

$$
Pf = \langle e^{x},1 \rangle \cdot 1 
+ \langle e^{x}, \sqrt{3}(2x-1)\rangle \cdot \sqrt{3}(2x-1) 
+ \langle e^{x}, \sqrt{5}(6x^{2}-6x+1)\rangle \cdot \sqrt{5}(6x^{2}-6x+1),
$$  

es decir,  

$$
Pf = (e-1) + \sqrt{3}(3-e)\sqrt{3}(2x-1) + \sqrt{5}(7e-19)\sqrt{5}(6x^{2}-6x+1).
$$  

Por lo tanto,  

$$
Pf \approx 1.01 + 0.85x + 0.84x^{2}.
$$

```{r}
#| echo: false
#| fig-width: 7
#| fig-height: 5
#| fig-dpi: 300
#| fig-align: center
#| fig-cap: "Gráfico de $e^x$ y aproximación cuadrática"
#| out-width: "70%"

# Datos
x <- seq(0, 1, length.out = 400)
f <- exp(x)
p <- 1.01 + 0.85*x + 0.84*x^2

# Estética básica
op <- par(mar = c(5,5,3,2))  # márgenes
on.exit(par(op), add = TRUE)

# Gráfico
plot(x, f, type = "l", lwd = 3, col = "red",
     xlab = "x", ylab = "y",
     main = "Gráficamente:\nGráfico de e^x y polinomio cuadrático")

# # Título superior (opcional, estilo como en la imagen)
# mtext("Gráficamente:", side = 3, line = 1.5, cex = 1.6, font = 2)

# Segunda curva
lines(x, p, lwd = 3, col = "blue")

# Leyenda
legend("topleft",
       legend = c(expression(e^x),
                  expression(1.01 + 0.85*x + 0.84*x^2)),
       col = c("red", "blue"), lwd = 3, bty = "n")
```

:::

::: {.callout-note title="Teorema"}

En una Regresión Múltiple, encontrar los parámetros $\beta_1, \beta_2, \ldots, \beta_p$ de tal manera que:

$$
\sum_{i=1}^n e_i^2
$$

sea mínima, es equivalente a proyectar ortogonalmente el vector $y$ sobre el espacio generado por $x_1, x_2, \ldots, x_p$.

Es decir, primero se ortonormaliza la base con *Gram–Schmidt* y luego se proyecta el vector $y$ sobre el subespacio generado por la base ortonormal $\{v_1, v_2, \ldots, v_p\}$.  
O sea, primero se aplica el método de Gram–Schmidt a la base $B = \{x_1, x_2, \ldots, x_p\}$ y se obtiene una base ortonormal $B' = \{v_1, v_2, \ldots, v_p\}$.

Así, la regresión se realiza proyectando el vector $y$ sobre el subespacio generado por la base ortonormal $B'$:

$$
\hat{y} = \langle y, v_1 \rangle v_1 + \langle y, v_2 \rangle v_2 + \cdots + \langle y, v_p \rangle v_p .
$$

Donde $\langle x, y \rangle = x \cdot y$ es el producto punto.  
Lo anterior es equivalente a hacerlo de la forma clásica:

$$
\hat{y} = X \beta = X (X^T X)^{-1} X^T y .
$$

#### Gráficamente:

<!--   -->

::: {.callout-caution collapse="true" title="Prueba"}


Es claro, pues este teorema es nada más otra forma de calcular la proyección $P$ de la variable a predecir sobre el subespacio generado por las variables predictoras. $\blacksquare$

:::

:::

# Solución numérica de ecuaciones no lineales.

## El método de aproximaciones sucesivas.

### Teoremas de convergencia y del error

::: {.callout-note title="Teorema"}

Sea $U$ un subconjunto completo de un espacio normado $X$ y $A:U \to U$ una contracción. Entonces las **aproximaciones sucesivas**:

$$
x_{n+1} = Ax_n, \quad \text{para } n = 0,1,2,\ldots,
$$

con $x_0$ arbitrario en $U$, convergen al punto fijo único $x$ de $A$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $x_0 \in U$ entonces definimos recursivamente la siguiente sucesión en $U$:

$$
x_{n+1} := Ax_n, \quad \text{para } n=0,1,2,\ldots
$$

De donde se tiene que:

$$
\|x_{n+1} - x_n\| = \|Ax_n - Ax_{n-1}\| \leq q \|x_n - x_{n-1}\|,
$$

luego por inducción se deduce que:

$$
\|x_{n+1} - x_n\| \leq q^n \|x_1 - x_0\|, \quad \text{para } n=0,1,2,\ldots
$$

Por lo tanto para $m > n$ se tiene que:

\begin{align*}
\|x_n - x_m\| &\leq \|x_n - x_{n+1}\| + \|x_{n+1} - x_{n+2}\| + \cdots + \|x_{m-1} - x_m\| \\
&\leq (q^n + q^{n+1} + \cdots + q^{m-1}) \|x_1 - x_0\| \\
&\leq \frac{q^n}{1-q} \|x_1 - x_0\|.
\end{align*}

Como $q^n \to 0$ cuando $n \to \infty$, entonces $(x_n)$ es una sucesión de Cauchy y como $U$ es completo existe $x \in U$ tal que $x_n \to x$ cuando $n \to \infty$.  

$\blacksquare$
:::

::: {.callout-note title="Corolario: Cota del Error a Priori"}

Con las mismas hipótesis del teorema anterior se tiene el siguiente *estimado para el error a priori*:

$$
\|x_n - x\| \leq \frac{q^n}{1-q} \, \|x_1 - x_0\|.
$$

::: {.callout-caution collapse="true" title="Prueba"}

Es evidente de la desigualdad (1.1).  

$\blacksquare$
:::
:::

::: {.callout-note title = "Corolario: Cota del Error a Posteriori"}
Con las mismas hipótesis del teorema anterior se tiene el siguiente *estimado para el error a posteriori*:

$$
\|x_n - x\| \leq \frac{q}{1-q} \, \|x_n - x_{n-1}\|.
$$

::: {.callout-caution collapse="true" title="Prueba"}

Se deduce del error a priori iniciando con $x_0 = x_{n-1}$.  

$\blacksquare$
:::
:::

::: {.callout-note title="Teorema [Versión 1]"}

Sea $D \subset \mathbb{R}$ un cerrado y sea $g: D \to D$ una función continuamente diferenciable con la siguiente propiedad:

$$
q := \sup_{x \in D} |g'(x)| < 1.
$$

Entonces la ecuación $g(x) = x$ tiene solución única $x \in D$ y la sucesión de aproximaciones sucesivas:

$$
x_{n+1} := g(x_n), \quad \text{para } n = 0,1,2,\ldots
$$

con $x_0$ arbitrario en $D$ converge a esta solución. Además se tiene el siguiente *estimado para el error a priori*:

$$
|x_n - x| \leq \frac{q^n}{1-q}|x_1 - x_0|,
$$

y el siguiente *estimado para el error a posteriori*:

$$
|x_n - x| \leq \frac{q}{1-q}|x_n - x_{n-1}|.
$$

Además, si $D = [a,b]$ entonces se tiene también la siguiente cota del error:

$$
|x_n - x| \leq q^n \max\{x_0 - a, b - x_0\}.
$$

::: {.callout-caution collapse="true" title="Prueba"}


El espacio $\mathbb{R}$ equipado de la norma valor absoluto $|\cdot|$ es un espacio de Banach.  
Por el teorema del valor medio, para todo $x, y \in D$ con $x < y$ se tiene que:

$$
g(x) - g(y) = g'(\xi)(x-y)
$$

para algún punto $\xi \in ]x,y[$. Por lo tanto:

$$
|g(x) - g(y)| \leq \sup_{\xi \in D} |g'(\xi)| \cdot |x-y| = q|x-y|,
$$

lo cual también es válido para $x,y \in D$ con $x \geq y$.  
Por lo tanto $g$ es una contracción, luego aplicando el Teorema de Banach o el Teorema 1 se tiene la existencia y unicidad del punto fijo.

De los corolarios 1 y 2 se tienen obviamente las desigualdades (1.2) y (1.3).  
Para probar la cota del error (1.4) note que:


\begin{align*}
|x_n - x|
&= |g(x_{n-1}) - g(x)| = |g'(\xi_1)| \cdot |x_{n-1} - x| \leq q |x_{n-1} - x| \\[1ex]
&= q |g(x_{n-2}) - g(x)| = q |g'(\xi_2)| \cdot |x_{n-2} - x| \leq q^2 |x_{n-2} - x| \\[1ex]
&\leq \ldots \\[1ex]
&\leq q^n |x_0 - x| \\[1ex]
&\leq q^n \max\{x_0 - a, \; b - x_0\}.
\end{align*}


$\blacksquare$
:::
:::

::: {.callout-note title="Teorema [Versión 2]"}

Sea $g \in C[a,b]$, con $g:[a,b] \to [a,b]$. Entonces:

1. $g$ tiene un punto fijo en $[a,b]$.

2. Además, si $g'(x)$ existe en $]a,b[$ y $|g'(x)| \leq q < 1$, para todo $x \in ]a,b[$, entonces $g$ tiene un punto fijo único en $[a,b]$.

::: {.callout-caution collapse="true" title="Prueba"}

**I Caso**: Si $g(a) = a$ o $g(b) = b$ se tiene la prueba.  

**II Caso**: Si $g(a) \neq a$ y $g(b) \neq b \Rightarrow g(a) > a$ y $g(b) < b$  

Tome $h(x) = g(x) - x$, note que:  

- $h$ es continua en $[a,b]$,  
- $h(a) = g(a) - a > 0$,  
- $h(b) = g(b) - b < 0$.  

Luego $h(a)$ y $h(b)$ tienen signos opuestos, usando el teorema de los valores intermedios se tiene que existe $x \in [a,b]$ tal que $h(x) = 0 \Rightarrow g(x) - x = 0 \Rightarrow g(x) = x$, por lo tanto $x$ es el punto fijo de $g$.  

2. Suponga que $g$ tiene dos puntos fijos en $[a,b]$, sean estos $x$ y $y$, con $x \neq y$, entonces por el Teorema del Valor Medio existe $\xi \in ]a,b[$ tal que:  

$$
|x-y| = |g(x) - g(y)| = g'(\xi)|x-y| \leq q|x-y| < |x-y|
$$

De donde $|x-y| < |x-y|$, lo cual es una contradicción, luego se tiene que $x=y$.  

$\blacksquare$
:::
:::


::: {.callout-tip collapse="true" title="Ejemplo"}

Pruebe que 

$$
f(x) = \frac{x^2 - 2x}{6}, \quad x \in [-1,1]
$$

tiene un punto fijo en $[-1,1]$.

::: {.callout-caution collapse="true" title="Solución"}

Se debe probar que $f(x) \in [-1,1]$ para todo $x \in [-1,1]$.  
Como 
$$
f'(x) = \tfrac{1}{6}(2x-2) = \frac{x-1}{3} = 0 \;\;\Leftrightarrow\;\; x=1
$$
entonces los máximos o mínimos posibles están en $x=-1$ o $x=1$.  

Como $f(-1) = \tfrac{1}{2}$ es máximo y $f(1) = -\tfrac{1}{6}$ es mínimo entonces para todo $x \in [-1,1]$ se tiene que $f(x)\in[-1,1]$, de donde $f$ tiene un punto fijo en $[-1,1]$.

Solo se ha probado que existe por lo menos un punto fijo, ahora tenemos que probar que es único.  
Se debe probar que existe $q<1$ tal que $|f'(x)| \leq q < 1$, para todo $x \in ]-1,1[$.  

Note que:

$$
|f'(x)| = \left|\frac{x-1}{3}\right| \leq \left|\frac{-1-1}{3}\right| = \tfrac{2}{3} < 1, 
\quad \text{para todo } x \in ]-1,1[.
$$

Luego $f(x)$ tiene un punto fijo único en $[-1,1]$.  
$\blacksquare$
:::

:::




::: {.callout-note title="Teorema"}


Sea $x$ un punto fijo de una función continuamente diferenciable $g$ tal que $|g'(x)|<1$.  
Entonces el método de las aproximaciones sucesivas 
$$
x_{n+1} := g(x_n), \quad \text{para } n=0,1,2,\ldots
$$
es localmente convergente, es decir, existe un vecindario $\mathcal{B}$ del punto fijo $x$ de $g$ tal que el método de aproximaciones sucesivas converge a $x$, para $x_0 \in \mathcal{B}$.

::: {.callout-caution collapse="true" title="Prueba"}


Como $g'$ es continua y $|g'(x)|<1$ entonces existe una constante $0<q<1$ y $\delta>0$ tal que $|g'(y)|<q$ para todo $y\in \mathcal{B}:=[x-\delta,x+\delta]$.  

Entonces se tiene que:

$$
|g(y)-x| = |g(y)-g(x)| \leq q|y-x| < |y-x| \leq \delta
$$

para todo $y\in \mathcal{B}$.  

Por lo que se deduce que $g$ mapea $\mathcal{B}$ en sí mismo, o sea que $g:\mathcal{B}\to \mathcal{B}$ es una contracción, por lo que el resultado se tiene del Teorema 1.  

$\blacksquare$
:::
El Teorema se ilustra en la Figura.
![El método de aproximaciones sucesivas.](Imagenes/3 graph.png)
:::


**Algoritmo**: Método de las aproximaciones sucesivas

**Entrada:** $x_0$ (aproximación inicial), $Tol$, $N$, $g(x)$  
**Salida:** $x$ (punto fijo aproximado) o mensaje de error  

**Pasos:**

1. $i \leftarrow 1$  
2. Mientras $i \leq N$, siga los pasos 3–6:  
   - $x \leftarrow g(x_0)$  
   - Si $|x - x_0| < Tol$:  
     - Salida: $x$  
     - **Parar**  
   - $i \leftarrow i+1$  
   - $x_0 \leftarrow x$  
3. Mensaje de error: *“Número máximo de iteraciones excedido”*.  
   **Parar**
   
Una implementación en R es la siguiente:
   
```{r}
punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_tem <- p0
  while (i <= n) {
    p <- g(p0_tem)
    if (abs(p - p0_tem) < tol) {
      return(p)
    }
    i <- i + 1
    p0_tem <- p
  }
  return(Inf)
}
```

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^{-x}$, es fácil probar que $f(x)$ mapea  $A = [0.5, 0.69]$ en sí mismo.

::: {.callout-caution collapse="true" title="Prueba"}


Como $f(x)$ es estrictamente decreciente y continua en $\mathbb{R}$, en particular lo es en el intervalo $A$, por lo que su imagen sobre $A$ es:
$$
f([0.5, 0.69]) = [f(0.69), f(0.5)]
$$
Calculamos:
$$
f(0.5) = e^{-0.5} \approx 0.6065, \quad f(0.69) = e^{-0.69} \approx 0.5016
$$
Por lo tanto:
$$
f([0.5, 0.69]) = [0.5016, 0.6065]
$$
Observamos que:
$$
[0.5016, 0.6065] \subseteq [0.5, 0.69]
$$
La función $f(x) = e^{-x}$ mapea el intervalo $A = [0.5, 0.69]$ en sí mismo, es decir:
$$
f(A) \subseteq A
$$
:::

Como $f$ es continuamente diferenciable, tome:

$$
q = \max_{x \in A} \, \big| f'(x) \big| 
   = \max_{x \in A} \, \big| -e^{-x} \big| 
   \approx 0.606531 < 1
$$

Si se ejecuta el programa iterativo en **R** como sigue:

```{r}
#| echo: false

punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_tem <- p0
  while (i <= n) {
    p <- g(p0_tem)
    cat(sprintf("En la iteración %d el valor de P es %.15f\n", i, p))
    if (abs(p - p0_tem) < tol) {
      return(p)
    }
    i <- i + 1
    p0_tem <- p
  }
  cat("El método no converge\n")
  return(NULL)
}
```


```{r}
g <- function(x) exp(-x)
sol3 <- punto.fijo(p0 = 0.55, tol = 1e-6, n = 30, g = g)
```

es decir, tomando $x_0 = 0.55$ como aproximación inicial, con $\varepsilon = Tol = 10^{-6}$ para el algoritmo anterior obtenemos que el “punto fijo” de $F$ es  

$$
x = x_{19} = 0.567143650676,
$$  

pues en $x_{19}$ se terminó la ejecución de la función,  
como se aprecia en la salida del programa:

```{r}
#| echo: false
cat("Solución punto.fijo:", sol3, "\n")
```

Por otro lado el error absoluto al calcular $x_{12} = 0.567124201933893$ es igual a:

$$
|x - x_{12}| \approx 1.91 \cdot 10^{-5},
$$

mientras que usando el error a priori se obtiene:

$$
|x - x_{12}| \leq \frac{q^{12}}{1-q} |x_1 - x_0| = 1.70 \cdot 10^{-4}
$$

y usando el error a posteriori se obtiene que:

$$
|x - x_{12}| \leq \frac{q}{1-q} |x_{12} - x_{11}| = 8.13 \cdot 10^{-5}
$$

que es una mejor estimación del verdadero error.  
Usando el error a priori se deduce que para obtener una precisión de $\varepsilon = 10^{-6}$ se requieren al menos:

$$
n \geq \frac{\log \left(\dfrac{\varepsilon(1-q)}{|x_1 - x_0|}\right)}{\log(q)} 
\approx 22.3 \leq 23 \text{ iteraciones},
$$

pero se observa que el programa requirió de 19 iteraciones. 

Ver la versión recursiva en el *HTML*.

```{r}
#| code-fold: true

punto.fijo.recursivo <- function(p0, tol, n, g) {
  p1 <- g(p0)
  if (abs(p0 - p1) < tol || n < 1) {
    if (n > 1) {
      return(p1)
    } else {
      return(Inf)
    }
  } else {
    return(punto.fijo.recursivo(p1, tol, n - 1, g))
  }
}
sol3 <- punto.fijo.recursivo(p0 = 0.55, tol = 1e-6, n = 30, g = g)
cat("Solución punto.fijo.recursivo:", sol3, "\n")
```

:::


::: {.callout-tip collapse="true" title="Ejemplo"}


Resuelva la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$.

**Solución:**  
Se debe plantear un problema de encontrar los puntos fijos de una función $g(x)$ que sea equivalente a resolver la ecuación $x^3 - x - 1 = 0$.  
Resolver $x^3 - x - 1 = 0$ es equivalente a resolver la ecuación $x^3 - 1 = x$, entonces se puede tratar de encontrar los puntos de $g(x) := x^3 - 1$.  

Pero $g(x)$ no cumple las hipótesis del Teorema de Punto Fijo de Banach, pues $g'(x) = 3x^2 > 0 \implies g(x)$ es creciente en $[1,2]$, luego $g(1) = 0$ y $g(2) = 7$ son el mínimo y el máximo de $g(x)$ en el intervalo $[1,2]$ respectivamente, por lo que $g(x) \notin [1,2] \quad \forall x \in [1,2]$.

Otro intento se puede hacer usando el hecho de que:

$$
x^3 - x - 1 = 0 \iff x = \pm \sqrt{1 + \frac{1}{x}},
$$

luego tome 

$$
g(x) := \sqrt{1 + \frac{1}{x}}, 
\quad g'(x) = -\frac{1}{2x^2\sqrt{1 + \frac{1}{x}}} < 0 \quad \text{en } [1,2].
$$

Esto implica que $g(x)$ es decreciente en $[1,2]$, luego:

$$
g(1) = \sqrt{2} \approx 1.41, 
\quad g(2) = \sqrt{\tfrac{3}{2}} \approx 1.22.
$$

El máximo y el mínimo respectivamente de $g(x)$ en $[1,2]$, por lo que $g(x) \in [1,2]$  
$\forall x \in [1,2]$, luego la función $g(x)$ tiene al menos un punto fijo en $[1,2]$.

Se probó que $g : [1,2] \to [1,2]$, falta probar que $g$ es una contracción en el intervalo $[1,2]$. Veamos

\begin{align*}
g'(x) = -\frac{1}{2x^2 \sqrt{1 + \frac{1}{x}}} 
\Rightarrow |g'(x)| = \frac{1}{2x^2 \sqrt{1 + \frac{1}{x}}} \leq \frac{1}{2} := q < 1
\end{align*}

de donde se puede tomar $q := \frac{1}{2}$.  
Se pueden ver las ejecuciones de las funciones en **R** de punto fijo con $x_0 = 2$ y $\epsilon = 10^{-5}$ en el archivo `punto_fijo.html`.

Luego el punto fijo de $g(x)$ y solución de la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$ es:  
$$x = 1.32471747253653$$  
Que coincide con la solución encontrada por nuestro programa. Gráficamente se ilustra en la Figura

```{r}
#| echo: false
# Definir los valores de x
x <- seq(0.01, 3, by = 0.01)

# Definir la función G(x)
G <- function(x) sqrt(1 + 1/x)

# Crear el gráfico
plot(x, G(x), type = "l", col = "blue", lwd = 2,
     ylim = c(0, 3.2), xlab = "x", ylab = "y", main = "Gráfico de G(x) y x")

# Agregar la recta y = x
lines(x, x, col = "red", lty = 2, lwd = 2)

# Agregar leyenda
legend("topright", legend = c("G(x) = sqrt(1 + 1/x)", "y = x"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```


$\blacksquare$

:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Dada la ecuación del ejemplo anterior $x^3 - x - 1 = 0$ en el intervalo $[1,2]$,  ¿cuántas iteraciones se requieren para obtener un error absoluto menor que $10^{-5}$?

**Solución:** Recuerde que $q = \frac{1}{2}$, de donde se tiene que:

\begin{align*}
|x_n - x| 
&\leq q^n \max\{x_0 - a, b - x_0\} \\
&= \left(\frac{1}{2}\right)^n \max\{1, 0\} \quad \text{(con $x_0 = 2$)} \\
&\leq \left(\frac{1}{2}\right)^n
\end{align*}

Luego:

\begin{align*}
|x_n - x| \leq 10^{-5} 
\Leftrightarrow \left(\frac{1}{2}\right)^n \leq 10^{-5} 
\Leftrightarrow n \geq 16.6
\end{align*}

Tome $n = 17$. $\blacksquare$

::: {.callout-important collapse="true" title="Observación"}


**Observación 1** En la práctica el programa requirió solamente 9 iteraciones.
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Para la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$, con $x_0 = 2$, estime usando el error a priori (1.2) ¿Cuántas iteraciones se requieren para obtener un error absoluto menor que $10^{-5}$?

**Solución:** Se tiene que

$$
q = \frac{1}{2}, \quad x_0 = 2, \quad g(x) = \sqrt{1 + \frac{1}{x}}
$$

De donde se obtiene que $x_1 = \sqrt{1 + \frac{1}{2}} = \sqrt{\frac{3}{2}} \approx 1.2$, entonces:

\begin{align*}
|x_n - x| 
&\leq \frac{\left(\frac{1}{2}\right)^n}{1 - \frac{1}{2}} |2 - 1.2| \\
&= \left(\frac{1}{2}\right)^{n-1} \cdot 0.8
\end{align*}

entonces:

\begin{align*}
|x_n - x| 
&\leq 10^{-5} 
\Leftrightarrow \left(\frac{1}{2}\right)^{n-1} \cdot 0.8 \leq 10^{-5} \\
&\Leftrightarrow (n-1)(-\log(2)) \leq -5 - \log(0.8)
\end{align*}

esto implica que $n \geq 17.28$, por lo que se puede tomar $n = 18$. $\blacksquare$

::: {.callout-important collapse="true" title="Observación"}


**Observación 2** En la práctica el programa requirió solamente 9 iteraciones.
:::
:::



## Método de bisección, método regula falsi y método de la secante.

### Método de la Bisección

Las hipótesis de este método son:

- $f$ debe ser continua en el intervalo $[a, b]$.
- $f(a)$ y $f(b)$ deben tener signos opuestos.

Luego, por el Teorema de los Valores Intermedios, existe $x \in [a, b]$ tal que $f(x) = 0$, gráficamente se ilustra en la Figura .

![El Método de la Bisección](Imagenes/4 grpah.png){width=400px fig-align="center"}

La idea es encontrar una sucesión $(x_n)$ tal que $x_n \to x$ cuando $n \to \infty$ tal que $f(x) = 0$. Para encontrar la sucesión $(x_n)$, la idea es la siguiente:

- Tome $a_1 = a$, $b_1 = b$, $x_1 = \dfrac{a_1 + b_1}{2}$.
- Si $f(x_1) = 0$, entonces ya se tiene el cero de la ecuación $x = x_1$.
- Si no:
  - Si $f(x_1)$ y $f(a)$ tienen el mismo signo, se toma:
  
    $$
    a_2 = x_1, \quad b_2 = b_1, \quad x_2 = \dfrac{a_2 + b_2}{2}.
    $$
  - Si no se toma:

  $$
  a_2 = a_1, \quad b_2 = x_1, \quad x_2 = \dfrac{a_2 + b_2}{2},
  $$

  y así sucesivamente hasta que $f(x_i) \approx 0$ o hasta superar el número máximo de iteraciones. El pseudocódigo se puede escribir como sigue:


#### Algoritmo Método de la Bisección

**Entrada:** $a, b, Tol$ (tolerancia), $N$, $f$  
**Salida:** Aproximación de $x$ (cero de la ecuación) o mensaje de error

1. $i \leftarrow 1$  
2. Mientras $i \leq N$, siga los pasos 3–6  
3. $x \leftarrow \dfrac{a + b}{2}$  
4. Si $f(x) = 0$ o $|b - a| < Tol$  
  -  Salida ($x$)  
  - **Parar**  
5. $i \leftarrow i + 1$  
6. Si $f(a) \cdot f(x) > 0$  
   - $a \leftarrow x$  
   -  Si no  
   -  $b \leftarrow x$  
7. Salida (“Número máximo de iteraciones excedido”)  
    **Parar**


Este algoritmo se puede programar iterativamente en **R**, ver el archivo `biseccion.html`.

```{r}
# Versión detallada: imprime cada iteración
biseccion <- function(a, b, tol, n, G) {
  i <- 1
  a1 <- a
  b1 <- b
  if (G(a) * G(b) > 0) {
    cat("No cumple las hipótesis\n")
  } else {
    while (i <= n) {
      X <- (a1 + b1) / 2
      cat("En la iteración", i, "el valor de X es", X, "\n")
      if (G(a) * G(X) > 0) {
        a1 <- X
      } else {
        b1 <- X
      }
      if ((b1 - a1) < tol) {
        return(X)
      }
      i <- i + 1
    }
    cat("El método no converge\n")
    return(NA)
  }
}
```


::: {.callout-tip collapse="true" title="Ejemplo"}


Resuelva la ecuación $x^3 + 4x^2 - 10 = 0$ en el intervalo $[1,2]$ con una tolerancia de $\varepsilon = 10^{-6}$.

**Solución:** 
```{r}
# Definición de la función
G <- function(x) x^3 + 4*x^2 - 10
# Gráfico de la función
curve(G, from = 1, to = 2, col = "blue", lwd = 2,
      main = "Gráfico de G(x) = x^3 + 4x^2 - 10",
      xlab = "x", ylab = "G(x)")
abline(h = 0, col = "red", lty = 2)
# Llamada a la función biseccion
resultado <- biseccion(1, 2, 1e-6, 50, G)
cat("Resultado final:", resultado, "\n")
```

$\blacksquare$
:::

#### Estudio del error en el método de la bisección

::: {.callout-note title="Teorema"}


Sea $f \in C[a,b]$, con $f(a)f(b) < 0$. Entonces el algoritmo de la bisección produce una sucesión $(x_n)$ que aproxima a $x$, el cero de la ecuación $f(x) = 0$, con un error absoluto tal que:

$$
|x_n - x| < \frac{b - a}{2^n} \quad \text{para } n \geq 1.
$$

::: {.callout-caution collapse="true" title="Prueba"}


\begin{align*}
|b_1 - a_1| &= |b - a| \\
|b_2 - a_2| &= \frac{1}{2} |b - a| \\
|b_3 - a_3| &= \frac{1}{2^2} |b - a| \\
&\vdots \\
|b_n - a_n| &= \frac{1}{2^{n-1}} |b - a|
\end{align*}

Como $x \in \, ]a_n, b_n[$ y $x_n = \dfrac{a_n + b_n}{2}$, se tiene que:

\begin{align*}
|x_n - x| &\leq \frac{|b_n - a_n|}{2} = \frac{1}{2} \cdot \frac{1}{2^{n-1}} |b - a| = \frac{1}{2^n}(b - a)
\end{align*}

$\blacksquare$
:::
:::
::: {.callout-important collapse="true" title="Observación"}


Sea $|x_n - x| < \dfrac{b - a}{2^n} \Rightarrow \dfrac{|x_n - x|}{\frac{1}{2^n}} < b - a = k$,  
esto implica que la sucesión $(x_n)$ es $\mathcal{O}\left( \dfrac{1}{2^n} \right)$,  
es decir, $x_n \to x$ con rapidez $\dfrac{1}{2^n}$, lo cual es bastante rápido.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Para $f(x) = x^3 + 4x^2 - 10$, con $a = 1$ y $b = 2$; usando (1.5), hallar el $n \in \mathbb{N}$ necesario para tener un error absoluto menor a $\varepsilon = 10^{-6}$.

**Solución:**

\begin{align*}
|x_n - x| &< \frac{b - a}{2^n} < 10^{-6} \\
\Leftrightarrow \quad 2^{-n} &< 10^{-6} \\
\Leftrightarrow \quad -n \log 2 &< -6 \\
\Leftrightarrow \quad n &> \frac{6}{\log 2} \\
\Leftrightarrow \quad n &> 19.9
\end{align*}

Entonces se puede escoger $n = 20$. $\blacksquare$
:::

::: {.callout-important collapse="true" title="Observación"}


$n = 20$ fue exactamente lo que requirió el programa.
:::

### Método de Newton–Raphson

El método de Newton–Raphson es uno de los más poderosos para resolver la ecuación $f(x) = 0$.  
Si $f$ es una función de una variable y $x_0$ es una aproximación a cero de la función $f$, entonces en un vecindario de $x_0$, por la fórmula de Taylor se tiene que:

$$
f(x) \approx f(x_0) + f'(x_0)(x - x_0).
$$

Si se define $g(x) := f(x_0) + f'(x_0)(x - x_0)$, entonces el cero de la función afín $g(x)$ se puede considerar como una nueva aproximación al cero de $f(x)$, el cual denotamos por $x_1$.  
Luego se tiene que:

$$
g(x_1) = f(x_0) + f'(x_0)(x_1 - x_0) = 0,
$$

despejando se tiene:

$$
x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}.
\tag{1.6}
$$

Geométricamente, la función afín $g(x)$ representa la recta tangente a $f(x)$ en el punto $x_0$. Esto se ilustra en la siguiente Figura.

![El Método de  Newton](Imagenes/MetodoNR.png){width=400px fig-align="center"}

De la Figura también se puede deducir el método de Newton–Raphson.  
Nótese que:

$$
f'(x_i) = \frac{f(x_i) - 0}{x_i - x_{i+1}}
$$

lo cual implica que:

$$
x_i - x_{i+1} = \frac{f(x_i)}{f'(x_i)}
$$

de donde:

$$
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
$$

que es la sucesión del método de Newton–Raphson.

::: {.callout-important collapse="true" title="Observación"}


El método de Newton–Raphson consiste en diseñar un algoritmo que calcule la sucesión:

$$
x_n =
\begin{cases}
x_0 & \text{si } n = 0 \\
x_{n-1} - \dfrac{f(x_{n-1})}{f'(x_{n-1})} & \text{si } n > 0
\end{cases}
$$
:::

::: {.callout-note title = "Algoritmo 3 – Método de Newton–Raphson"}

**Entrada:** $x_0$, $N$, $Tol$, $f$  
**Salida:** La solución $x$ aproximada de la ecuación $f(x) = 0$ o mensaje de error.

**Pasos:**

1 $i \leftarrow 1$  
2 Mientras $i \leq N$, hacer pasos 3–6:  
  3 $x \leftarrow x_0 - \dfrac{f(x_0)}{f'(x_0)}$  
  4 Si $|x - x_0| < Tol$  
  Salida $(x)$  
  Parar  
   5 $i \leftarrow i + 1$  
   6 $x_0 \leftarrow x$  
7 Salida (“Número máximo de iteraciones excedido”)  
 Parar
:::

El método de Newton–Raphson se puede implementar iterativamente en **R**.  
Ver el archivo `Newton_Raphson.html`.

::: {.callout-tip collapse="true" title="Ejemplo"}


**Ejemplo 8.** Ejecutando el programa iterativo para resolver la ecuación  
$e^{-x} - x = 0$ con $x_0 = 1$ como aproximación inicial y una tolerancia de $\varepsilon = 10^{-6}$,  
se obtiene la siguiente solución en el archivo `Newton_Raphson.html`.
:::

::: {.callout-note title="Teorema"}


Sea $f \in C^2[a,b]$. Si $x \in [a,b]$ con $f(x) = 0$ y $f'(x) \ne 0$, entonces existe $\varepsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que está bien definida y converge a $x$ cuando $n \to \infty$, para todo $x_0 \in [x - \varepsilon, x + \varepsilon]$.

::: {.callout-caution collapse="true" title="Prueba"}


Sea

$$
g(x) := x - \frac{f(x)}{f'(x)}.
$$

Nótese que si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, entonces $g(\tilde{x}) \in [a,b]$ y $g(\tilde{x}) = \tilde{x}$.

Se define:

$$
p_n := \begin{cases}
p_0 & \text{si } n = 0 \\
g(p_{n-1}) & \text{si } n \geq 1
\end{cases}
$$

Es claro que la $(p_n)$ es la sucesión de Newton–Raphson, entonces basta probar que $g$ cumple las hipótesis del Teorema de punto fijo de Banach, a saber:

$$
\exists \, \varepsilon > 0 \text{ y } q \in ]0,1[ \text{ tal que } \forall x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \Rightarrow |g'(x)| \leq q < 1,
$$

y que $g$ mapea el intervalo $[\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$ en sí mismo.

Como $f'(\tilde{x}) \ne 0$ y $f'$ es continua, se tiene que existe un $\varepsilon_1 > 0$ tal que $f'(x) \ne 0$ para todo $x \in [\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1] \subset [a,b]$,  
de esta manera $g$ está definida y es continua en $[\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1]$.  
También lo está:

$$
g'(x) = \frac{f(x) f''(x)}{[f'(x)]^2} \quad \text{para todo } x \in [\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1].
$$

Como $f \in C^2[a,b]$, entonces $g \in C^1[\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1]$.  
Como por hipótesis $f(\tilde{x}) = 0$, entonces:

$$
g'(\tilde{x}) = \frac{f(\tilde{x}) f''(\tilde{x})}{[f'(\tilde{x})]^2} = 0.
$$

Luego, como $g'$ es continua, esto implica que para cualquier constante $q < 1$ existe un $\varepsilon$, con $0 < \varepsilon < \varepsilon_1$, tal que:

$$
|g'(x)| \leq q < 1 \quad \text{para todo } x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon].
$$

Solo falta probar que $g : [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \to [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$,  
pero si $x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$, por el Teorema del Valor Medio para algún $\xi$ entre $x$ y $\tilde{x}$ se tiene que:

$$
|g(x) - g(\tilde{x})| = |g'(\xi)||x - \tilde{x}|,
$$

entonces:

$$
|g(x) - \tilde{x}| = |g(x) - g(\tilde{x})| = |g'(\xi)| \cdot |x - \tilde{x}| \leq q \cdot |x - \tilde{x}| < |x - \tilde{x}|.
\tag{1.7}
$$

Como $x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$, entonces $|x - \tilde{x}| < \varepsilon$,  
y luego por (1.7) se tiene que $|g(x) - \tilde{x}| < \varepsilon$,  
lo cual implica que $g(x) \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$,  
con lo que se prueba que $g : [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \to [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$.

$\blacksquare$
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


**Ejemplo 9.** Hallar un método para calcular $\sqrt{A}$, con $A \geq 0$.

**Solución:** Probaremos que la sucesión $(x_n)$ definida por  
$x_{n+1} = \dfrac{1}{2} \left( x_n + \dfrac{A}{x_n} \right)$  
converge a $\sqrt{A}$.  

Nótese que calcular $\sqrt{A}$ es equivalente a resolver la ecuación:

$$
x^2 - A = 0,
$$

usando el método de Newton–Raphson con $f(x) = x^2 - A$ y $f'(x) = 2x$, se tiene que:

\begin{align*}
x_{n+1} &= x_n - \frac{f(x_n)}{f'(x_n)} \\
        &= x_n - \frac{x_n^2 - A}{2x_n} \\
        &= \frac{1}{2} \left( x_n + \frac{A}{x_n} \right).
\end{align*}

De donde es claro que la sucesión $x_n \to \sqrt{A}$ cuando $n \to \infty$. $\blacksquare$
:::

### Método de la Secante

El problema con el método de Newton–Raphson es que requiere la derivada de $f(x)$,  
la cual en muchos casos no se tiene.  

La idea del método de la secante es usar una aproximación para esta derivada,  
como se ilustra en la Figura 1.5.

![El Método de la Secante](Imagenes/MetodoSecante.png){width=400px fig-align="center"}

La deducción del método es la siguiente:

$$
f'(x_{n-1}) = \lim_{x \to x_{n-1}} \frac{f(x) - f(x_{n-1})}{x - x_{n-1}} \approx \frac{f(x_{n-2}) - f(x_{n-1})}{x_{n-2} - x_{n-1}}.
\tag{1.8}
$$

Si en el método de Newton–Raphson $x_n = x_{n-1} - \dfrac{f(x_{n-1})}{f'(x_{n-1})}$ sustituimos $f'(x_{n-1})$ por la aproximación dada en (1.8), se tiene que:

$$
x_n \approx x_{n-1} - \frac{f(x_{n-1})}{\dfrac{f(x_{n-2}) - f(x_{n-1})}{x_{n-2} - x_{n-1}}},
$$

simplificando se obtiene el Método de la Secante:

$$
x_n \approx x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}.
$$

Entonces el método de la secante consiste en calcular la sucesión $(x_n)$ definida por:

$$
x_n =
\begin{cases}
x_0 & \text{si } n = 0 \\
x_1 & \text{si } n = 1 \\
x_{n-1} - \dfrac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})} & \text{si } n \geq 2
\end{cases}
$$

::: {.callout-note title="Algoritmo 4 – Método de la Secante"}

**Entrada:** $N$, $Tol$, $x_0$, $x_1$  
**Salida:** Aproximación de $x$, con $f(x) = 0$, o mensaje de error.

 Paso 1. $i \leftarrow 2$  

  Paso 2. Mientras $i \leq N$, hacer pasos 3–6:

  Paso 3. $x \leftarrow x_1 - \dfrac{(x_0 - x_1)f(x_1)}{f(x_0) - f(x_1)}$

  Paso 4. Si $|x - x_0| < Tol$  

    Salida $(x)$  

   Parar  

  Paso 5. $x_0 \leftarrow x_1$ 

     $x_1 \leftarrow x$  
  Paso 6. $i \leftarrow i + 1$  
  Paso 7. Salida (“Número máximo de iteraciones excedido”)  
 Parar
:::

Una implementación iterativa y otra recursiva en **R** se pueden ver en el archivo `secante.html`.

::: {.callout-tip collapse="true" title="Ejemplo"}


**Ejemplo 10.** Ejecutando el programa iterativo para resolver la ecuación  
$e^{-x} - x = 0$ con $x_0 = 1$ y $x_1 = \dfrac{1}{2}$ como aproximaciones iniciales  
y con una tolerancia de $\varepsilon = 10^{-6}$, se obtienen las siguientes soluciones  
en el archivo `secante.html`.
:::

::: {.callout-note title="Teorema"}


Sea $f$ una función de clase $C^2[a,b]$ con $a < b$.  
Si existe un punto $x$ tal que $f(x) = 0$ y $f'(x) \ne 0$,  
entonces existe un número $\varepsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$,  
la sucesión generada por el método de la secante

$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

está bien definida dentro del intervalo $[x - \varepsilon, x + \varepsilon]$  
y converge a $x$ (cero de la ecuación $f(x) = 0$).
:::

::: {.callout-caution collapse="true" title="Prueba"}


Ejercicio. $\blacksquare$
:::

### Orden de convergencia

::: {.callout-note title="Definición"}

Sea $(x_n)$ una sucesión que converge a $x$, se denota $e_n := x_n - x$ para $n \geq 0$.  
Si existen constantes $\alpha$ y $\lambda$ positivas, tal que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - x|}{|x_n - x|^\alpha} = 
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|^\alpha} = \lambda,
$$

entonces se dice que la sucesión $(x_n)$ converge a $x$ con **orden $\alpha$**  
y con **constante asintótica** $\lambda$.
:::

::: {.callout-important collapse="true" title="Observación"}


- Entre mayor sea el orden de convergencia, o sea entre mayor sea $\alpha$, mayor será la “velocidad” de convergencia.
- Si $\alpha = 1$, se dice que el método tiene **orden lineal**.
- Si $\alpha = 2$, se dice que el método tiene **orden cuadrático**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Suponga que tenemos dos esquemas (sucesiones) $x_n$ y $\tilde{x}_n$ tal que:

- $\displaystyle \lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} = 0.75$ (método lineal),
- $\displaystyle \lim_{n \to \infty} \frac{|\tilde{e}_{n+1}|}{|\tilde{e}_n|^2} = 0.75$ (método cuadrático),
- Se supone además que $e_0 = 0.5$ y $\tilde{e}_0 = 0.5$.

**¿Cuántas iteraciones requieren $x_n$ y $\tilde{x}_n$ para converger con un error absoluto menor a $10^{-8}$?**

**Solución:**

**1. Analicemos primero la sucesión (esquema) $x_n$:**

\begin{align*}
|x_{n+1} - x| < 10^{-8} &\Leftrightarrow |e_{n+1}| < 10^{-8} \\
\text{Pero } \frac{|e_{n+1}|}{|e_n|} &\approx 0.75 \Rightarrow |e_{n+1}| \approx 0.75 |e_n| \approx 0.75^2 |e_{n-1}| \approx \cdots \\
&\approx 0.75^n |e_0| = 0.75^n \cdot 0.5 \\
\Rightarrow |e_{n+1}| < 10^{-8} &\Leftrightarrow 0.75^n \cdot 0.5 < 10^{-8} \\
\Leftrightarrow \log(0.75^n \cdot 0.5) &< \log(10^{-8}) \\
n \log(0.75) + \log(0.5) &< -8 \\
n &> \frac{-8 - \log(0.5)}{\log(0.75)} \approx 61.62
\end{align*}

Se puede tomar $n = 62$, por lo tanto $x_n$ requiere aproximadamente **62 iteraciones** para converger a $x$.

---

**2. Analicemos ahora la sucesión $\tilde{x}_n$:**

\begin{align*}
|\tilde{x}_{n+1} - x| < 10^{-8} &\Leftrightarrow |\tilde{e}_{n+1}| < 10^{-8} \\
\text{Pero } \frac{|\tilde{e}_{n+1}|}{|\tilde{e}_n|^2} &\approx 0.75 \Rightarrow |\tilde{e}_{n+1}| \approx 0.75 |\tilde{e}_n|^2 \\
&\approx 0.75 \left[ 0.75 |\tilde{e}_{n-1}|^2 \right]^2 = 0.75^3 |\tilde{e}_{n-1}|^4 \\
&\approx 0.75^3 \left[ 0.75 |\tilde{e}_{n-2}|^2 \right]^4 = 0.75^7 |\tilde{e}_{n-2}|^8 \\
&\approx \cdots \approx 0.75^{2^n - 1} |\tilde{e}_0|^{2^n}
\end{align*}
de donde
\begin{align*}
|\tilde{e}_{n+1}| &< 10^{-8} \\
0.75^{2^{n+1}-1} |\tilde{e}_0|^{2^{n+1}} &< 10^{-8} \\
0.75^{2^{n+1}-1} \cdot 0.5^{2^{n+1}} &< 10^{-8} \\
0.75^{-1} \cdot 0.375^{2^{n+1}} &< 10^{-8} \\
2^{n+1} \log(0.375) &< -8 + \log(0.75) \\
2^{n+1} &> \frac{-8 + \log(0.5)}{\log(0.375)} \\
(n + 1) \log(2) &> \log(19.07) \\
n &> \frac{\log(19.07)}{\log(2)} - 1 \approx 3.24
\end{align*}

Luego $n = 4$, por lo que $\tilde{x}_n$ requiere solamente de **4 iteraciones** para converger a $x$, mientras que el esquema lineal requirió aproximadamente **62 iteraciones** para converger a $x$.

$\blacksquare$

:::

::: {.callout-note title="Teorema"}


Sea $x_n = \begin{cases}
x_0 & \text{si } n = 0 \\
g(x_{n-1}) & \text{si } n \geq 1
\end{cases}$ el esquema (la sucesión) de punto fijo visto en la sección 1.1.  

Si además se supone que:

- $g : [a,b] \to [a,b]$
- $g \in C^2[a,b]$
- Existe $q$ tal que $0 \leq q < 1$ y $|g'(x)| \leq q < 1$ para todo $x \in ]a,b[$
- $g'(x) \ne 0$ para el punto fijo de $g$

Entonces $(x_n)$ converge al menos **linealmente a $x$**, cuando $n \to \infty$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


\begin{align*}
|e_{n+1}| &= |x_{n+1} - x| = |g(x_n) - g(x)| = |g'(\xi_n)(x_n - x)| = |g'(\xi_n)||e_n|
\end{align*}

con $\xi_n$ entre $x_n$ y $x$.  
Como $x_n \to x$ cuando $n \to \infty$, y $\xi_n$ está entre $x_n$ y $x$, entonces la sucesión $(\xi_n)$ también converge a $x$ cuando $n \to \infty$.  

Luego:

$$
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} 
= \lim_{n \to \infty} |g'(\xi_n)| 
= |g'\left( \lim_{n \to \infty} \xi_n \right)| = |g'(x)|
$$

Por lo tanto:

$$
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} = g'(x) := \lambda < 1,
$$

de donde la convergencia es lineal.  
$\blacksquare$
:::

::: {.callout-note title="Teorema"}


Sea $\tilde{x}$ un punto fijo de $g$, además $g'(\tilde{x}) = 0$, $g''$ es continua en un intervalo abierto $I$ que contiene a $x$ y $g''(\tilde{x}) \ne 0$.  
Entonces existe $\epsilon > 0$ tal que para $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ la sucesión  
$x_n = \begin{cases}
x_0 & \text{si } n = 0 \\
g(x_{n-1}) & \text{si } n \geq 1
\end{cases}$  
converge al menos **cuadráticamente** a $\tilde{x}$.

::: {.callout-caution collapse="true" title="Prueba"}


Escoja $\epsilon$ tal que el intervalo $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ esté contenido en $I$,  
$|g'(x)| \leq q < 1$ y $g''$ sea continua.  

Como $|g'(x)| \leq q < 1$, se tiene que los términos de la sucesión $(x_n)$ están contenidos en $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.  

Expandiendo $g(x)$ en su polinomio de Taylor para $x \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$, se tiene que:

\begin{align*}
g(x) &= g(\tilde{x}) + g'(\tilde{x})(x - \tilde{x}) + \frac{g''(\xi)}{2}(x - \tilde{x})^2
\end{align*}

con $\xi$ entre $x$ y $\tilde{x}$. Por hipótesis $g(\tilde{x}) = \tilde{x}$ y $g'(\tilde{x}) = 0$, esto implica que:

\begin{align*}
g(x) = \tilde{x} + \frac{g''(\xi)}{2}(x - \tilde{x})^2
\end{align*}

En particular si se toma $x = x_n$, entonces:

\begin{align*}
x_{n+1} = g(x_n) = \tilde{x} + \frac{g''(\xi_n)}{2}(x_n - \tilde{x})^2
\end{align*}

con $\xi_n$ entre $x_n$ y $\tilde{x}$. Luego:

\begin{align}
x_{n+1} - \tilde{x} = \frac{g''(\xi_n)}{2}(x_n - \tilde{x})^2 \tag{1.9}
\end{align}

Como $|g'(x)| \leq q < 1$ y $g$ mapea $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ en sí mismo, es claro que $(x_n)$ converge a $\tilde{x}$ punto fijo de $g$,  
entonces como $\xi_n$ está entre $x_n$ y $\tilde{x}$, la sucesión $(\xi_n)$ converge también a $\tilde{x}$.  
Luego usando (1.9) se deduce que:

\begin{align*}
\lim_{n \to \infty} \frac{|x_{n+1} - \tilde{x}|}{|x_n - \tilde{x}|^2}
= \left| \frac{g''(\xi_n)}{2} \right| = \frac{|g''(\tilde{x})|}{2} = \lambda \ne 0
\end{align*}

Entonces la sucesión $(x_n)$ converge cuadráticamente a $\tilde{x}$.  
$\blacksquare$
:::
:::


::: {.callout-note title="Corolario"}

Sea $f \in C^3[a,b]$. Si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$,  
entonces existe $\epsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que **converge al menos cuadráticamente a $\tilde{x}$** cuando $n \to \infty$,  
para todo $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.
:::

::: {.callout-note title="Corolario"}

Sea $f \in C^3[a,b]$.  
Si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$,  
entonces existe $\epsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que converge al menos cuadráticamente a $\tilde{x}$  
cuando $n \to \infty$, para todo $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.


::: {.callout-caution collapse="true" title="Prueba"}


La convergencia de $(x_n)$ se demostró en el teorema anterior.  
Usando el teorema anterior se concluye el corolario, pues si se toma $g(x) := x - \frac{f(x)}{f'(x)}$  
entonces:

\begin{align*}
g(\tilde{x}) = \tilde{x} - \frac{f(\tilde{x})}{f'(\tilde{x})} = \tilde{x} \quad \text{pues } f(\tilde{x}) = 0 \text{ y } f'(\tilde{x}) \ne 0.
\end{align*}

Además,

\begin{align*}
g'(x) &= 1 - \frac{f'(x)f'(x) - f(x)f''(x)}{(f'(x))^2} = 1 - 1 - \frac{f(x)f''(x)}{[f'(x)]^2} = -\frac{f(x)f''(x)}{[f'(x)]^2}
\end{align*}

de donde $g'(\tilde{x}) = 0$, pues $f(\tilde{x}) = 0$.

Como:

\begin{align*}
g''(x) = \frac{[f'(x)]^2 f''(x) + f(x)f'(x)f^{(3)}(x) - 2f(x)[f''(x)]^2}{[f'(x)]^3}
\end{align*}

entonces:

\begin{align*}
g''(\tilde{x}) = \frac{f''(\tilde{x})}{f'(\tilde{x})} \ne 0.
\end{align*}

Es claro que $g''$ es continua en un intervalo que contiene a $\tilde{x}$ dado que $f \in C^3[a,b]$.  
Entonces $g(x)$ cumple todas las hipótesis del teorema anterior por lo que $(x_n)$ converge cuadráticamente a $\tilde{x}$.  
$\blacksquare$
:::
:::

::: {.callout-note title="Teorema"}


Sea $f$ una función de clase $C^2[a,b]$.  
Si existe un punto $\tilde{x}$ tal que $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$  
entonces existe un número $\epsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \epsilon, x + \epsilon]$  
la sucesión generada por el método de la secante:

$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1})f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

converge a $\tilde{x}$ con orden de al menos:

$$
\frac{1 + \sqrt{5}}{2} \approx 1.618.
$$


::: {.callout-caution collapse="true" title="Prueba"}


(Ejercicio)  
Sugerencias: Pruebe que

\begin{align}
|e_{n+1}| \approx C \cdot |e_n| \cdot |e_{n-1}| \tag{1.10}
\end{align}

con $C := \left| \frac{f''(\tilde{x})}{2f'(\tilde{x})} \right|$.  
Por la definición 1 se busca una solución aproximada a la ecuación:

\begin{align}
|e_n| = \lambda |e_{n-1}|^\alpha \tag{1.11}
\end{align}

con $\lambda > 0$ y $\alpha \geq 1$.  
Pero considerando (1.10) como una ecuación y sustituyendo en (1.11), se tiene que:

\begin{align}
\lambda |e_n|^\alpha = \lambda \lambda^\alpha |e_{n-1}|^{\alpha^2} = C \lambda |e_{n-1}|^{\alpha + 1} \tag{1.12}
\end{align}

La ecuación (1.12) es válida solamente para $n$ suficientemente grandes si:

\begin{align*}
\lambda^\alpha = C \quad \text{y} \quad \alpha^2 = \alpha + 1
\end{align*}

de donde:

\begin{align*}
\alpha = \frac{1 + \sqrt{5}}{2}, \quad \text{y} \quad \lambda = C^{1/\alpha}.
\end{align*}

$\blacksquare$
:::
:::
## Aceleración de la convergencia y método de Steﬀensen.

### Método $\triangle^2$ de Aitken

El método $\triangle^2$ de *Aitken* tiene las siguientes hipótesis:

1. $(x_n)$ converge linealmente a $x$ con constante asintótica $\lambda$ tal que $0 < \lambda < 1$.

2. Los signos de $x_n - x$, $x_{n+1} - x$ y $x_{n+2} - x$ son iguales.

3. Se asume que para $n$ suficientemente grande:

$$
\frac{x_{n+1} - x}{x_n - x} \approx \frac{x_{n+2} - x}{x_{n+1} - x}.
$$

La idea es encontrar una sucesión $(\tilde{x}_n)$ la cual converge “más rápidamente” a $x$ que la sucesión $(x_n)$. Note que de la expresión anterior:

\begin{align*}
&\Rightarrow (x_{n+1} - x)(x_n - x) \approx (x_n - x)(x_{n+2} - x) \\
&\Rightarrow x_{n+1}^2 - 2x_nx_{n+1} + x^2 \approx x_nx_{n+2} - xx_{n+2} - xx_n + x^2 \\
&\Rightarrow -2x_nx_{n+1} + xx_{n+1} + xx_n \approx -x_{n+1}^2 + x_nx_{n+2} \\
&\Rightarrow (-2x_nx_{n+1} + x_nx_{n+2} + x_nx_{n+1})x \approx -x_{n+1}^2 + x_nx_{n+2} \\
&\Rightarrow x \approx \frac{-x_{n+1}^2 + x_nx_{n+2}}{(-2x_nx_{n+1} + x_nx_{n+2} + x_n)} \\
&\Rightarrow x \approx \frac{x_nx_{n+2} - x_{n+1}^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx \frac{x_n^2 + x_nx_{n+2} - 2x_nx_{n+1} - x_{n+1}^2 + 2x_nx_{n+1} - x_{n+1}^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx \frac{x_n(x_n + x_{n+2} - 2x_{n+1}) - (x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n}
\end{align*}

El método de $\triangle^2$ de Aitken se basa en el hecho de que la sucesión $(\tilde{x}_n)$ definida por:

$$
\tilde{x}_{n+3} := x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n},
$$

bajo ciertas condiciones, converge “más rápidamente” a $x$ que la sucesión $(x_n)$.


::: {.callout-note title="Notación: Diferencia Progresiva"}


- $\triangle(x_n) := x_{n+1} - x_n$ para $n \ge 0$.
- $\triangle^k(x_n) := \triangle^{k-1}(\triangle x_n)$ para $k \ge 2$.
:::
::: {.callout-important collapse="true" title="Observación"}

Nótese que con esta notación se tiene:

\begin{align*}
\triangle^2(x_n) &= \triangle(\triangle x_n) \\
              &= \triangle x_{n+1} - \triangle x_n \\
              &= x_{n+2} - x_{n+1} - (x_{n+1} - x_n) \\
              &= x_{n+2} - 2x_{n+1} + x_n.
\end{align*}

Por lo que el método $\triangle^2$ de Aitken se puede escribir como:

\begin{equation}
\tilde{x}_{n+3} := x_n - \frac{(\triangle x_n)^2}{\triangle^2 x_n}\tag{1.14}
\end{equation}
:::

::: {.callout-note title="Definición"}

Sean $(x_n)$ y $(\tilde{x}_n)$ dos sucesiones que convergen a $x$, se dice que la sucesión $(\tilde{x}_n)$ **converge más rápido a** $x$ que la sucesión $(x_n)$ si:

$$
\lim_{n \to \infty} \frac{\tilde{x}_n - x}{x_n - x} = 0.
$$
:::

::: {.callout-note title="Teorema"}


Sea $(x_n)$ una sucesión que converge a $x$ con orden lineal con constante asintótica $\lambda < 1$, además se asume que $e_n = x_n - x \ne 0$ para todo $n$. Entonces la sucesión $(\tilde{x}_n)$, definida como en (1.14), converge a $x$ más rápido que $(x_n)$.

::: {.callout-caution collapse="true" title="Prueba"}


Ejercicio. $\blacksquare$
:::
:::

### El método de Steffensen

Por el Teorema 8 la sucesión de aproximaciones sucesivas $(x_n)$ converge linealmente a $x$, cuando $n \to \infty$ con constante asintótica $\lambda < 1$, entonces tiene sentido aplicar el método $\triangle^2$ de Aitken a esta sucesión. Es así como una combinación entre el método de aproximaciones sucesivas y el método $\triangle^2$ de Aitken produce un método conocido como el **Método de Steffensen**, el cual consiste en calcular la sucesión:

$$
x_{n+1} := x_n - \frac{[g(x_n) - x_n]^2}{g(g(x_n)) - 2g(x_n) + x_n}.
$$

Algorítmicamente, el Método de Steffensen para acelerar el método de punto fijo se puede expresar de la siguiente manera. Sea $x_0$ la aproximación inicial en el método de punto fijo, entonces se toma:

\begin{align*}
x_0^{(0)} &= x_0 \\
x_1^{(0)} &= g\left(x_0^{(0)}\right) \\
x_2^{(0)} &= g\left(x_1^{(0)}\right) \\
x_0^{(1)} &= \Delta^2\left(x_0^{(0)}\right) = x_0^{(0)} - \frac{\left(x_1^{(0)} - x_0^{(0)}\right)^2}{x_2^{(0)} - 2x_1^{(0)} + x_0^{(0)}} \\
x_1^{(1)} &= g\left(x_0^{(1)}\right) \\
x_2^{(1)} &= g\left(x_1^{(1)}\right) \\
x_0^{(2)} &= \Delta^2\left(x_0^{(1)}\right) = x_0^{(1)} - \frac{\left(x_1^{(1)} - x_0^{(1)}\right)^2}{x_2^{(1)} - 2x_1^{(1)} + x_0^{(1)}} \\
\vdots &
\end{align*}


::: {.callout-note title="Algoritmo [Método de Steffensen]"}

**Entrada**: $N$, $Tol$, $x_0$, $g$  
**Salida**: Aproximación de $x$ o mensaje de error

\begin{align*}
&\text{Paso 1. } i \gets 2 \\
&\text{Paso 2. Mientras } i \leq N, \text{ siga los pasos 3–6} \\
&\quad \text{Paso 3. } x_1 = g(x_0), \quad x_2 = g(x_1) \\
&\qquad x = x_0 - \frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} \\
&\quad \text{Paso 4. Si } |x - x_0| < Tol \\
&\qquad \text{Salida } (x) \\
&\qquad \text{Parar} \\
&\quad \text{Paso 5. } i \gets i + 1 \\
&\quad \text{Paso 6. } x_0 \gets x \\
&\text{Paso 7. Salida (``Número máximo de iteraciones excedido'')} \\
&\text{Parar}
\end{align*}

Este algoritmo se puede programar iterativamente y recursivamente en **R**, como se muestra en el archivo `steffensen.html`.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Resolver la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1, 2]$ con $\epsilon = 10^{-5}$.

**Solución**: Como se mostró en el ejemplo 3, utilizando $g(x) = \sqrt{1 + \frac{1}{x}}$, el método de punto fijo requirió de 9 iteraciones, con $x_0 = 2$, para resolver esta ecuación, con una tolerancia de $\epsilon = 10^{-5}$, mientras que el Método de Steffensen requiere solamente de 3 iteraciones, como se muestra en el archivo `steffensen.html`. $\blacksquare$
:::
::: {.callout-note title="Teorema"}


Si el método de punto fijo $x_{n+1} = g(x_n)$ converge linealmente, entonces el orden de convergencia del método de Steffensen es al menos dos.

::: {.callout-caution collapse="true" title="Prueba"}


(Ejercicio) Suponga que $g(x)$ es un número suficientemente de veces derivable, luego pruebe que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - \widetilde{x}|}{|x_n - \widetilde{x}|^2} 
= \frac{1}{2} \left| \frac{g'(\widetilde{x})g''(\widetilde{x})}{g'(\widetilde{x}) - 1} \right| := \lambda \neq 0.
$$

$\blacksquare$
:::
:::

# Interpolación

## Interpolación y aproximaciones polinómicas

### Polinomios de Bernstein

::: {.callout-note title="Teorema: [Weierstrass]"}

Sea $f$ continua en $[a, b]$; entonces dado $\varepsilon > 0$, existe $n \in \mathbb{N}$ y $P_n(x) \in P_n$ tal que

$$
\left| f(x) - P_n(x) \right| < \varepsilon, \quad \forall x \in [a, b].
$$


![Polinomio de Bernstein](Imagenes/Polinomio de Bernstein.png){width=400px fig-align="center"}

::: {.callout-caution collapse="true" title="Prueba"}


Sin pérdida de generalidad, suponga que $a = 0$ y $b = 1$. Sea:


$$
B_n(x) = \sum_{k=0}^n \binom{n}{k} x^k (1 - x)^{n-k} f\left( \frac{k}{n} \right),
$$

se puede probar que $B_n(x) \to f(x)$ uniformemente en $[0,1]$ (ejercicio). $\blacksquare$
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^x$ en $[0, 1]$, entonces:

\begin{align*}
B_2(x) 
&= \sum_{k=0}^2 \binom{2}{k} x^k (1 - x)^{2-k} e^{\frac{k}{2}} \\
&= \binom{2}{0} x^0 (1 - x)^2 e^0 
   + \binom{2}{1} x^1 (1 - x)^1 e^{\frac{1}{2}} 
   + \binom{2}{2} x^2 (1 - x)^0 e^1 \\
&= (1 - x)^2 + 2x(1 - x) e^{\frac{1}{2}} + x^2 e.
\end{align*}

:::

```{r}
#| code-fold: true

# Polinomio de Bernstein en [0,1]
bernstein <- function(n, F, x) {
  resultado <- numeric(length(x))
  for (i in seq_along(x)) {
    xi <- x[i]
    suma <- 0
    for (k in 0:n) {
      coef <- choose(n, k) * (xi^k) * ((1 - xi)^(n - k))
      suma <- suma + coef * F(k / n)
    }
    resultado[i] <- suma
  }
  return(resultado)
}

# Ejemplo: F(x) = exp(x) en [0,1]
F <- function(x) exp(x)
xs <- seq(0, 1, length.out = 400)

# Aproximación con n = 10
vals <- bernstein(10, F, xs)

# Graficar
plot(xs, F(xs), type = "l", col = "red", lwd = 2,
     main = "Aproximación de Bernstein de exp(x) en [0,1]",
     ylab = "f(x)", xlab = "x")
lines(xs, vals, col = "blue", lwd = 2)
legend("topleft", legend = c("exp(x)", "Bernstein n=10"),
       col = c("red", "blue"), lwd = 2)
```

**Observación.** El polinomio de Bernstein tiene solo valor teórico y no práctico.

### Existencia y unicidad del polinomio de interpolación

::: {.callout-note title="Problema"}

Sean $(x_i, y_i)$, para $i = 0, 1, 2, \ldots, n$, una secuencia de $n + 1$ puntos (nodos). Se busca un polinomio de grado $n$:

$$
P_n(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n,
$$

tal que satisfaga las condiciones de interpolación:

$$
P_n(x_i) = y_i \qquad i = 0, 1, 2, \ldots, n.
$$
:::

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 7

# Paquetes necesarios
library(ggplot2)

# Datos de los nodos
x <- c(-2, -1, 0, 1, 2)
y <- c(-1.2, 0.5, 1, 2.8, 3.5)

# Ajuste del polinomio interpolante de grado 4
modelo <- lm(y ~ poly(x, 4, raw = TRUE))

# Datos para graficar el polinomio
x_vals <- seq(min(x) - 0.5, max(x) + 0.5, length.out = 500)
y_vals <- predict(modelo, newdata = data.frame(x = x_vals))

# Crear un data frame con los nodos para etiquetas
df_nodos <- data.frame(x = x, y = y,
                       etiqueta = paste0("(", x, ", ", y, ")"))

# Gráfico
ggplot() +
  geom_line(aes(x = x_vals, y = y_vals), color = "darkred", linewidth = 1.3) +
  geom_point(data = df_nodos, aes(x = x, y = y), size = 3, shape = 21, fill = "white") +
  geom_text(data = df_nodos, aes(x = x, y = y, label = etiqueta),
            vjust = -1.2, size = 3.5) +
  labs(
    title = "Interpolación polinómica con nodos: (-2,-1.2), (-1,0.5), (0,1), (1,2.8), (2,3.5)",
    subtitle = "Polinomio P[n](x) tal que  P_n(xᵢ) = yᵢ",
    x = "x",
    y = "y"
  ) +
  theme_minimal(base_size = 14)
```

::: {.callout-note title="Teorema"}


Sean $(x_i, y_i)$ una secuencia de $n + 1$ puntos, con $i = 0, 1, 2, \ldots, n$, tal que $x_i \ne x_j$, $\forall i \ne j$, entonces existe un único polinomio (de interpolación) que satisface la condición de interpolación:

$$
P_n(x_i) = y_i, \qquad i = 0, 1, \ldots, n,
$$

el cual tiene a lo más grado $n$.

::: {.callout-caution collapse="true" title="Prueba"}


**Existencia:**

Sea

$$
L_i(x) := \prod_{\substack{j = 0 \\ j \ne i}}^n \frac{x - x_j}{x_i - x_j}
= \frac{(x - x_0)\cdots(x - x_{i-1})(x - x_{i+1})\cdots(x - x_n)}
{(x_i - x_0)\cdots(x_i - x_{i-1})(x_i - x_{i+1})\cdots(x_i - x_n)}.
$$

Note que:

$$
L_i(x_k) =
\begin{cases}
1 & \text{si } i = k \\
0 & \text{si } i \ne k
\end{cases}
= \delta_{ik}.
$$

Entonces el polinomio

$$
P_n(x) := \sum_{i = 0}^n y_i L_i(x),
$$

tiene la propiedad de interpolación, pues:

$$
P_n(x_k) = \sum_{i = 0}^n y_i L_i(x_k) = \sum_{i = 0}^n y_i \delta_{ik} = y_k, \qquad \text{para } k = 0, 1, \ldots, n.
$$

Además, el grado de $P_n(x)$ es menor o igual a $n$, pues es combinación lineal de polinomios de grado $n$.

**Unicidad:**

Sean $P_n(x)$ y $Q_n(x)$ dos polinomios de grado $n$ que satisfacen las condiciones de interpolación,

$$
P_n(x_k) = Q_n(x_k) = y_k, \qquad k = 0, 1, \ldots, n.
$$

Sea $D(x) := P_n(x) - Q_n(x)$, note que $D(x)$ es un polinomio de grado a lo más $n$ y tiene $n + 1$ raíces $x_0, x_1, \ldots, x_n$, por lo que de acuerdo al Teorema Fundamental del Álgebra se tiene que:

$$
D(x) \equiv 0 \Rightarrow P_n(x) - Q_n(x) = 0 \Rightarrow P_n(x) = Q_n(x). \quad \blacksquare
$$
:::
:::

### Interpolación de Lagrange


::: {.callout-note title="Definición: Polinomio de interpolación de Lagrange"}

El polinomio:

$$
P_n(x) = \sum_{i=0}^n y_i L_i(x),
$$

con $y_i = f(x_i)$ y $L_i(x)$ definido por:

$$
L_i(x) = \frac{(x - x_0)(x - x_1)\cdots(x - x_{i-1})(x - x_{i+1})\cdots(x - x_n)}
{(x_i - x_0)(x_i - x_1)\cdots(x_i - x_{i-1})(x_i - x_{i+1})\cdots(x_i - x_n)},
$$

se llama el **polinomio de interpolación de Lagrange**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^x$, $0 \leq x \leq 2$. Calcule $P_2(x)$ con $x_0 = 0$, $x_1 = 1$, y $x_2 = 2$.

**Solución:**

\begin{align*}
P_2(x) &= \frac{(x - 1)(x - 2)}{(0 - 1)(0 - 2)} e^0 + \frac{(x - 0)(x - 2)}{(1 - 0)(1 - 2)} e^1 + \frac{(x - 1)(x - 0)}{(2 - 0)(2 - 1)} e^2 \\
&= \frac{(x - 1)(x - 2)}{2} - x(x - 2)e + \frac{(x - 1)x}{2}e^2.
\end{align*}

$\blacksquare$
:::

![Polinomio de interpolación de Lagrange](Imagenes/Polinomio de interpolación de Lagrange.png){width=400px fig-align="center"}

#### Estudio del error

Recordemos el Teorema de Rolle Generalizado:

::: {.callout-note title="Teorema"}


Sea $f \in C[a, b]$ y $f \in C^n[a, b]$, si $f$ se anula en $n + 1$ puntos distintos $x_0, x_1, \ldots, x_n$ en $[a, b]$, entonces $\exists\, c \in ]a, b[$ tal que $f^{(n)}(c) = 0$.

::: {.callout-caution collapse="true" title="Prueba"}


Se omite. $\blacksquare$
:::
:::

::: {.callout-note title="Teorema: Error en el método de Lagrange"}

Sean $x_0, x_1, \ldots, x_n \in [a, b]$ y sea $f \in C^{n+1}[a, b]$. Entonces $\forall\, x \in [a, b]$, $\exists\, \xi_x \in ]a, b[$ tal que:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x - x_0)(x - x_1) \cdots (x - x_n).
$$

Es decir, el error absoluto es:

$$
\left|f(x) - P_n(x)\right| = \left| \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x - x_0)(x - x_1) \cdots (x - x_n) \right|.
$$

::: {.callout-caution collapse="true" title="Prueba"}


**I caso**  
Si $x = x_k$, entonces $f(x_k) = P_n(x_k)$ y el error es cero, por lo tanto cualquier $\xi_x$ funciona.

**II caso**  
Si $x \ne x_k$, se define:

$$
g(t) = f(t) - P_n(t) - \left[ f(x) - P_n(x) \right] \frac{(t - x_0)(t - x_1)\cdots(t - x_n)}{(x - x_0)(x - x_1)\cdots(x - x_n)}.
$$

Vamos a probar que $g(t)$ cumple las hipótesis del Teorema Generalizado de Rolle para $n + 1$.

- $g$ es $n + 1$ veces derivable pues $f \in C^{n+1}[a, b]$ y $P \in C^\infty[a, b]$.
- $g$ se anula en $n + 2$ puntos, a saber: $t = x_0, x_1, \ldots, x_n$ y $t = x$.


Como $g$ cumple las hipótesis del Teorema Generalizado del Rolle, entonces:

$$
\exists \, \xi_x \in ]a, b[ \text{ tal que } g^{(n+1)}(\xi_x) = 0.
$$

Vamos a calcular $g^{(n+1)}(t)$:

\begin{align*}
\frac{d^{n+1}}{dt^{n+1}} g(t) 
&= f^{(n+1)}(t) - 0 - [f(x) - P_n(x)] \cdot \frac{d^{n+1}}{dt^{n+1}} \prod_{i=0}^n \frac{t - x_i}{x - x_i} \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{1}{\prod_{i=0}^n (x - x_i)} \cdot \frac{d^{n+1}}{dt^{n+1}} \prod_{i=0}^n (t - x_i) \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{1}{\prod_{i=0}^n (x - x_i)} \cdot \frac{d^{n+1}}{dt^{n+1}} \left( t^{n+1} + \text{términos de grado } \leq n \right) \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{(n+1)!}{\prod_{i=0}^n (x - x_i)}.
\end{align*}

De donde se concluye que:

$$
g^{(n+1)}(\xi_x) = f^{(n+1)}(\xi_x) - [f(x) - P_n(x)] \cdot \frac{(n+1)!}{\prod_{i=0}^n (x - x_i)} = 0,
$$

por lo tanto:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!} \prod_{i=0}^n (x - x_i).
$$

$\blacksquare$
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^x$, con $x \in [0, 2]$ y sean $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, tenemos que:

$$
P_2(x) = \frac{(x - 1)(x - 2)}{2} - x(x - 2)e + \frac{(x - 1)x}{2}e^2,
$$

así el error absoluto de aproximar $f(0.25)$ por $P_2(0.25)$ es:

$$
\text{Error Absoluto} = |f(0.25) - P_2(0.25)| = 0.1312511.
$$

Mientras que el error teórico es:

$$
\text{Error Teórico} = \left| \frac{e^{\xi_x}}{3!}(x - 0)(x - 1)(x - 2) \right| 
\leq \left| \frac{e^2}{3!}x(x - 1)(x - 2) \right|,
$$

luego con $x = 0.25$:

$$
\text{Error Teórico} = \left| \frac{e^2}{3!}(0.25)(0.25 - 1)(0.25 - 2) \right| = 0.404089.
$$

$\blacksquare$
:::

::: {.callout-note title="Método 1: Calcula el Polinomio de Lagrange retornando una función para ser evaluada en un $x$."}
```{r}
#| code-fold: true

PLagrange <- function(nodos, f, tol = 1e-12) {
  n  <- length(nodos)
  yi <- vapply(nodos, f, numeric(1))
  #  w_i = 1 / ∏_{j≠i} (x_i - x_j)
  pesos <- numeric(n)
  for (i in seq_len(n)) {
    pesos[i] <- 1 / prod(nodos[i] - nodos[-i])
  }
  # Función que evalúa el polinomio de interpolación
  polinomio <- function(x) {
    x <- as.numeric(x)
    salida <- numeric(length(x))
    for (k in seq_along(x)) {
      diferencias <- abs(x[k] - nodos)
      j <- which.min(diferencias)
      if (diferencias[j] < tol) {
        # Caso: x coincide con un nodo
        salida[k] <- yi[j]
      } else {
        denominador <- sum(pesos / (x[k] - nodos))
        numerador   <- sum((pesos * yi) / (x[k] - nodos))
        salida[k]   <- numerador / denominador
      }
    }
    salida
  }
  return(polinomio)
}
```
:::
::: {.callout-note title="Método 2: Calcula los coeficientes del Polinomio de Lagrange"}

Salida: $c(a_1, a_2,\ldots, a_{n-1})$ donde $$P_n(x) = \sum_{k=0}^{n-1}a_kx^k$$
```{r}
#| code-fold: true
PLagrangeCoeficientes <- function(nodos, f) {
  n     <- length(nodos)
  valores <- vapply(nodos, f, numeric(1))
  # Multiplicación de polinomios: coeficientes en orden ascendente
  multiplicar_polinomios <- function(a, b) {
    resultado <- numeric(length(a) + length(b) - 1)
    for (i in seq_along(a)) {
      for (j in seq_along(b)) {
        resultado[i + j - 1] <- resultado[i + j - 1] + a[i] * b[j]
      }
    }
    resultado
  }
  # ∏ (x - r_j), devuelve vector de coeficientes
  polinomio_desde_raices <- function(raices) {
    polinomio <- 1
    for (r in raices) {
      polinomio <- multiplicar_polinomios(polinomio, c(-r, 1))
    }
    polinomio
  }
  coeficientes <- numeric(n)  # grado ≤ n-1 → longitud n
  for (i in seq_len(n)) {
    raices_i     <- nodos[-i]
    numerador    <- polinomio_desde_raices(raices_i)   # longitud n
    denominador  <- prod(nodos[i] - raices_i)
    coeficientes <- coeficientes + valores[i] * (numerador / denominador)
  }
  return(coeficientes)
}
```

:::

::: {.callout-tip collapse="true" title="Ejemplo"}
 
1) Polinomio de Lagrange
```{r}
#| code-fold: true
## Nodos y función
nodos <- c(0, 1, 2)
f     <- function(x) exp(x)

## 1) Polinomio de Lagrange
P <- PLagrange(nodos, f)

## Evalua en algunos puntos
x_vals <- seq(0, 2, length.out = 5)
data.frame(x = x_vals,
           exp_x = exp(x_vals),
           P_x   = P(x_vals))
```
2) Obtener coeficientes del polinomio de Lagrange
```{r}
#| echo: false
## 2) Obtener coeficientes del polinomio de Lagrange
coefs <- PLagrangeCoeficientes(nodos, f)
coefs
```
3) Graficar
```{r}
#| echo: false

# --- Preparar datos para graficar ---
xi <- seq(-1, 3, length.out = 400)
df <- data.frame(
  x   = xi,
  expx = exp(xi),
  Lx   = P(xi)
)

# Nodos para marcar los puntos de interpolación
df_nodes <- data.frame(x = nodos, y = f(nodos))

# --- Gráfico ---
ggplot(df, aes(x = x)) +
  geom_line(aes(y = expx, color = "exp(x)"), linewidth = 1) +
  geom_line(aes(y = Lx,   color = "L(x)"),  linewidth = 1, linetype = "dashed") +
  geom_point(data = df_nodes, aes(x = x, y = y), shape = 21, size = 3, fill = "white") +
  scale_color_manual(values = c("exp(x)" = "blue", "L(x)" = "red")) +
  labs(title = "Interpolación de Lagrange",
       subtitle = "Nodos {0,1,2}, f(x) = exp(x)",
       y = "Valor", color = "Función") +
  theme_minimal(base_size = 14)
```
:::
::: {.callout-tip collapse="true" title="Ejemplo"}

1) Polinomio de Lagrange
```{r}
# --- Nodos y función ---
nodos <- c(0, 3, 6, 9, 12, 15)
f     <- function(x) exp(x)

# Polinomio de Lagrange
P <- PLagrange(nodos, f)

# --- Datos para graficar ---
xi <- seq(0, 15, length.out = 600)   # intervalo que cubre todos los nodos
df <- data.frame(
  x    = xi,
  expx = exp(xi),
  Lx   = P(xi)
)
df_nodes <- data.frame(x = nodos, y = f(nodos))
```
2) Obtener coeficientes del polinomio de Lagrange
```{r}
#| code-fold: true
## 2) Obtener coeficientes del polinomio de Lagrange
coefs <- PLagrangeCoeficientes(nodos, f)
coefs
```
3) Grafico
```{r}
# --- Gráfico ---

ggplot(df, aes(x = x)) +
  geom_line(aes(y = expx, color = "exp(x)"), linewidth = 1) +
  geom_line(aes(y = Lx,   color = "L(x)"),  linewidth = 1, linetype = "dashed") +
  geom_point(data = df_nodes, aes(x = x, y = y), shape = 21, size = 3, fill = "white") +
  scale_color_manual(values = c("exp(x)" = "blue", "L(x)" = "red")) +
  labs(title = "Interpolación de Lagrange",
       subtitle = "Nodos {0, 3, 6, 9, 12, 15},  f(x) = exp(x)",
       y = "Valor", color = "Función") +
  theme_minimal(base_size = 14)
```
:::

#### Interpolación iterada

::: {.callout-note title="Definición: Polinomio de Lagrange en subconjuntos de nodos"}

Sea $f$ una función definida en $x_0, x_1, \ldots, x_n$ y sean $0 \le m_i \le n$, para $i = 1, 2, \ldots, k$, entonces el polinomio de Lagrange de grado $\le (k - 1)$ que coincide con $f$ en $x_{m_1}, x_{m_2}, \ldots, x_{m_k}$ se denota por:
$$
P_{m_1, m_2, \ldots, m_k}(x).
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = x^3$, $x_0 = 1$, $x_1 = 2$, $x_2 = 3$, $x_3 = 4$, $x_4 = 6$, calcule $P_{0,3,4}(x)$.

**Solución:** $P_{0,3,4}(x)$ es el polinomio que coincide con $f$ en $x_0 =  = 1$, $x_3 = 4$, $x_4 = 6$, de donde:
\begin{align*}
P_{0,3,4}(x) &= \frac{(x - 4)(x - 6)}{(1 - 4)(1 - 6)} 1^3 + \frac{(x - 1)(x - 6)}{(4 - 1)(4 - 6)} 4^3 + \frac{(x - 1)(x - 4)}{(6 - 1)(6 - 4)} 6^3 \\
&= 11x^2 - 34x + 24.
\end{align*}
$\blacksquare$
:::
::: {.callout-tip collapse="true" title="Ejemplo"}


Calcule $P_{1,2,4}(x)$:

**Solución:**

\begin{align*}
P_{1,2,4}(x) &= \frac{(x - 3)(x - 6)}{(2 - 4)(2 - 6)} 2^3 
+ \frac{(x - 2)(x - 6)}{(3 - 2)(3 - 6)} 3^3 
+ \frac{(x - 2)(x - 3)}{(6 - 2)(6 - 3)} 6^3 \\
&= 10x^2 - 27x + 18.
\end{align*}
$\blacksquare$

:::

Los siguientes párrafos se dedican a encontrar un método para calcular los polinomios de Lagrange en forma recursiva.

::: {.callout-note title="Teorema"}


Sea $f$ una función definida en $x_0, x_1, \ldots, x_k$ y sea $x_i \ne x_j$ con $i,j \in \{0, 1, 2, \ldots, k\}$. Entonces el polinomio de Lagrange que coincide con $f$ en $x_0, x_1, \ldots, x_k$ se puede escribir como:

\begin{align*}
P(x) = \frac{(x - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x) - (x - x_i) P_{0,1,\ldots,(i-1),(i+1),\ldots,k}(x)}{x_i - x_j}.
\end{align*}

::: {.callout-caution collapse="true" title="Prueba"}


Hay que probar que $P(x_s) = f(x_s)$, $\ \forall \ s = 0, 1, 2, \ldots, k$.

**Primer caso:**  
Sea $x_r \ne x_i$ y $x_r \ne x_j$ un nodo:

\begin{align*}
P(x_r) &= \frac{(x_r - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x_r) - (x_r - x_i) P_{0,1,\ldots,(i-1),(i+1),\ldots,k}(x_r)}{x_i - x_j} \\
&= \frac{(x_r - x_j) f(x_r) - (x_r - x_i) f(x_r)}{x_i - x_j} \\
&= \frac{(-x_j + x_i) f(x_r)}{x_i - x_j} \\
&= f(x_r).
\end{align*}

**Segundo caso:**  
Sea $x_r = x_i$:

\begin{align*}
P(x_r) &= \frac{(x_i - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x_i) - 0}{x_i - x_j} \\
&= \frac{(x_i - x_j)}{(x_i - x_j)} f(x_i) \\
&= f(x_i).
\end{align*}

Es análogo si $x_r = x_j$, por lo tanto $P(x)$ es el polinomio de Lagrange que coincide con $f$ en $x_0, x_1, \ldots, x_k$ pues este es único. $\blacksquare$
:::
:::

#### Método de Neville

Se desea aproximar $f(x^*)$ dada la siguiente tabla de valores para $f$:

\begin{align*}
\begin{array}{c|c}
x & f(x) \\
\hline
x_0 & f(x_0) \\
x_1 & f(x_1) \\
\vdots & \vdots \\
x_n & f(x_n)
\end{array}
\end{align*}

Se genera la tabla de $f(x^*)$:

$$
\begin{array}{ccccccc}
x_0 & P_0 \\
x_1 & P_1 & P_{0,1} \\
x_2 & P_2 & P_{1,2} & P_{0,1,2} \\
x_3 & P_3 & P_{2,3} & P_{1,2,3} & P_{0,1,2,3} \\
x_4 & P_4 & P_{3,4} & P_{2,3,4} & P_{1,2,3,4} & P_{0,1,2,3,4} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & P_n & P_{n-1,n} & P_{n-2,n-1,n} & P_{n-3,n-2,n-1,n} & \cdots & P_{0,1,\ldots,n}
\end{array}
$$

Con $P_i(x) = f(x_i)$ una función constante, polinomio de Lagrange de grado 0.  
Esta tabla puede ser calculada usando el Teorema anterior, veamos algunos ejemplos:


\begin{align*}
P_{0,1}(x) &= \frac{(x - x_0)P_1 - (x - x_1)P_0}{x_1 - x_0} \\
P_{1,2}(x) &= \frac{(x - x_1)P_2 - (x - x_2)P_1}{x_2 - x_1} \\
&\vdots \\
P_{n-1,n}(x) &= \frac{(x - x_{n-1})P_n - (x - x_n)P_{n-1}}{x_n - x_{n-1}} \\
P_{0,1,2}(x) &= \frac{(x - x_0)P_{1,2} - (x - x_2)P_{0,1}}{x_2 - x_0} \\
P_{1,2,3}(x) &= \frac{(x - x_1)P_{2,3} - (x - x_3)P_{1,2}}{x_3 - x_1} \\
&\vdots \\
P_{n-2,n-1,n}(x) &= \frac{(x - x_{n-2})P_{n-1,n} - (x - x_n)P_{n-2,n-1}}{x_n - x_{n-2}} \\
P_{0,1,2,3}(x) &= \frac{(x - x_0)P_{1,2,3} - (x - x_3)P_{0,1,2}}{x_3 - x_0} \\
P_{1,2,3,4}(x) &= \frac{(x - x_1)P_{2,3,4} - (x - x_4)P_{1,2,3}}{x_4 - x_1} \\
&\vdots
\end{align*}

::: {.callout-tip collapse="true" title="Ejemplo"}


Aproxime $f(2.5)$ dada la siguiente tabla:

$$
\begin{array}{c|c}
x & f(x) \\
\hline
x_0 = 2.0 & 0.5103757 \\
x_1 = 2.2 & 0.5207843 \\
x_2 = 2.4 & 0.5104147 \\
x_3 = 2.6 & 0.4813306 \\
x_4 = 2.8 & 0.4359160
\end{array}
$$

**Solución:**

Construimos la tabla de Neville:

\begin{align*}
x_0 &: P_0 \\
x_1 &: P_1 \quad P_{0,1} \\
x_2 &: P_2 \quad P_{1,2} \quad P_{0,1,2} \quad f(2.5) \\
x_3 &: P_3 \quad P_{2,3} \quad P_{1,2,3} \quad P_{0,1,2,3} \\
x_4 &: P_4 \quad P_{3,4} \quad P_{2,3,4} \quad P_{1,2,3,4} \quad P_{0,1,2,3,4}
\end{align*}

La tabla de Neville es:

$$
\begin{array}{cccccc}
x_0 & 0.5103757 \\
x_1 & 0.5207843 & \boxed{0.5363972} & \hookleftarrow P_{0,1}\\
x_2 & 0.5104147 & 0.5052299 & 0.4974380 \\
x_3 & 0.4813306 & 0.4958726 & 0.4982119 & 0.4980829 \\
x_4 & 0.4359160 & 0.5040379 & 0.4979139 & 0.4980629 & 0.49807047 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
$$

De donde $f(2.5) \approx 0.49807047$. Un ejemplo del cálculo en la matriz anterior es:

\begin{align*}
P_{0,1}(x) 
&= \frac{(x - x_0)P_1 - (x - x_1)P_0}{x_1 - x_0} \\
&= \frac{(2.5 - 2.0) \cdot 0.5207843 - (2.5 - 2.2) \cdot 0.5103757}{2.2 - 2.0} \\
&= 0.5363972.
\end{align*}

$\blacksquare$

:::


::: {.callout-note title="Notación"}

Se denota por $Q_{ij}$ el polinomio interpolante de Lagrange de grado $j$ que pasa por los $j + 1$ nodos siguientes:

$$
x_{i-j},\ x_{i-j+1},\ \ldots,\ x_{i-1},\ x_i
$$

es decir,

$$
Q_{ij} = P_{i-j,i-j+1,\ldots,i-1,i}(x).
$$

Ahora, usando el método de Neville (teorema anterior):

\begin{align*}
Q_{ij} 
&= \frac{(x - x_i)P_{i-j,i-j+1,\ldots,i-1}(x) - (x - x_{i-j})P_{i-j+1,\ldots,i}(x)}{x_i - x_{i-j}} \\
&= \frac{(x - x_{i-j})P_{i-j+1,\ldots,i}(x) - (x - x_i)P_{i-j,\ldots,i-1}(x)}{x_i - x_{i-j}} \\
&= \frac{(x - x_{i-j})Q_{i,j-1} - (x - x_i)Q_{i-1,j-1}}{x_i - x_{i-j}}.
\end{align*}

Pues:

$$
P_{i-j+1,i-j+2,\ldots,i-1,i} = Q_{i,j-1} \quad \text{dado que } (i - (j - 1)) = i - j + 1,
$$

$$
P_{i-j,i-j+1,\ldots,i-1} = Q_{i-1,j-1} \quad \text{dado que } (i - 1 - (j - 1)) = i - j.
$$

Note que:

$$
Q_{i0} = P_i = f(x_i), \quad \forall i = 0,1,\ldots,n.
$$

Con esta nueva notación, la tabla de Neville se puede escribir como:

$$
\begin{array}{cccccccc}
x_0 & Q_{00} \\
x_1 & Q_{10} & Q_{11} \\
x_2 & Q_{20} & Q_{21} & Q_{22} \\
x_3 & Q_{30} & Q_{31} & Q_{32} & Q_{33} \\
x_4 & Q_{40} & Q_{41} & Q_{42} & Q_{43} & Q_{44} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & Q_{n0} & Q_{n1} & Q_{n2} & Q_{n3} & \cdots & Q_{nn}
\end{array}
$$

$$
Q_{22} = P_{0,1,2}, \quad Q_{nn} = P_{0,1,\ldots,n}
$$
:::

::: {.callout-note title="Algoritmo: Para calcular la tabla de Neville y aproximar $f(x^*) \approx P_n(x^*)$"}

**Entrada:**  
Los nodos $x_0, x_1, \ldots, x_n$.  
Sus imágenes $f(x_0), f(x_1), \ldots, f(x_n)$ como primera columna de la matriz $Q$, es decir $Q_{00}, Q_{10}, Q_{n0}$.

**Salida:**  
La tabla o matriz $Q$, donde $f(x^*) \approx Q_{nn}$.

**Paso 1:** Para $i = 1$ hasta $n$  
\quad Para $j = 1, 2, \ldots, i$
$$
Q_{ij} = \frac{(x - x_{i-j})Q_{i,j-1} - (x - x_i)Q_{i-1,j-1}}{x_i - x_{i-j}}.
$$

**Paso 2:** Salida $Q_{nn}$, parar.  
FIN
:::

#### Diferencias divididas de Newton

La ventaja de este método es que permite calcular el polinomio de Lagrange en cualquier punto $x$.

::: {.callout-note title="Notación: Recursiva"}

\begin{align*}
f[x_i] &:= f(x_i) \\
f[x_i, x_{i+1}] &:= \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i} \\
f[x_i, x_{i+1}, x_{i+2}] &:= \frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i} \\
&\vdots \\
f[x_i, x_{i+1}, \ldots, x_{i+k}] &:= \frac{f[x_{i+1}, x_{i+2}, \ldots, x_{i+k}] - f[x_i, x_{i+1}, \ldots, x_{i+k-1}]}{x_{i+k} - x_i}
\end{align*}
:::

::: {.callout-note title="Teorema"}


Si $P_n(x)$ es el polinomio de Lagrange que coincide con $f(x)$ en $x_0, x_1, \ldots, x_n$, entonces:

\begin{align*}
P_n(x) &= f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) \\
&\quad + \cdots + f[x_0, x_1, \ldots, x_n](x - x_0)(x - x_1) \cdots (x - x_{n-1}) \\
&= f[x_0] + \sum_{k=1}^n f[x_0, \ldots, x_k](x - x_0) \cdots (x - x_{k-1}).
\end{align*}

::: {.callout-caution collapse="true" title="Prueba"}


Si $P_n(x)$ se escribe de la forma

\begin{align*}
P_n(x) &= a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \cdots + \\
&\quad a_n(x - x_0)(x - x_1)\cdots(x - x_{n-1}),
\end{align*}

entonces:

$$
P_n(x_0) = a_0, \quad \text{como } P_n(x_0) = f(x_0) \Rightarrow a_0 = f(x_0) = f[x_0].
$$

Además:

$$
P_n(x_1) = a_0 + a_1(x_1 - x_0), \quad \text{como } P_n(x_1) = f(x_1) \text{ y } a_0 = f[x_0]
\Rightarrow f[x_0] + a_1(x_1 - x_0) = f(x_1)
$$

$$
\Rightarrow a_1 = \frac{f[x_1] - f[x_0]}{x_1 - x_0} = f[x_0, x_1].
$$

Luego, por inducción se puede probar fácilmente que $a_k = f[x_0, x_1, \ldots, x_k]$ (ejercicio). $\blacksquare$
:::
:::

La **Tabla de diferencias divididas de Newton** es la siguiente:

$$
\begin{array}{cccccc}
x_0 & f[x_0] \\
x_1 & f[x_1] & f[x_0, x_1] \\
x_2 & f[x_2] & f[x_1, x_2] & f[x_0, x_1, x_2] \\
\vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & f[x_n] & f[x_{n-1}, x_n] & f[x_{n-2}, x_{n-1}, x_n] & \cdots & f[x_0, x_1, \ldots, x_n]
\end{array}
$$

En la diagonal de la matriz anterior están los coeficientes del polinomio de Lagrange $P_n(x)$, según la forma presentada en el teorema anterior. El siguiente algoritmo calcula el polinomio de Lagrange usando la Tabla de diferencias divididas de Newton.

::: {.callout-note title="Algoritmo"}

**Objetivo**: Calcular el polinomio de Lagrange usando la Tabla de diferencias divididas de Newton.

**Entrada**: Los nodos $x_0, x_1, \ldots, x_n$ y los valores $f(x_0), f(x_1), \ldots, f(x_n)$ como primera columna de la matriz $F$.

**Salida**: $F_{00}, F_{11}, \ldots, F_{nn}$, los coeficientes de $P_n(x)$, donde:  
$$
P_n(x) = \sum_{i=0}^n F_{ii} \prod_{k=0}^{i-1} (x - x_k)
$$


**Paso 1**: Para $i = 1, \ldots, n$

\quad\quad Para $j = 1, 2, \ldots, i$

$$
F_{ij} = \frac{F_{i,j-1} - F_{i-1,j-1}}{x_i - x_{i-j}}
$$

**Paso 2**: Salida $(F_{00}, F_{11}, \ldots, F_{nn})$  
Parar.
:::


### Interpolación de Hermite


Los *Polinomios Osculantes* generalizan a los Polinomios de Taylor y a los Polinomios de Lagrange como veremos más adelante. 
La idea del polinomio de Hermite (que es un caso particular de los polinomios osculantes) es que si se tienen $x_0, x_1, \ldots, x_n$, $n+1$ nodos, entonces $f$ y $f'$ coincidan con $P(x)$ y $P'(x)$ en los nodos respectivamente.

::: {.callout-note title="Definición: Polinomio osculante"}

- Sean $x_0, x_1, \ldots, x_n$, $n + 1$ nodos distintos en $[a,b]$.
- Sea $m_i$ un entero no negativo, con $m_i$ asociado a $x_i$; para $i = 0,1,\ldots,n$.
- Sea $m = \max_{0 \leq i \leq n} m_i$.
- Sea $f \in C^m[a,b]$.

Entonces el **Polinomio Osculante** que aproxima a $f$ es el polinomio de grado menor tal que:

$$
\frac{d^k P(x_i)}{dx^k} = \frac{d^k f(x_i)}{dx^k},
$$

para $i = 0,1,\ldots,n$ y $k = 1,\ldots,m_i$. Es decir, en el $i$-ésimo nodo el polinomio y la función $f$ coinciden hasta la derivada $m_i$.
:::

::: {.callout-important collapse="true" title="Observación"}


1. Si $n = 0$, el polinomio osculante es el polinomio de Taylor de grado $m_0$ para $f$ en $x_0$.

Para ver esto, sea $P(x)$ polinomio de Taylor de grado $m_0$ para $f$ en $x_0$, entonces:

$$
P(x) = f(x_0) + f'(x_0)(x - x_0) + f''(x_0) \frac{(x - x_0)^2}{2!} + \cdots + f^{(m_0)}(x_0) \frac{(x - x_0)^{m_0}}{m_0!}
$$

De donde se deduce que: $P(x_0) = f(x_0)$.

Además:

$$
P'(x) = f'(x_0) + 2f''(x_0) \frac{(x - x_0)}{2!} + \cdots + m_0 f^{(m_0)}(x_0) \frac{(x - x_0)^{m_0 - 1}}{m_0!},
$$

lo cual implica que: $P'(x_0) = f'(x_0)$ y así sucesivamente se puede probar que $P^{(k)}(x_0) = f^{(k)}(x_0)$ para todo $k \leq m_0$.

2. Si $m_i = 0$, e $i = 0,1,\ldots,n$, entonces el polinomio osculante es el polinomio de Lagrange que interpola a $f$ en $x_0, x_1, \ldots, x_n$.  
Pues se está pidiendo que solamente coincida con $f$ en $x_0, x_1, \ldots, x_n$ (no en sus derivadas) y se probó que el polinomio de menor grado que hace esto es el de Lagrange, además se probó que es único.
:::

Cuando $P(x)$ coincide con $f$ y $f'$ en $x_0, x_1, \ldots, x_n$, se dice que $P(x)$ tiene "la misma apariencia" que $f$ en los nodos y se denomina el **Polinomio de Hermite**.

El siguiente Teorema da un método para calcular el Polinomio de Hermite.

::: {.callout-note title="Teorema: Polinomio de Hermite (osculante con derivadas)"}

- Sea $f \in C[a,b]$.
- Sean $x_0, x_1, \ldots, x_n$, $(n+1)$ nodos distintos en $[a,b]$.

Entonces el polinomio de grado menor que coincide con $f$ y $f'$ en $x_0, x_1, \ldots, x_n$:

- Tiene grado $2n+1$.
- Está dado por

$$
H_{2n+1}(x) = \sum_{j=0}^{n} f(x_j) H_{nj}(x) + \sum_{j=0}^{n} f'(x_j) \widetilde{H}_{nj}(x),
$$

donde

$$
H_{nj}(x) = \big[1 - 2(x - x_j)L'_{nj}(x_j)\big]L_{nj}^2(x),
$$

y

$$
\widetilde{H}_{nj}(x) = (x - x_j) L_{nj}^2(x).
$$

- Además, el error absoluto es:

$$
\left| f(x) - H_{2n+1}(x) \right| = \left| \frac{(x - x_0)^2 \cdots (x - x_n)^2}{(2n+2)!} f^{(2n+2)}(\xi) \right|, \quad \text{con } \xi \in ]a, b[.
$$

::: {.callout-caution collapse="true" title="Prueba"}


- Se debe demostrar que $H_{2n+1}(x_i) = f(x_i)$ para todo $i = 0, 1, \ldots, n$. Para ver esto, recordemos que:

$$
L_{nj}(x_i) =
\begin{cases}
0 & \text{si } i \ne j \\
1 & \text{si } i = j
\end{cases}
$$

de donde, cuando $i \ne j$:

$$
H_{nj}(x_i) = 0 \quad \text{y} \quad \widetilde{H}_{nj}(x_i) = 0.
$$

Mientras que:

$$
H_{ni}(x_i) = [1 - 2(x_i - x_i)L'_{ni}(x_i)] \cdot 1 = 1,
$$

$$
\widetilde{H}_{ni}(x_i) = (x_i - x_i) \cdot 1^2 
$$

Luego:

$$
H_{2n+1}(x_i) = \sum_{\substack{j = 0 \\ j \ne i}}^n f(x_j) \cdot 0 + f(x_i) \cdot 1 + \sum_{j=0}^n f'(x_j) \cdot 0 = f(x_i).
$$

Por lo tanto:

- $H_{2n+1}(x_i) = f(x_i)$ para $i = 0, 1, 2, \ldots, n$.

- Se debe demostrar que $H'_{2n+1}(x_i) = f'(x_i)$ para todo $i = 0, 1, \ldots, n$.

Nótese que $L_{nj}(x)$ es un factor de $H'_{nj}(x)$, lo cual implica que $H'_{nj}(x_i) = 0$ cuando $i \ne j$.

Además, si $i = j$:

\begin{align*}
H'_{ni}(x_i) &= -2L'_{ni}(x_i)L^2_{ni}(x_i) + [1 - 2(x_i - x_i)L'_{ni}(x_i)] \cdot 2 \cdot L_{ni}(x_i)L'_{ni}(x_i) \\
&= -2L'_{ni}(x_i) + 2L'_{ni}(x_i) \\
&= 0.
\end{align*}

Por lo tanto, $H'_{nj}(x_i) = 0$ para todo $i = 0, 1, 2, \ldots, n$ y para todo $j = 0, 1, 2, \ldots, n$.

Además:

$$
\widetilde{H}_{nj}(x_i) = L^2_{nj}(x_i) + (x_i - x_j)L'_{nj}(x_j) \cdot 2 \cdot L_{nj}(x_i)L'_{nj}(x_i),
$$

de donde:

$$
\widetilde{H}'_{nj}(x_i) =
\begin{cases}
0 & \text{si } i \ne j \\
1 & \text{si } i = j
\end{cases}
$$

por lo tanto:

$$
\widetilde{H}'_{2n+1}(x_i) = \sum_{j=0}^n f(x_j) \cdot 0 + \sum_{\substack{j = 0 \\ j \ne i}}^n f'(x_j) \cdot 0 + f'(x_i) \cdot 1 = f'(x_i).
$$

es decir:

$$
H'_{2n+1}(x_i) = f'(x_i) \quad \text{para } i = 0, 1, 2, \ldots, n.
$$

- La unicidad queda de ejercicio al lector.

$\blacksquare$

:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Calcule $H_5(x)$ que aproxima a $f(x) = e^x$ en $x_0 = 0$, $x_1 = 1$, $x_2 = 2$.

**Solución:**

| $k$ | $x_k$ | $f(x_k)$     | $f'(x_k)$     |
|----:|:-----:|:------------:|:-------------:|
|  0  |   0   | 1            | 1             |
|  1  |   1   | 2.7182818    | 2.7182818     |
|  2  |   2   | 7.3890561    | 7.3890561     |

$$
H_5 = \sum_{j=0}^{2} f(x_j) H_{nj}(x) + \sum_{j=0}^{2} f'(x_j) \widetilde{H}_{nj}(x).
$$

Calculemos cada uno de los términos:

$$
H_{20} = [1 - 2(x - x_0) L'_{20}(x_0)] L_{20}^2(x),
$$

con

\begin{align*}
L_{20}(x) &= \frac{(x - 1)(x - 2)}{(0 - 1)(0 - 2)} \\
          &= \frac{x^2 - x - 2x + 2}{2} \\
          &= \frac{x^2 - 3x + 2}{2},
\end{align*}

esto implica que:

$$
L'_{20}(x) = \frac{2x - 3}{2},
$$

de donde:

$$
L'_{n0}(0) = -\frac{3}{2},
$$

luego:

$$
H_{20} = (1 - 3x) \left( \frac{x^2 - 3x + 2}{2} \right)^2.
$$


\begin{align*}
L_{21}(x) &= \frac{x(x - 2)}{(1 - 0)(1 - 2)} \\
         &= \frac{x(x - 2)}{-1} \\
         &= -x(x - 2) \\
         &= -x^2 + 2x.
\end{align*}

Esto implica que:

$$
L'_{21}(x) = -2x + 2,
$$

de donde:

$$
L'_{21}(x_1) = 0,
$$

por lo que:

\begin{align*}
H_{21} &= (1 - 2(x - 1) \cdot 0)(-x(x - 2))^2 \\
      &= x^2 (x - 2)^2.
\end{align*}

\begin{align*}
L_{22}(x) &= \frac{x(x - 1)}{(2 - 0)(2 - 1)} \\
         &= \frac{x(x - 1)}{2} \\
         &= \frac{x^2 - x}{2},
\end{align*}

de donde:

$$
L'_{22}(x) = \frac{2x - 1}{2},
$$

luego:

$$
L'_{22}(x_2) = \frac{3}{2},
$$

por lo que:


\begin{align*}
H_{22}(x) &= \left(1 - 2(x - 2) \cdot \frac{3}{2} \right) \left( \frac{x^2 - x}{2} \right)^2 \\
         &= (1 - 3(x - 2)) \left( \frac{x^2 - x}{2} \right)^2.
\end{align*}

Por otra parte,

\begin{align*}
\widehat{H}_{20}(x) &= (x - x_0) L_{20}^2(x) \\
                    &= x \left( \frac{x^2 - 3x + 2}{2} \right)^2.
\end{align*}

Análogamente:

$$
\widehat{H}_{21}(x) = (x - 1)x^2(x - 2)^2,
$$

$$
\widehat{H}_{22}(x) = (x - 2) \left( \frac{x^2 - x}{2} \right)^2.
$$

Finalmente:

\begin{align*}
H_5(x) =\; & (1 - 3x) \left( \frac{x^2 - 3x + 2}{2} \right)^2 + 2.7182818\,x^2(x - 2)^2 \\
          & +\; 7.3890561 (1 - 3(x - 2)) \left( \frac{x^2 - x}{2} \right)^2 \\
          & +\; x \left( \frac{x^2 - 3x + 2}{2} \right)^2 + 2.7182818 (x - 1)x^2(x - 2)^2 \\
          & +\; 7.3890561 (x - 2) \left( \frac{x^2 - x}{2} \right)^2.
\end{align*}

$\blacksquare$

---

2. Aproximando $f(0.25) \cong H(0.25)$ se tiene que:

$$
H_5(0.25) = 1.28364
$$

con el siguiente error absoluto:

Aproximando $e^{0.25} - H_5(0.25)$:

\begin{align*}
\left| e^{0.25} - H_5(0.25) \right| &= \left| 1.28402 - 1.28364 \right| \\
                                   &= 3.8 \times 10^{-5} \\
                                   &\cong 0.3 \times 10^{-4},
\end{align*}

mientras que utilizando el polinomio de Lagrange el error fue:

$$
\left| P(0.25) - e^{0.25} \right| \cong 0.1312511,
$$

que es mucho mayor.

---

3. Calculando una cota para el error teórico se tiene que:

$$
|e^x - H_5(x)| = \left| \frac{x^2(x - 1)^2(x - 2)^2}{(2 \cdot 2 + 2)!} e^{\xi} \right|, \quad \text{con } \xi \in [0, 2],
$$

esto implica que:

\begin{align*}
|e^{0.25} - H_5(0.25)| &\leq \frac{(0.25)^2 (0.75)^2 (1.75)^2}{6!} e^2 \\
                       &= \frac{0.7955}{720} \\
                       &= 1.1 \times 10^{-3} \\
                       &= 0.1 \times 10^{-2}.
\end{align*}

Como se ha visto, el cálculo del polinomio de Hermite es sumamente tedioso, por esto en la siguiente sección se propone un algoritmo que facilite dicho cálculo.
:::

#### Algoritmo para el polinomio de Hermite

Sabemos que:

\begin{align*}
P_n(x) = f[x_0] + \sum_{k=1}^{n} f[x_0, x_1, \dots, x_k](x - x_0) \cdots (x - x_{k-1}),
\end{align*}

además utilizaremos el siguiente lema.

::: {.callout-note title="Lema: Generalización del Teorema del valor medio"}

Si $f \in C^n[a, b]$ y $x_0, x_1, \dots, x_n$ son los $(n+1)$ nodos distintos en $[a,b]$, entonces:  
existe $\xi \in ]a, b[$ tal que:

$$
f[x_0, x_1, \dots, x_n] = \frac{f^{(n)}(\xi)}{n!}.
$$

::: {.callout-caution collapse="true" title="Prueba"}


Ejercicio al lector. $\blacksquare$
:::
:::

Suponga que se conocen

\begin{align*}
\begin{array}{ccc}
x_0 & f(x_0) & f'(x_0) \\
x_1 & f(x_1) & f'(x_1) \\
\vdots & \vdots & \vdots \\
x_n & f(x_n) & f'(x_n)
\end{array}
\end{align*}

Definimos la sucesión

$$
\{z_n\}_{n \in \mathbb{N}}, \; z_{2i} = z_{2i+1} = x_i, \quad \text{para } i = 0, 1, \dots, n.
$$

Luego se forma la tabla de Hermite como sigue:

\begin{align*}
\begin{array}{lllll}
z_0 = x_0 & f[z_0] = f(x_0) \\
z_1 = x_0 & f[z_1] = f(x_0) & f[z_0,z_1] \approx f'(z_0) \\
z_2 = x_1 & f[z_2] = f(x_1) & f[z_1,z_2] & f[z_0, z_1, z_2] \\
z_3 = x_1 & f[z_3] = f(x_1) & f[z_2,z_3] \approx f'(z_1) & f[z_1,z_2,z_3] & f[z_0,z_1,z_2,z_3] \\
z_4 = x_2 & f[z_4] = f(x_2) & f[z_3,z_4] & f[z_2,z_3,z_4] & f[z_1,z_2,z_3,z_4] \\
z_5 = x_2 & f[z_5] = f(x_2) & f[z_4,z_5] \approx f'(z_2) & f[z_3,z_4,z_5] & \cdots \\
\quad\;\vdots & \quad\;\vdots & \quad\;\vdots & \quad\;\vdots & \quad\;\vdots \\
\end{array}
\end{align*}

A partir de la tercera columna de la matriz anterior el cálculo se hace exactamente igual que en el método de Neville.  
Además, nótese que $f[z_0, z_1]$ no se puede calcular usando la definición, pues daría $\frac{0}{0}$, pero resulta “razonable” tomar $f[z_0, z_1] \approx f'(x_0)$. ¿Por qué?

Luego:

\begin{align*}
H_{2n+1}(x) 
&= f[z_0] + f[z_0, z_1]\overbrace{(x - z_0)}^{(z-z_0)} + f[z_0, z_1, z_2]\overbrace{(x - z_0)^2}^{(z-z_0)(z-z_1)} \\
&\quad + f[z_0, z_1, z_2, z_3](x - z_0)^2(x - z_1) \\
&\quad + f[z_0, z_1, z_2, z_3, z_4](x - z_0)^2(x - z_1)^2 + \cdots
\end{align*}

::: {.callout-note title="Algoritmo: Para obtener los coeficientes del polinomio de Hermite"}

**Entrada:** $x_0, x_1, \dots, x_n;\; f(x_0), f(x_1), \dots, f(x_n)\;$ y $\;f'(x_0), f'(x_1), \dots, f'(x_n)$  
**Salida:** Los números $Q_{0,0}, Q_{1,1}, \dots, Q_{(2n+1),(2n+1)}$ coeficientes de:

\begin{align*}
H_{2n+1}(x) &= Q_{0,0} + \\
&\quad Q_{1,1}(x - x_0) + \\
&\quad Q_{2,2}(x - x_0)^2 + \\
&\quad Q_{3,3}(x - x_0)^2(x - x_1) + \\
&\quad Q_{4,4}(x - x_0)^2(x - x_1)^2 + \cdots + \\
&\quad Q_{(2n+1),(2n+1)}(x - x_0)^2 \cdots (x - x_{n-1})^2(x - x_n)
\end{align*}

**Paso 1**: Para $i = 1, \dots, n$ siga pasos 2–3

**Paso 2**: Tomar  
\begin{align*}
z_{2i} &= x_i \\
z_{2i+1} &= x_i \\
Q_{2i,0} &= f(x_i) \\
Q_{2i+1,0} &= f(x_i) \\
Q_{2i+1,1} &= f'(x_i)
\end{align*}

**Paso 3**: Si $i \ne 0$, tome  
$$
Q_{2i,1} = \frac{Q_{2i,0} - Q_{2i-1,0}}{z_{2i} - z_{2i-1}}
$$

**Paso 4**: Para $i = 2,3,\dots, 2n+1$  
\quad Para $j = 2,3,\dots,i$ tomar  
$$
Q_{i,j} = \frac{Q_{i,j-1} - Q_{i-1,j-1}}{z_i - z_{i-j}}
$$

**Paso 5**: Salida $\big(Q_{0,0}, Q_{1,1}, \dots, Q_{(2n+1),(2n+1)}\big)$.  
**Parar**
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Si se corre el algoritmo con $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, entonces:

\begin{align*}
Q_{0,0} &= 1 \\
Q_{1,1} &= 1 \\
Q_{2,2} &= 0.71828183 \\
Q_{3,3} &= 0.28171817 \\
Q_{4,4} &= 0.09726402 \\
Q_{5,5} &= 0.02375378
\end{align*}

Así se obtiene el siguiente polinomio:

\begin{align*}
H_5(x) &= 1 + x + 0.71828183x^2 + 0.28171817x^2(x - 1) \\
&\quad + 0.09726402x^2(x - 1)^2 + 0.02375378x^2(x - 1)^2(x - 2)
\end{align*}
:::

::: {.callout-important title="NOTA"}

Ver `hermite.nb`.
:::

### Interpolación por Splines Cúbicos

#### Presentación geométrica

![](Imagenes/Interpolación por Splines Cúbicos.png)

Es encontrar polinomios cúbicos tales que:

- Coincidan con $f(x)$ en los nodos y $P(x)$ sea continuo.
- $P(x)$ tenga primera derivada en los nodos internos.
- $P(x)$ tenga segunda derivada en los nodos internos.

Así, se deben encontrar $n$ polinomios con cuatro coeficientes cada uno. Por lo tanto, se tienen $4n$ incógnitas. Para encontrarlas, se deben establecer $4n$ ecuaciones.

¿Cómo se encuentran tales ecuaciones?

1. Condición de continuidad en los nodos internos

 Como $P(x)$ debe coincidir con $f$ en los nodos internos y $P(x)$ debe ser continuo, se tienen las siguientes ecuaciones:

$$
\left\{
\begin{aligned}
a_{i-1}x_{i-1}^3 + b_{i-1}x_{i-1}^2 + c_{i-1}x_{i-1} + d_{i-1} &= f(x_{i-1}) \\
a_i x_{i-1}^3 + b_i x_{i-1}^2 + c_i x_{i-1} + d_i &= f(x_{i-1})
\end{aligned}
\right\}
\quad \text{para } i = 2, \dots, n
$$

 De aquí se tienen $2(n - 1) = 2n - 2$ ecuaciones.

2. Condición en los extremos

 Como $P(x)$ debe coincidir con $f$ en los extremos, se tienen las siguientes ecuaciones:

$$
\left\{
\begin{aligned}
a_1 x_0^3 + b_1 x_0^2 + c_1 x_0 + d_1 &= f(x_0) \\
a_n x_n^3 + b_n x_n^2 + c_n x_n + d_n &= f(x_n)
\end{aligned}
\right.
$$

 De aquí se tienen 2 ecuaciones.

3. Como las primeras derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:

\begin{align*}
3a_{i-1}x_{i-1}^2 + 2b_{i-1}x_{i-1} + c_{i-1} &= 3a_i x_{i-1}^2 + 2b_i x_{i-1} + c_i,
\quad \text{con } i = 2, \ldots, n.
\end{align*}

 De aquí se tienen $(n - 1)$ ecuaciones.

4. Como las segundas derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:

\begin{align*}
6a_{i-1}x_{i-1} + 2b_{i-1} &= 6a_i x_{i-1} + 2b_i,
\quad \text{con } i = 2, \ldots, n.
\end{align*}

 De aquí se tienen $(n - 1)$ ecuaciones.

En total tenemos: $2n - 2 + 2 + (n - 1) + (n - 1) = 4n - 2$ por lo que faltan todavía 2 ecuaciones.

5. Asumiendo que las segundas derivadas en los nodos extremos deben ser 0, se obtienen 2 ecuaciones más:

\begin{align*}
\left\{
\begin{aligned}
6a_1 x_0 + 2b_1 &= 0 \\
6a_{n-1} x_n + 2b_{n-1} &= 0
\end{aligned}
\right.
\end{align*}

::: {.callout-tip collapse="true" title="Ejemplo"}

Para $f(x)$ dada por la siguiente tabla:

$$
\begin{array}{c|c}
x & f(x) \\
\hline
3.0 & 2.5 \\
4.5 & 1.0 \\
7.0 & 2.5 \\
9.0 & 0.5
\end{array}
$$

Calcule el polinomio de interpolación usando Splines Cúbicos.


**Solución:** Se requieren $3 \cdot 4 = 12$ ecuaciones, las cuales se obtienen como sigue:

1. Como $P(x)$ debe coincidir con $f$ en los nodos internos y $P(x)$ debe ser continuo, se tienen las siguientes ecuaciones:

\begin{align*}
91.125a_1 + 20.25b_1 + 4.5c_1 + d_1 &= 1 \\
91.125a_2 + 20.25b_2 + 4.5c_2 + d_2 &= 1 \\
343a_2 + 49b_2 + 7c_2 + d_2 &= 2.5 \\
343a_3 + 49b_3 + 7c_3 + d_3 &= 2.5
\end{align*}

2. Como $P(x)$ debe coincidir con $f$ en los extremos, se tienen las siguientes ecuaciones:

\begin{align*}
27a_1 + 9b_1 + 3c_1 + d_1 &= 2.5 \\
729a_3 + 81b_3 + 9c_3 + d_3 &= 0.5
\end{align*}

 Igualdad de derivadas en nodos internos

 Como las primeras derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:
	- con $x = 4.5$:

\begin{align*}
60.75a_1 + 9b_1 + 3c_1 - 60.75a_2 - 9b_2 - c_2 &= 0,
\end{align*}
	- con $x = 7$:

\begin{align*}
147a_2 + 14b_2 + c_2 - 147a_3 - 14b_3 - c_3 &= 0,
\end{align*}
	- con $x = 4.5$:

\begin{align*}
27a_1 + 2b_1 - 27a_2 - 2b_2 &= 0,
\end{align*}
	- con $x = 7$:

\begin{align*}
42a_2 + 2b_2 - 42a_3 - 2b_3 &= 0.
\end{align*}

Condición de derivadas segundas nulas en extremos

Asumiendo que las segundas derivadas en los nodos extremos deben ser $0$, se obtienen 2 ecuaciones más:
	- con $x = 3$:

\begin{align*}
18a_1 + 2b_1 &= 0,
\end{align*}
	- con $x = 9$:

\begin{align*}
54a_3 + 2b_3 &= 0.
\end{align*}

Sistema completo de ecuaciones

De donde, resolviendo el siguiente sistema de ecuaciones:

\begin{align*}
&91.125a_1 + 20.25b_1 + 4.5c_1 + d_1 &&= 1 \\
&91.125a_2 + 20.25b_2 + 4.5c_2 + d_2 &&= 1 \\
&343a_2 + 49b_2 + 7c_2 + d_2 &&= 2.5 \\
&343a_3 + 49b_3 + 7c_3 + d_3 &&= 2.5 \\
&27a_1 + 9b_1 + 3c_1 + d_1 &&= 2.5 \\
&729a_3 + 81b_3 + 9c_3 + d_3 &&= 0.5 \\
&60.75a_1 + 9b_1 + 3c_1 - 60.75a_2 - 9b_2 - c_2 &&= 0 \\
&147a_2 + 14b_2 + c_2 - 147a_3 - 14b_3 - c_3 &&= 0 \\
&27a_1 + 2b_1 - 27a_2 - 2b_2 &&= 0 \\
&42a_2 + 2b_2 - 42a_3 - 2b_3 &&= 0 \\
&18a_1 + 2b_1 &&= 0 \\
&54a_3 + 2b_3 &&= 0
\end{align*}

se obtiene la siguiente solución: 

\begin{align*}
a_1 &= 0.187 & a_2 &= -0.214 & a_3 &= 0.128 \\
b_1 &= -1.679 & b_2 &= 3.73 & b_3 &= -3.449 \\
c_1 &= 3.617 & c_2 &= -20.726 & c_3 &= 29.534 \\
d_1 &= 1.722 & d_2 &= 38.237 & d_3 &= -79.035
\end{align*}

por lo que el spline cúbico es:

$$
P(x) =
\begin{cases}
0.183x^3 - 1.679x^2 + 3.617x + 1.722 & \text{si } 3 \leq x \leq 4.5, \\
-0.214x^3 + 3.73x^2 - 20.726x + 38.237 & \text{si } 4.5 \leq x \leq 7, \\
0.128x^3 - 3.499x^2 + 29.53x - 79.035 & \text{si } 7 \leq x \leq 9.
\end{cases}
$$

$\blacksquare$

:::

#### Presentación algorítmica
---
title: "Apuntes Teoría MA0501"
author: 
  - name: "Diego Alberto Vega Víquez"
    email: "diegovv13@gmail.com"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 3
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    pdf-engine: xelatex
    include-in-header:
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    execute:
      code-fold: true
      message: false
      warning: false
    theme: flatly 
    toc: true
    toc-depth: 3
    toc-location: left
    html-math-method: katex
    css: styles.css
    embed-resources: true
---
\newpage
# Fundamentos de la Programación en Lenguaje R

## Funciones

A continuación un ejemplo de una función en **R**

```{r}
#| code-fold: true
area.triangulo <- function(base,altura) {   # <1>
  area <- (base*altura)/2
  return(area)
}

area.triangulo(2,5)
```
1. Comunmente usamos el punto para separar las palabras en el nombre de las variables

### Usando For, If y While

```{r}
#| code-fold: true
encuentra.cero.f <- function(v) {
  s <- -1
  for(i in 1:length(v)) {
    if((v[i]==0) && (s == -1)) {
      s<-i
    }
  }
  if (s != -1) {
    return(s)
  }
  else {
    return("¡El vector no tiene ningún cero!")
  }
}

vec<-c(4,-7,2,1,9)
encuentra.cero.f(vec)
```

La siguiente función calcula la suma del valor absoluto de las entradas de un vector, es de decir, la norma 1 del vector dada por

$$|v[1]|+|v[2]|+\ldots+|v[n]|$$

```{r}
#| code-fold: true
norma1 <- function(v) {
   suma <- 0
   i = 1
   while (i <= length(v)) {
      suma <- suma + abs(v[i])
      i <- i + 1
   }
   return(suma)
}

vec0<-c(4,-7,2,1)
norma1(vec0)
```


# Algoritmos, aproximaciones y error

## Aproximaciones y Error

### Aritmética punto flotante


#### Punto flotante

::: {.callout-note}
##### Definición: Forma Punto Flotante

Un número real $x$ está en **forma punto flotante** si se escribe de la forma

$$
0.d_1 d_2 \cdots d_k \times 10^{n},
$$

donde $0 \le d_i \le 9$, $d_1 \ne 0$, $i = 1,2,\ldots,k$.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

$$
\pi = 0.31415 \times 10^{1}.
$$
:::

::: {.callout-important collapse="true"}
###### Observación

- En análisis numérico todas las respuestas deben darse en **notación de punto flotante**.
- Si $x \in \mathbb{R}$, entonces se escribe como  
  $$x = 0.d_1 d_2 d_3 \cdots \times 10^{n},$$
  pero el computador solo puede **almacenar una cantidad finita de dígitos** por limitaciones de memoria.

Por lo tanto, quedan **dos posibilidades**:

1. **Cortar**
   - $$x \approx {fl}(x) = 0.d_1 d_2 \cdots d_k \times 10^{n},$$
     es decir, **cortar a partir del dígito $k+1$**.

2. **Redondear y cortar**
   - Si $d_{k+1} \ge 5$ entonces **sume 1 a $d_k$** y corte.
   - Si no, **solamente corte**.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

$$
\pi = 3.14159265\cdots, \quad \text{con } k = 5 \text{ tenemos:}
$$

$$
fl(\pi) = 0.31415 \times 10^{1} \quad (\textbf{Cortando})
$$

$$
fl(\pi) = 0.31416 \times 10^{1} \quad (\textbf{Redondeando})
$$
:::

::: {.callout-note}
##### Definición: Error de Redondeo

El error que resulta de reemplazar $x$ por $fl(x)$ se denomina **Error de Redondeo.**
:::

#### ¿Cómo medir, controlar, la propagación del error?

::: {.callout-note}
##### Definición: Error absoluto & Error relativo

Si $P^*$ es una aproximación de $P$ se llama:

$$
\text{Error absoluto} \;=\; |P - P^*|
$$

$$
\text{Error relativo} \;=\; \frac{|P - P^*|}{|P|}, \quad \text{para } P \neq 0
$$
:::

::: {.callout-note}
##### Teorema

Sea $x \in \mathbb{R}$, $x \neq 0$, entonces:

1. Si $fl(x)$ se obtiene usando $k$-dígitos de $x$ **cortando**, entonces:

$$
\left| \frac{x - fl(x)}{x} \right| \leq 10^{-k+1}.
$$

2. Si $fl(x)$ se obtiene usando $k$-dígitos de $x$ **redondeado**, entonces:

$$
\left| \frac{x - fl(x)}{x} \right| \leq 0.5 \times 10^{-k+1} \;=\; 5 \times 10^{-k}.
$$
:::
::: {.callout-tip collapse="true"}
###### Ejemplo

Si $P = 0.3000 \times 10^{1}$ y $P^* = 0.3100 \times 10^{1}$ entonces:

$$
|P - P^*| = 0.1 \times 10^{0}, 
\qquad \frac{|P - P^*|}{|P|} = 0.33 \times 10^{-1}
$$

Pero si $P = 0.3000 \times 10^{4}$ y $P^* = 0.3100 \times 10^{4}$ entonces:

$$
|P - P^*| = 0.1 \times 10^{3}, 
\qquad \frac{|P - P^*|}{|P|} = 0.33 \times 10^{-1}
$$
:::

::: {.callout-caution collapse="true"}
###### Prueba

**1.**

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_1 d_2 \cdots \times 10^n - 0.d_1 d_2 \cdots d_k \times 10^n}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots \times 10^{n-k}}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
&\le \frac{1}{0.1} \times 10^{-k} \\[1ex]
&= 10^{-k+1}.
\end{align*}

---

**2.** 


Ya que ${fl}(x)$ es una aproximación de $x$ con redondeo a $k$ dígitos eso significa que podemos escribir ${fl}(x)$ de la siguiente forma

$${fl}(x) = 0.d_1 d_2 \cdots d_k \times 10^{n}$$

Sea $x\in\mathbb{R}$ que escribiremos como 

$$x=0.d_1 d_2 \cdots \times 10^{n}$$

De esta forma

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_1 d_2 \cdots \times 10^n - 0.d_1 d_2 \cdots d_k \times 10^n}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots \times 10^{n-k}}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
\end{align*}

Aquí hay que analizar por casos:

- Suponga que $d_{k+1} < 5$

En este caso basta con cortar en $d_k$ así:

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
&\leq 0.5\cdot\left| \frac{1}{0.1} \right| \times 10^{-k} \\[1ex]
&= 0.5\times 10^{-k+1}
\end{align*}

- Suponga que $d_{k+1} \ge 5$.

Recuerde que para este caso en ${fl}(x)$ pasa que $d_k$ es una unidad mayor que el $d_k$ de $x$. De esta forma se va a cumplir que $d_{j_\text{real}}$$=d_{j_\text{aproximado}}$ para $j=\{1,2,\ldots,k-1\}$ así se tiene que 

$$
|x - fl(x)| = 10^{1-k}\cdot(1-0.d_{k+1}\cdots)
$$
$$
\left| \frac{x - fl(x)}{x} \right| = \frac{10^{1-k}\cdot(1-0.d_{k+1}\cdots)}{|0.d_1 d_2 \cdots\times 10^{n}|}
$$
Vea que 
\begin{align*}
d_{k+1} \ge 5 &\implies 0.d_{k+1}\cdots\ge\frac{1}{2}\\
&\implies 1-0.d_{k+1}\cdots\le\frac{1}{2}\\
&\implies 1-0.d_{k+1}\cdots\le\frac{1}{2}
\end{align*}
Así
$$
\left| \frac{x - fl(x)}{x} \right| \le \frac{10^{1-k} \cdot 0.5}{|0.d_1 d_2 \cdots\times 10^{n}|} \le 10^{1-k} \cdot 0.5
$$
Luego, concluya que 
$$
\left| \frac{x - {fl}(x)}{x} \right| \leq 0.5 \times 10^{-k+1} \qquad \blacksquare
$$
:::

::: {.callout-note}
##### Definición: Dígitos significativos

Se dice que un número $P^*$ aproxima a $P$ con $t$ **dígitos significativos** si $t \in \mathbb{N}$ es el número más grande tal que:

$$
\frac{|P - P^*|}{|P|} < 5 \times 10^{-t} \;=\; 0.5 \times 10^{-t+1}.
$$
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

$$
x \;=\; \pi \;=\; 0.\underbrace{3141}_{\text{4 dígitos}}59265 \cdots \times 10^{1}.
$$

$$
x^* \;=\; \frac{22}{7} \;=\; 0.\underbrace{3142}_{\text{dígitos distintos}}8517 \times 10^{1}.
$$

$$
\frac{|x - x^*|}{|x|} = 0.402 \times 10^{-3} < 0.5 \times 10^{-4+1}.
$$

Por lo que $x^*$ aproxima $x$ con **4 dígitos significativos**.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

¿Qué valores puede tomar $x^*$ para aproximar 1000 con **4 dígitos significativos**?  

$$
x = 1000 = 0.1000 \times 10^{4}, \quad \text{luego tenemos que:}
$$

$$
\left| \frac{x^* - 1000}{1000} \right|
< 5 \times 10^{-4}
\;\;\;\;\;\;\;\; \Rightarrow \;\;\;\;\;\;\;\;
-5 \times 10^{-4}
< \frac{x^* - 1000}{1000}
< 5 \times 10^{-4},
$$

$$
\Rightarrow \;\; 999.5 < x^* < 1000.5.
$$

Note que 
$$x^* = 0.9996 \times 10^{3}$$ 
aproxima a $x$ con **4 dígitos significativos**,  

pero 
$$y = 0.1001 \times 10^{4}$$ 
**no** aproxima a $x$ con 4 dígitos significativos.
:::

### Problemas con la aritmética punto flotante


- **División:**  
  Si $x \cong x + \varepsilon$ y se divide entre $\delta$ muy pequeño se tiene:

  $$
  \frac{x}{\delta} \cong \frac{x + \varepsilon}{\delta} = \frac{x}{\delta} + \underbrace{\frac{\varepsilon}{\delta}}_{(*)}
  $$
$(*)$: Nuevo error enorme


- **Resta de dos números casi iguales:**

\begin{align*}
  fl(x) \; &= \; 0.d_1 d_2 \cdots d_p \alpha_{p+1} \alpha_{p+2} \cdots \alpha_k \times 10^n\\
  fl(y) \; &= \; 0.d_1 d_2 \cdots d_p \beta_{p+1} \beta_{p+2} \cdots \beta_k \times 10^n\\
  x - y \; &\cong \; fl(x) - fl(y) \;=\; 0.\underbrace{\gamma_{p+1}\gamma_{p+2}\cdots\gamma_k}_{(**)} \times 10^{n-p}
\end{align*}

$(**)$: Podría ser basura.

::: {.callout-note}
###### Notación

\begin{align*}
\Delta_x \;&=\; |x - x^*|.\\
\delta_x \;&=\; \frac{|x - x^*|}{|x|} \;=\; \frac{\Delta_x}{|x|}.
\end{align*}
:::

---

::: {.callout-note}
##### Teorema

Si $x = x_1 + x_2 + \cdots + x_n$ y $x^* = x_1^* + x_2^* + \cdots + x_n^*$ con $x_i \geq 0$ entonces:

\begin{align*}
\Delta_x \;&\leq\; \sum_{i=1}^n \Delta_{x_i}. \\
\delta_x \;&\leq\; \max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\}.
\end{align*}
:::

::: {.callout-caution collapse="true"}
###### Prueba


\begin{align*}
\Delta_x &= |x - x^*|
= \left| \sum_{i=1}^n x_i - \sum_{i=1}^n x_i^* \right|
\leq \sum_{i=1}^n |x_i - x_i^*|
= \sum_{i=1}^n \Delta_{x_i}.
\end{align*}


\begin{align*}
\text{Sabemos que } \quad
\delta_x &= \frac{\Delta_x}{|x|}
\leq \frac{\Delta_{x_1} + \Delta_{x_2} + \cdots + \Delta_{x_n}}
{|x_1 + x_2 + \cdots + x_n|},\\[1ex]
\delta_{x_i} &= \frac{\Delta_{x_i}}{|x_i|}
\;\Rightarrow\; \Delta_{x_i} = \delta_{x_i}\,|x_i|,\\[2ex]
\Rightarrow \quad
\delta_x &\leq \frac{|x_1|\delta_{x_1} + |x_2|\delta_{x_2} + \cdots + |x_n|\delta_{x_n}}
{|x_1 + x_2 + \cdots + x_n|}\\[2ex]
&\leq
\frac{\max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\}\, (|x_1| + |x_2| + \cdots + |x_n|)}
{|x_1 + x_2 + \cdots + x_n|}\\[2ex]
&\leq \max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\},
\qquad \text{pues } x_i \ge 0 \ \forall i .
\end{align*}

:::

::: {.callout-important collapse="true"}
##### Observación

Se debe evitar la pérdida de dígitos significativos:

Por ejemplo, al evaluar:  

$$
f(x) = 1 - \cos(x),
$$  

con $x$ cercano a $0$ se producirá una pérdida de dígitos significativos.  
Esto se puede evitar racionalizando, como sigue:

$$
f(x) = 1 - \cos(x) \;=\; \frac{\sin^{2}(x)}{1 + \cos(x)}.
$$

:::

### Algoritmos y convergencia

#### ¿Qué es un algoritmo?

::: {.callout-note}
##### Definición: Algoritmo

Un algoritmo es un procedimiento que describe, sin ninguna ambigüedad, una sucesión finita de pasos a realizar en orden específico, con el propósito de resolver un problema.

:::

**Características:**

- Finito.  
- Definido (no ambiguo).  
- Entrada.  
- Salida.  
- Efectivo.  
- Eficiente.  

Para representar las instrucciones utilizaremos pseudocódigo.

::: {.callout-tip collapse="true"}
###### Ejemplo

Para calcular  
$$
\sum_{k=a}^{\infty} f(x,k)
$$
tenemos:

**Entrada:** $\varepsilon, f, a, x$.  

**Salida:** Valor aproximado de  
$$
\sum_{k=a}^{\infty} f(x,k).
$$

---

```{python}
#| eval: false
#| message: false
k <- a
s <- 0
T <- f(x,k)
while |T| < ε do
    s <- s + t
    t <- t * (f(x,k+1) / f(x,k))
    k <- k + 1
end while
return s
```
:::

::: {.callout-note}
##### Definición: Algoritmos Estables

Un algoritmo se dice **estable** si pequeños cambios en la entrada producen pequeños cambios en la salida.  
En caso contrario, es decir, pequeños cambios en la entrada producen grandes cambios en la salida, entonces el algoritmo se dice **inestable (caótico)**.
:::

::: {.callout-note}
##### Notación  

\begin{align*}
E \;&=\; \text{Error inicial.}\\
E_n \;&=\; \text{Error luego de }n\text{ pasos.}
\end{align*}
:::

::: {.callout-note}
##### Definición: Crecimiento lineal del error  

Si  
$$|E_n| = CnE,$$  
con $C$ constante, entonces el crecimiento del error es **lineal**.  
:::

::: {.callout-note}
##### Definición: Crecimiento exponencial del error  

Si  
$$|E_n| = K^n E, \quad K > 1,$$  
entonces el crecimiento del error es **exponencial**.  
:::

::: {.callout-important collapse="true"}
###### Observación 

- Crecimiento del error lineal $\;\Leftrightarrow\;$ estable.  
- Crecimiento del error exponencial $\;\Leftrightarrow\;$ inestable.  
:::

::: {.callout-note}
##### Definición: Rapidez de convergencia  

Sea $\{ \alpha_n \}_{n \in \mathbb{N}}$ una sucesión que converge a $\alpha$, se dice que $\{ \alpha_n \}_{n \in \mathbb{N}}$ converge con una rapidez $\mathcal{O}(\beta_n)$, donde $\{ \beta_n \}_{n \in \mathbb{N}}$ es otra sucesión ($\beta_n \neq 0 \ \forall n$) si:  

$$
\frac{|\alpha_n - \alpha|}{|\beta_n|} < K
$$  

para $n$ suficientemente grande y $K$ constante que no depende de $n$.  
:::

::: {.callout-note}
##### Notación  

$$
\alpha_n = \alpha + \mathcal{O}(\beta_n).
$$  

$$
\alpha_n \;\to\; \alpha \;\;\; \text{con rapidez } \mathcal{O}(\beta_n).
$$  
:::

::: {.callout-note}
##### Observación  

$$
\frac{|\alpha_n - \alpha|}{|\beta_n|} < K 
\;\;\;\Longleftrightarrow\;\;\;
- K \beta_n + \alpha < \alpha_n < K \beta_n + \alpha.
$$
:::

::: {.callout-tip collapse="true"}
##### Ejemplo  

Si $\alpha_n = \dfrac{n+3}{n^3}$, entonces  

$$
\alpha_n = 0 + \mathcal{O}\!\left(\frac{1}{n^2}\right)
$$  

pues:  

$$
\left| \frac{\alpha_n - \alpha}{\beta_n} \right|
= \left| \frac{\tfrac{n+3}{n^3} - 0}{\tfrac{1}{n^2}} \right|
= \left| \frac{n^3 + 3n^2}{n^3} \right|
= \left| 1 + \frac{3}{n} \right|
\leq 4, \quad \text{si } n \to \infty.
$$  

Es decir, $\dfrac{n+3}{n^3}$ converge a $0$ tan rápido como $\dfrac{1}{n^2}$ converge a $0$.  
:::

::: {.callout-tip collapse="true"}
##### Ejemplo  

Si 
$$
\alpha_n = \frac{\sin(n)}{n},
$$  
entonces  
$$
\alpha_n = 0 + \mathcal{O}\!\left(\frac{1}{n}\right), \quad \text{cuando } n \to \infty.
$$
:::

::: {.callout-note}
##### Observación: Velocidad de convergencia de los ciclos  

1\. $\texttt{For[i=1, i++, i<=N, \ldots]}$

tiene una velocidad de convergencia $\mathcal{O}(N)$.  

2\. $\texttt{For[i=1, i++, i<=N,} \quad$

  $\quad\texttt{For[j=1, j++, j<=M, \ldots]]}$
   
tiene una velocidad de convergencia $\mathcal{O}(N^2)$.  

3\. $\texttt{For[i=1, i++, i<=N,} \quad$

  $\qquad\texttt{For[j=1, j++, j<=M,} \quad$
   
  $\qquad\qquad\texttt{For[k=1, k++, k<=L, \ldots]]]}$
      
tiene una velocidad de convergencia $\mathcal{O}(N^3)$.  
:::

### Origen del Error

#### Tipos de Error

Existen dos tipos de error: El error en los datos y el error computacional

![Tipos de Error](Imagenes/Grafico Error.png){width=400px fig-align="center"}

::: {.callout-note}
##### Definición: Propagación del error

Sea $a \in \mathbb{R}$ y sea $\hat{a}$ una aproximación de $a$ y $f$ una función o procedimiento,  
entonces 

$$
f(\hat{a}) - f(a)
$$  

se llama **propagación del error** o **error propagado**.
:::
Esto se ilustra en el siguiente gráfico

AGRAGAR GRAFICO ANDREY

::: {.callout-tip collapse="true"}
###### Ejemplo

Suponga que se desea calcular $c$ en el siguiente triángulo:

![](Imagenes/Triangulo.png){width=300px fig-align="center"}

Supongamos que $a$ tiene un error y usamos $100.1$.  
Entonces el error se propaga como se muestra en la siguiente tabla:

| Expresión | Exacto | Aproximado | Error relativo |
|-----------|--------|-------------|----------------|
| $a$ | $100$ | $100.1$ | $0.1 \,\%$ |
| $b-a$ | $1$ | $0.9$ | $-10 \,\%$ |
| $(b-a)^2$ | $1$ | $0.81$ | $-19 \,\%$ |
| $4ab \sin^2\!\left(\tfrac{\gamma}{2}\right)$ | $3.0765\ldots$ | $3.0796\ldots$ | $0.1 \,\%$ |
| $(b-a)^2 + 4ab \sin^2\!\left(\tfrac{\gamma}{2}\right)$ | $4.0765\ldots$ | $3.8896\ldots$ | $-4.6 \,\%$ |
| $c$ | $2.0190\ldots$ | $1.9722\ldots$ | $-2.3 \,\%$ |
| **Error relativo inicial:** | $0.1 \,\%$  | **Error relativo final:** | $-2.3 \,\%$ |

: {tbl-colwidths="[35,20,25,20]"}

:::

::: {.callout-tip collapse="true"}
###### Ejemplo: Inestabilidad Numérica

Suponga que se desea calcular  

$$
z = 1.000 - \frac{1.208}{x}
$$  

en una computadora que solamente utiliza 4 dígitos, con $x = 1.209$.

**Algoritmo 1.**  
Calcule primero $y := \dfrac{1.208}{x}$ y luego $z := 1.000 - y$  

![](Imagenes/1 graph.png){width=350px fig-align="center"}

**Algoritmo 2.**  
Calcule primero $y := x - 1.208$ y luego $z := \dfrac{y}{x}$  

![](Imagenes/2 graph.png){width=350px fig-align="center"}
:::

#### Principio Básico:

Si es posible, evite operaciones sensibles con operandos contaminados por la propagación del error.

::: {.callout-tip collapse="true"}
###### Ejemplo: Raíces cuadráticas y propagación del error

Usualmente, para calcular las raíces de una ecuación cuadrática se usa:

$$
x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},
$$

pero si $b > 0$ es mejor usar:

$$
x_1 = \frac{-2c}{-b - \sqrt{b^2 - 4ac}},
$$

y si $b < 0$ es mejor usar:

$$
x_2 = \frac{2c}{-b + \sqrt{b^2 - 4ac}}.
$$
:::

::: {.callout-tip collapse="true"}
###### Ejemplos: Evitar cancelación numérica

Otros ejemplos que evitan problemas de cancelación cuando $x \approx y$ son:

\begin{align*}
x^2 - y^2 \;\;&\longrightarrow\;\; (x-y)(x+y),\\
\cos(x) - 1 \;\;&\longrightarrow\;\; 2 \sin^2\!\left(\tfrac{x}{2}\right), \quad \text{para } x \to 0,\\
\ln(x) - \ln(y) = \ln\!\left(\tfrac{x}{y}\right)&\longrightarrow\; 2 \tanh^{-1}\!\left(\tfrac{x-y}{x+y}\right),\\
e^x - e^y \;\;&\longrightarrow\;\; 2 \sinh\!\left(\tfrac{x-y}{2}\right) e^{\tfrac{x+y}{2}}.
\end{align*}
:::  

# Elementos de análisis funcional

## Elementos de Análisis Funcional para Análisis Numérico

### Espacios Normados  

::: {.callout-note}
##### Definición: Espacio normado

Sea $X$ un espacio vectorial complejo (o real).  
Una función $\|\cdot\| : X \to \mathbb{R}$ con las siguientes propiedades:

1. $\|x\| \geq 0,$  
2. $\|x\| = 0$ si y solamente si $x = 0,$  
3. $\|\alpha x\| = |\alpha| \, \|x\|,$  
4. $\|x + y\| \leq \|x\| + \|y\|,$  

para todo $x, y \in X$ y para todo $\alpha \in \mathbb{C}$ (o $\mathbb{R}$),  
se llama **norma** en $X$.  

El espacio vectorial $X$ provisto de una norma se llama **espacio normado**.
:::

::: {.callout-tip collapse="true" collapsed="true"}
###### Ejemplo

Algunas normas de $\mathbb{R}^n$ y $\mathbb{C}^n$ son:

$$
\|x\|_1 := \sum_{i=1}^n |x_i|, 
\qquad
\|x\|_2 := \left( \sum_{i=1}^n |x_i|^2 \right)^{1/2}, 
\qquad
\|x\|_\infty := \max_{i=1,2,\ldots,n} |x_i|,
$$

para $x = (x_1, x_2, \ldots, x_n)^t$.

Las normas anteriores son conocidas como las normas $\ell_1$, $\ell_2$ y $\ell_\infty$ respectivamente.  
Las tres son casos particulares de la **norma $\ell_p$**:

$$
\|x\|_p := \left( \sum_{i=1}^n |x_i|^p \right)^{1/p},
\qquad p \geq 1.
$$

La norma $\ell_\infty$ es el límite de $\ell_p$ cuando $p \to \infty$.
:::

::: {.callout-note}
##### Proposición

Para toda norma se tiene la **segunda desigualdad triangular**:

$$
\big| \|x\| - \|y\| \big| \leq \|x - y\|, 
$$

para todo $x, y \in X$.
:::

::: {.callout-caution collapse="true" collapsed="true"}
###### Prueba

De la desigualdad triangular se tiene que

$$
\|x\| = \|x - y + y\| \leq \|x - y\| + \|y\|,
$$

por lo tanto

$$
\|x\| - \|y\| \leq \|x - y\|.
$$

Análogamente, cambiando el rol de $x$ y $y$, se obtiene

$$
\|y\| - \|x\| \leq \|y - x\|,
$$

y por lo tanto se cumple la desigualdad.
:::

::: {.callout-note}
##### Definición: Norma

Para dos elementos $x,y$ en un espacio normado $X$ la norma $\|x-y\|$ se llama **distancia** entre $x$ y $y$.
:::

::: {.callout-note}
##### Definición: Sucesión Convergente

Una sucesión $(x_n)$ de elementos en un espacio normado $X$ se llama **convergente** si existe un elemento $x \in X$ tal que:

$$
\lim_{n \to \infty} \|x_n - x\| = 0,
$$

es decir, para todo $\varepsilon > 0$ existe un entero $N(\varepsilon)$ tal que $\|x_n - x\| < \varepsilon$ para todo $n > N(\varepsilon)$.  

El elemento $x$ es llamado el **límite** de la sucesión $(x_n)$ y se escribe:

$$
\lim_{n \to \infty} x_n = x.
$$

Si una sucesión no converge se llama **divergente**.
:::

::: {.callout-note}
##### Proposición
El límite de una sucesión convergente es **único**.
:::

::: {.callout-caution collapse="true"}
###### Prueba
*(Ejercicio)* $\blacksquare$
:::

::: {.callout-note}
##### Definición: Normas Equivalentes
Dos normas en un espacio vectorial se llaman **equivalentes** si tienen el mismo conjunto de sucesiones convergentes.
:::

::: {.callout-note}
##### Teorema
Dos normas $\|\cdot\|_{a}$ y $\|\cdot\|_{b}$ en un espacio vectorial $X$ son equivalentes si y solamente si existen números positivos $c$ y $C$ tal que:
$$
c \|x\|_{a} \;\leq\; \|x\|_{b} \;\leq\; C \|x\|_{a}, \quad \forall x \in X.
$$
:::

::: {.callout-caution collapse="true"}
###### Prueba
Sean las normas $\|\cdot\|_{a}$ y $\|\cdot\|_{b}$ equivalentes. Suponga que no existe $C>0$ tal que 
$\|x\|_{b} \leq C \|x\|_{a}$ para todo $x \in X$, entonces existe una sucesión $(x_{n})$ con $\|x_{n}\|_{a} = 1$ y $\|x_{n}\|_{b} \geq n^{2}$.  
Luego la sucesión $y_{n} := x_{n}/n$ converge a cero con respecto a $\|\cdot\|_{a}$ pero no con respecto a $\|\cdot\|_{b}$, porque $\|y_{n}\|_{b} \geq n$.  

Recíprocamente, si se tiene (2), entonces si $\|x_{n}-x\|_{a}\to 0$ es claro que $\|x_{n}-x\|_{b}\to 0$ y viceversa.  
$\blacksquare$
:::

::: {.callout-note}
##### Teorema
En un espacio vectorial de dimensión finita todas las normas son equivalentes.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Ejercicio. $\blacksquare$
:::

::: {.callout-note}
##### Definición: Subconjunto cerrado
- Un subconjunto $U$ de un espacio normado se llama **cerrado** si este contiene el límite de todas las sucesiones convergentes de $U$.  

- La **clausura** $\overline{U}$ de un conjunto $U$ de un espacio normado $X$ es el conjunto de todos los límites de sucesiones convergentes de $U$.  

- Un subconjunto $U$ de $X$ se llama **abierto** si su complemento $X \setminus U$ es cerrado.  

- Un conjunto $U$ se llama **denso** en otro conjunto $V$ si $V \subset \overline{U}$, es decir, si cada elemento de $V$ es el límite de una sucesión convergente de $U$.  

- Para cada $x_{0} \in X$ y $r > 0$ el conjunto  
  $$
  B[x_{0}, r] := \{x \in X \;:\; \|x - x_{0}\| \leq r\}
  $$  
  es cerrado y se llama la **bola cerrada** de radio $r$ y centro $x_{0}$.  

  El conjunto  
  $$
  B(x_{0}, r) := \{x \in X \;:\; \|x - x_{0}\| < r\}
  $$  
  es abierto y se llama la **bola abierta** de radio $r$ y centro $x_{0}$.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

- **Cerrado.** $[0,1] \subset \mathbb{R}$ es cerrado: toda sucesión convergente de puntos en $[0,1]$ converge a un punto en $[0,1]$.

- **Clausura.** Si $U = ]0,1[$, entonces $\overline{U} = [0,1]$.  
  Si $U = \mathbb{Q}$, entonces $\overline{U} = \mathbb{R}$.

- **Abierto.** $]0,1[ \subset \mathbb{R}$ es abierto porque su complemento es  
  $\mathbb{R} \setminus ]0,1[ = ]-\infty,0] \cup [1,\infty[$.

- **Denso en $V$.** $\mathbb{Q}$ es denso en $\mathbb{R}$ ($\overline{\mathbb{Q}} = \mathbb{R}$).  
  También $]0,1[$ es denso en $[0,1]$ pues $\overline{]0,1[} = [0,1]$.

- **Bolas (norma usual).**  
  En $\mathbb{R}$, con $x_0 = 2$, $r = 3$:  
  $$
  B[2,3] = \{ x \in \mathbb{R} : |x-2| \leq 3 \} = [-1,5],
  $$
  $$
  B(2,3) = \{ x \in \mathbb{R} : |x-2| < 3 \} = ]-1,5[.
  $$

  En $\mathbb{R}^2$, con $x_0 = (1,2)$, $r = 2$:  
  $$
  B[(1,2),2] = \{ (x,y) \in \mathbb{R}^2 : \sqrt{(x-1)^2+(y-2)^2} \leq 2 \},
  $$
  $$
  B((1,2),2] = \{ (x,y) \in \mathbb{R}^2 : \sqrt{(x-1)^2+(y-2)^2} < 2 \}.
  $$
:::

::: {.callout-note}
##### Definición
Un conjunto $U$ se llama **acotado** si existe un número positivo $C$ tal que  
$$
\|x\| < C \quad \text{para todo } x \in U.
$$
:::

::: {.callout-note}
##### Teorema
Toda sucesión acotada en un espacio normado de dimensión finita $X$ contiene una subsucesión convergente.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Sea $u_{1}, u_{2}, \ldots, u_{n}$ una base de $X$ y sea $(x_{\nu})$ una sucesión acotada. Entonces se puede escribir:
$$
x_{\nu} = \sum_{j=1}^{n} \alpha_{j\nu} u_{j}.
$$

Como $x_{\nu}$ es una sucesión acotada y usando la norma
$$
\|x_{\nu}\|_{\infty B} := \max_{j=1,2,\ldots,n} |\alpha_{j\nu}|,
$$
se tiene que cada una de las sucesiones $(\alpha_{j\nu})$ es acotada en $\mathbb{C}$ para cada $j=1,2,\ldots,n$.  

Por lo tanto, usando el teorema de Bolzano–Weierstrass se puede seleccionar una subsucesión $\alpha_{j\nu(\ell)} \to \alpha_{j}$ cuando $\ell \to \infty$ para cada $j=1,2,\ldots,n$.  

Esto implica que:
$$
x_{\nu(\ell)} \to \sum_{j=1}^{n} \alpha_{j} u_{j} \in X, \quad \text{cuando } \ell \to \infty.
$$
$\blacksquare$
:::

### Productos escalares  

::: {.callout-note}
##### Definición: Producto interno y espacio pre-Hilbert

Sea $X$ un espacio vectorial complejo (o real).  
Una función $\langle \cdot,\cdot \rangle : X \times X \to \mathbb{C}$ (o $\mathbb{R}$) con las siguientes propiedades:

1. $\langle x, x \rangle \ge 0,$
2. $\langle x, x \rangle = 0 \quad \text{si y sólo si } x = 0,$
3. $\langle x, y \rangle = \overline{\langle y, x \rangle},$
4. $\langle \alpha x + \beta y, z \rangle = \alpha \langle x, z \rangle + \beta \langle y, z \rangle, \quad \forall\, x,y,z \in X,\; \alpha,\beta \in \mathbb{C}\; (\text{o } \mathbb{R}),$

se llama **producto interno** en $X$.  
Un espacio vectorial $X$ provisto de un producto interno se llama **espacio pre-Hilbert**.
:::

::: {.callout-important collapse="true"}
###### Observación
Una consecuencia inmediata de 3. y 4. es la **antilinealidad**:
$$
\langle x, \alpha y + \beta z \rangle = \overline{\alpha} \langle x, y \rangle + \overline{\beta} \langle x, z \rangle.
$$
:::

::: {.callout-tip collapse="true"}
###### Ejemplo
Un ejemplo de producto interno en $\mathbb{C}^n$ (o $\mathbb{R}^n$) está dado por:
$$
\langle x, y \rangle := \sum_{i=1}^{n} x_i \overline{y_i},
$$
donde $x := (x_1, x_2, \ldots, x_n)^t$ y $y := (y_1, y_2, \ldots, y_n)^t$.
:::

::: {.callout-note}
##### Teorema
Para todo producto interno se tiene la desigualdad de Cauchy–Schwarz:
$$
|\langle x, y \rangle|^2 \leq \langle x, x \rangle \langle y, y \rangle,
$$
para todo $x, y \in X$. Además se tiene igualdad si para todo $x, y$ son linealmente dependientes.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Si $x = 0$ la desigualdad es trivial.  
Si $x \neq 0$, tome
$$
z = y - \frac{\langle y, x \rangle}{\|x\|^2}x,
$$
luego es claro que $\langle z, x \rangle = 0$ y que:
$$
0 \leq \|z\|^2
= \left\langle y - \frac{\langle y, x \rangle}{\|x\|^2}x , \; y - \frac{\langle y, x \rangle}{\|x\|^2}x \right\rangle
= \langle y, y \rangle - \frac{\langle y, x \rangle \langle x, y \rangle}{\|x\|^2}
= \|y\|^2 - \frac{|\langle x, y \rangle|^2}{\|x\|^2},
$$
de donde se tiene la desigualdad. Además se tiene igualdad si para todo $x, y$ son linealmente dependientes (ejercicio).
$\blacksquare$
:::

::: {.callout-note}
##### Teorema
Sea $X$ un espacio vectorial complejo (o real).  
Entonces la función:
$$
\|x\| := \langle x, x \rangle^{1/2}
$$
define una norma en $X$, es decir, un espacio pre–Hilbert es siempre un espacio normado.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Ejercicio (use la desigualdad de Cauchy–Schwarz para probar la desigualdad triangular).  
$\blacksquare$
:::

::: {.callout-note}
##### Definición: Elementos Ortogonales
- Dos elementos $x$ y $y$ de un espacio pre–Hilbert se llaman **ortogonales** si:
$$
\langle x, y \rangle = 0.
$$

- Dos subconjuntos $U$ y $V$ se llaman **ortogonales** si $\langle u, v \rangle = 0$ para todo $u \in U$ y $v \in V$.  

- Si dos elementos son ortogonales se denota $x \perp y$ y si dos conjuntos son ortogonales se denota $U \perp V$.  

- Un subconjunto $U \subseteq X$ se llama un **sistema ortogonal** si $\langle x, y \rangle = 0$ para todo $x, y \in U$ con $x \neq y$.  

- Un sistema ortogonal $U$ de $X$ se llama **ortonormal** si $\|x\| = 1$ para todo $x \in U$.
:::

::: {.callout-note}
##### Teorema
Los elementos de un sistema ortogonal son linealmente independientes.
:::

::: {.callout-caution collapse="true"}
###### Prueba

Sea $\{q_{1},q_{2},\ldots,q_{n}\}$ un sistema ortogonal.  
Si 
$$
\sum_{k=1}^{n} \alpha_{k} q_{k} = 0,
$$
y se multiplica a ambos lados por $q_{j}$, es inmediato que $\alpha_{j} = 0$ para todo $j=1,2,\ldots,n$.  
$\blacksquare$
:::

::: {.callout-note}
##### Teorema  [Gram–Schmidt]

Sea $\{u_{0},u_{1},\ldots\}$ un conjunto finito o numerable de elementos linealmente independientes de un espacio de pre–Hilbert.  
Entonces existe un sistema ortogonal único $\{q_{0},q_{1},\ldots\}$ de la forma:

$$
q_{n} = u_{n} + r_{n}, \quad \text{para } n = 0,1,\ldots,
\tag{3}
$$

con $r_{0}=0$ y $r_{n} \in \operatorname{gen}\{u_{0},u_{1},\ldots,u_{n-1}\}$ para $n=1,2,\ldots$.  
Además se tiene que:

$$
\operatorname{gen}\{u_{0},u_{1},\ldots,u_{n}\} 
= 
\operatorname{gen}\{q_{0},q_{1},\ldots,q_{n}\},
\quad \text{para } n=0,1,\ldots
\tag{4}
$$
:::

::: {.callout-caution collapse="true"}
###### Prueba

Asumamos que hemos construido elementos de la forma (3) con la propiedad (4) hasta $q_{n-1}$.  
Por la propiedad (4) los elementos $\{q_{0},q_{1},\ldots,q_{n-1}\}$ son linealmente independientes y por lo tanto $\|q_{k}\|\neq 0$ para $k=0,1,\ldots,n-1$.  
Por lo tanto:

$$
q_{n} = u_{n} - \sum_{k=0}^{n-1}\frac{\langle u_{n},q_{k}\rangle}{\langle q_{k},q_{k}\rangle}q_{k},
$$

está bien definido, y usando la hipótesis de inducción es fácil notar que  

$$
\langle q_{n},q_{k}\rangle = 0, \quad \text{para } k=0,1,\ldots,n-1.
$$

Es claro que  

$$
r_{n}=\sum_{k=0}^{n-1}\frac{\langle u_{n},q_{k}\rangle}{\langle q_{k},q_{k}\rangle}q_{k}
\;\;\in\;\;
\operatorname{gen}\{q_{0},q_{1},\ldots,q_{n-1}\}
=
\operatorname{gen}\{u_{0},u_{1},\ldots,u_{n-1}\}.
$$

La unicidad queda de ejercicio al lector. $\blacksquare$
:::

### Completitud  


::: {.callout-note}
##### Definición: Sucesión de Cauchy

Una sucesión de elementos en un espacio normado $X$ se llama **sucesión de Cauchy** si para todo $\varepsilon > 0$ existe un $N(\varepsilon) \in \mathbb{N}$ tal que:

$$
\|x_n - x_m\| < \varepsilon,
$$

para todo $n, m \geq N(\varepsilon)$, es decir, si  

$$
\lim_{n \to \infty, \; m \to \infty} \|x_n - x_m\| = 0.
$$
:::

::: {.callout-note}
##### Teorema
Toda sucesión convergente es de Cauchy
:::

::: {.callout-caution collapse="true"}
###### Prueba

Sea $x_n \to x$ cuando $n \to \infty$. Entonces, para todo $\varepsilon > 0$ existe un $N(\varepsilon) \in \mathbb{N}$ tal que  

$$
\|x_n - x\| < \tfrac{\varepsilon}{2}, \quad \text{para todo } n \geq N(\varepsilon).
$$

Ahora, usando la desigualdad triangular:  

$$
\|x_n - x_m\| 
= \|x_n - x + x - x_m\|
\leq \|x_n - x\| + \|x_m - x\| < \varepsilon,
$$

para todo $n, m \geq N(\varepsilon)$. $\;\blacksquare$
:::

::: {.callout-important collapse="true"}
###### Observación
El recíproco del teorema anterior no es válido en general, por esto tiene sentido dar la siguiente definición.
:::

::: {.callout-note}
##### Definición: Subconjunto Completo
Un subconjunto $U$ de un espacio normado $X$ se llama **completo** si toda sucesión de Cauchy de elementos de $U$ converge a un elemento en $U$.  

Un espacio normado completo se llama **espacio de Banach**.  

Un espacio pre–Hilbert se llama **espacio de Hilbert** si este es un espacio completo.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo
El espacio vectorial $C[a,b]$ provisto con la norma:

$$
\|f\|_\infty := \max_{x \in [a,b]} |f(x)|
$$

es un espacio de Banach.

::: {.callout-caution collapse="true"}
###### Prueba
(Ejercicio) $\blacksquare$
:::

:::

::: {.callout-tip collapse="true"}
###### Ejemplo
El espacio vectorial $C[a,b]$ provisto con la norma $L_1$:

$$
\|f\|_1 := \int_a^b |f(x)| \, dx
$$

NO es un espacio de Banach.

::: {.callout-caution collapse="true"}
###### Prueba
Es evidente que $\|f\|_1$ es una norma.  
Sin pérdida de generalidad se toma $[a,b] = [0,2]$ y se escoge:

$$
f_n(x) := 
\begin{cases}
x^n & \text{si } 0 \leq x \leq 1, \\
1   & \text{si } 1 < x \leq 2.
\end{cases}
$$

Para todo $m > n$ se tiene que:

$$
\| f_n - f_m \|_1 
= \int_0^1 \big( x^n - x^m \big) dx 
= \frac{1}{n+1} + \frac{1}{m+1} \to 0
\quad \text{cuando } n,m \to \infty,
$$

por lo tanto $(f_n)$ es una sucesión de Cauchy. 

Ahora, supongamos que la sucesión $(f_n)$ converge a una función continua $f$ con respecto a la norma $L_1$, es decir:

$$
\| f_n - f \|_1 \to 0 \quad \text{cuando } n \to \infty.
$$

Entonces:

$$
\int_0^1 |f(x)| \, dx 
\leq \int_0^1 |f(x) - x^n| \, dx + \int_0^1 x^n \, dx 
\leq \| f - f_n \|_1 + \frac{1}{n+1} \to 0
$$

cuando $n \to \infty$, de donde $f(x) = 0$ para $0 \leq x \leq 1$.

Además se tiene que

$$
\int_1^2 |f(x)-1| \, dx 
= \int_1^2 |f(x)-f_n(x)| \, dx 
\leq \| f - f_n \|_1 \to 0 \quad \text{cuando } n \to \infty,
$$

esto implica que $f(x) = 1$ para $1 \leq x \leq 2$.  
Por lo tanto $f$ no es continua, lo cual es una contradicción. $\blacksquare$
:::

:::

::: {.callout-tip collapse="true"}
##### Ejemplo
El espacio vectorial $C[a,b]$ provisto con la norma $L_2$:

$$
\| f \|_2 := \left( \int_a^b |f(x)|^2 \, dx \right)^{2}
$$

NO es un espacio de Banach.

::: {.callout-caution collapse="true"}
###### Prueba
Hay que probar que el espacio vectorial $C[a,b]$ normado provisto con la norma $L_2$ **NO** es completo. Es decir,
hay que encontrar una sucesión de Cauchy de elementos de $U$ que **NO** converge a un elemento en $U$, siendo $U$ un subconjunto del espacio vectorial $C[a,b]$

Sin pérdida de generalidad se toma $[a,b] = [0,2]$ y se escoge:

$$
f_n(x) := 
\begin{cases}
x^n & \text{si } 0 \leq x \leq 1, \\
1   & \text{si } 1 < x \leq 2.
\end{cases}
$$

Para todo $m > n$ se tiene que:

\begin{align*}
\| f_n - f_m \|_1
&= \left( \int_0^2 |f_n(x) - f_m(x)|^2 \, dx \right)^2\\[1ex]
&= \left( \int_0^1 |x^n - x^m|^2 \, dx \right)^2\\[1ex]
&= \left( \int_0^1 \big(x^{2n} - 2x^{n+m} + x^{2m}\big) \, dx \right)^2\\[1ex]
&= \left( \frac{1}{2n+1} - \frac{2}{n+m+1} + \frac{1}{2m+1} \right)^2 \to 0
\quad \text{cuando } n,m \to \infty,
\end{align*}

por lo tanto $(f_n)$ es una sucesión de Cauchy. 

Ahora, supongamos que la sucesión $(f_n)$ converge a una función continua $f$ con respecto a la norma $L_1$, es decir:

$$
\| f_n - f \|_1 \to 0 \quad \text{cuando } n \to \infty.
$$
Entonces,

\begin{align*}
\|f\|_{1} 
  &= \|(f-x^{n})+x^{n}\|_{1}
\\
  &\le \|f-x^{n}\|_{1} + \|x^{n}\|_{1}
  \qquad\text{(desigualdad triangular)}
\\
  &= \|f-f_{n}\|_{1} + \left(\int_{0}^{1} |x^{n}|^{2}\,dx\right)^{2}
\\
  &= \|f-f_{n}\|_{1} + \left(\int_{0}^{1} x^{2n}\,dx\right)^{2}
\\
  &= \|f-f_{n}\|_{1} + \left(\frac{1}{2n+1}\right)^{2}
  \xrightarrow[n\to\infty]{} 0
\end{align*}
de donde $f(x) = 0$ para $0 \leq x \leq 1$.

Vea además que 

\begin{align*}
\Biggl(\int_{1}^{2}\!\bigl|f(x)-1\bigr|^{2}\,dx\Biggr)^{2} 
&= \Biggl(\int_{1}^{2}\!\bigl|f(x)-f_n(x)\bigr|^{2}\,dx\Biggr)^{2} 
\qquad\text{(pues $f_n(x)=1$ en $(1,2]$)}\\[4ex]
&\le \Biggl(\int_{0}^{2}\!\bigl|f(x)-f_n(x)\bigr|^{2}\,dx\Biggr)^{2} 
= \,\|f-f_n\|_{1}\;\xrightarrow[n\to\infty]{}\;0 .
\end{align*}

esto implica que $f(x) = 1$ para $1 \leq x \leq 2$.  
Por lo tanto $f$ no es continua, lo cual es una contradicción. $\qquad\blacksquare$
:::
:::

::: {.callout-note}
##### Teorema
Todo espacio normado de dimensión finita es un espacio de Banach.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Sea $X$ un espacio normado con base $u_1, u_2, \ldots, u_n$ y sea $(x_\nu)$ una sucesión de Cauchy en $X$.  
Se puede escribir:  

$$
x_\nu = \sum_{j=1}^n \alpha_{j\nu} u_j
$$

usando el Teorema 1 se tiene que existe un $C > 0$ tal que:  

$$
\max_{j=1,2,\ldots,n} |\alpha_{j\nu} - \alpha_{j\mu}| \leq C \, \| x_\nu - x_\mu \|
$$

para todo $\nu, \mu \in \mathbb{N}$.

Por lo tanto $(\alpha_{j\nu})$ es una sucesión de Cauchy en $\mathbb{C}$, entonces existen $\alpha_1, \alpha_2, \ldots, \alpha_n$ tales que $\alpha_{j\nu} \to \alpha_j$ cuando $\nu \to \infty$ para cada $j = 1,2,\ldots,n$.  
Por lo tanto $(x_\nu)$ converge a:  

$$
x_\nu \to x := \sum_{j=1}^n \alpha_j u_j \in X \quad \text{cuando } \nu \to \infty.
$$
$\blacksquare$
:::

### El teorema de punto fijo de Banach  

::: {.callout-note}
##### Definición: Contracción  

Sea $U$ un subconjunto de un espacio normado $X$.  
Un operador (una función) $A: U \to X$ se llama **una contracción** si existe una constante $q \in [0,1[$ tal que:  

$$
\|Ax - Ay\| \leq q \|x - y\|, \quad \forall\, x,y \in U.
$$

:::

::: {.callout-note}
##### Definición: Continuidad de un operador  

Sea $U$ un subconjunto de un espacio normado $X$, y sea $Y$ un espacio normado.  

- Una función $A: U \to Y$ se llama **continua en $x \in U$** si para toda sucesión $(x_n)$ de $U$ tal que $\lim\limits_{n \to \infty} x_n = x$, se cumple que  

$$
\lim\limits_{n \to \infty} Ax_n = Ax.
$$

- Un operador $A: U \to Y$ se llama **continuo** si es continuo en $x$ para todo $x \in U$.
:::

::: {.callout-note}
##### Proposición
Toda contracción es un operador continuo.
:::

::: {.callout-caution collapse="true"}
###### Prueba
La prueba es evidente, puesto que si $\|x_n - x\| \to 0$ cuando $n \to \infty$ entonces  
$\|Ax_n - Ax\| \to 0$ cuando $n \to \infty$ ya que  

$$
\|Ax_n - Ax\| \leq q \|x_n - x\| \to 0 \quad \text{cuando } n \to \infty.
$$

$\blacksquare$
:::

::: {.callout-note}
##### Definición: Operador Lipschitz
Un operador $A : U \to X$ se llama *Lipschitz* con constante de Lipschitz $L$ si existe una constante positiva $L$ tal que:  

$$
\|Ax - Ay\| \leq L \|x - y\|
$$

para todo $x, y \in U$.  
Es decir, una contracción es un operador *Lipschitz* con constante menor que uno.
:::

::: {.callout-note}
##### Definición: Operador lineal
Un operador $A : X \to Y$ donde $X$ y $Y$ son espacios normados se llama **lineal** si:  

$$
A(\alpha x + \beta y) = \alpha Ax + \beta Ay
$$  

para todo $x, y \in X$ y $\alpha, \beta \in \mathbb{C}$ (o $\mathbb{R}$).
:::

::: {.callout-note}
##### Teorema
Un operador lineal es continuo si y solo si es continuo en un elemento.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Sea $A : X \to Y$ un operador lineal continuo en $x_0 \in X$.  
Entonces, para todo $x \in X$ y para toda sucesión $(x_n) \to x$ cuando $n \to \infty$, se tiene que:

$$
Ax_n = A(x_n - x + x_0) + A(x - x_0) \;\to\; A(x_0) + A(x - x_0) = Ax,
\quad \text{cuando } n \to \infty.
$$  

::: {.callout-important collapse="true"}
###### Observación
Como $x_n - x + x_0 \to x_0$, entonces se puede aplicar el límite dentro de la expresión.
:::

$\blacksquare$
:::

::: {.callout-note}
##### Definición: Punto fijo
Un elemento $x \in X$ (espacio normado) se llama **punto fijo** de un operador 
$A : U \subseteq X \to X$ si:
$$
Ax = x.
$$
:::

::: {.callout-note}
##### Teorema
Toda contracción tiene a lo más un único punto fijo.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Supongamos que $x$ y $y$ son puntos fijos de una contracción $A$, entonces
$$
0 \neq \|x - y\| = \|Ax - Ay\| \leq q \|x - y\|,
$$
lo que implica que $q \geq 1$, lo cual es una contradicción. $\blacksquare$
:::

::: {.callout-note}
##### Teorema: (Banach) 
Sea $U$ un subconjunto completo de un espacio normado $X$ y sea $A : U \to U$ una contracción. Entonces $A$ tiene un punto fijo único.
:::

::: {.callout-caution collapse="true"}
###### Prueba
Sea $x_0 \in U$ entonces definimos recursivamente la siguiente sucesión en $U$:
$$
x_{n+1} := A x_n, \quad \text{para } n = 0,1,2,\ldots
$$

De donde se tiene que:
$$
\|x_{n+1} - x_n\| = \|Ax_n - Ax_{n-1}\| \leq q \|x_n - x_{n-1}\|,
$$

luego, por inducción se deduce que:
$$
\|x_{n+1} - x_n\| \leq q^n \|x_1 - x_0\|, \quad \text{para } n = 0,1,2,\ldots
$$

Por lo tanto para $m > n$ se tiene que:
$$
\|x_n - x_m\| \leq \|x_n - x_{n+1}\| + \|x_{n+1} - x_{n+2}\| + \cdots + \|x_{m-1} - x_m\|
$$
$$
\leq (q^n + q^{n+1} + \cdots + q^{m-1}) \|x_1 - x_0\|
$$
$$
\leq \frac{q^n}{1-q} \|x_1 - x_0\|.
$$

Como $q^n \to 0$ cuando $n \to \infty$, entonces $(x_n)$ es una sucesión de Cauchy y como $U$ es completo entonces existe $x \in U$ tal que $x_n \to x$ cuando $n \to \infty$. Finalmente, por la continuidad de $A$ se tiene que:
$$
x = \lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} A x_n = A x.
$$

La unicidad se tiene por el teorema anterior. $\blacksquare$
:::

::: {.callout-tip collapse="true"}
###### Ejemplo
Pruebe que la función $f(x) = \dfrac{x^2 - 2x}{6}, \; x \in [-1,1]$ tiene un punto fijo único en $[-1,1]$.

**Solución:** Como $\mathbb{R}$ es un espacio de Banach con la norma valor absoluto, se debe probar que $f(x) \in [-1,1] \quad \forall x \in [-1,1]$.

Como 
$$
f'(x) = \tfrac{1}{6}(2x-2) = \dfrac{x-1}{3} = 0 \iff x=1
$$ 
se tiene que los máximos y mínimos posibles están en $x=-1$ o $x=1$, así el valor máximo es 
$$
f(-1)=\tfrac{1}{2}
$$ 
y el valor mínimo es 
$$
f(1)=-\tfrac{1}{6}.
$$  

Por lo que para todo $x \in [-1,1]$ se tiene que $f(x) \in [-1,1]$, o sea que $f$ tiene un punto fijo en $[-1,1]$.

Para probar la unicidad, por el teorema del valor medio la constante $L$ de Lipschitz está dada por:
$$
L = \max_{x \in [-1,1]} |f'(x)| = \max_{x \in [-1,1]} \left|\frac{x-1}{3}\right| 
= \left|\frac{-1-1}{3}\right| = \frac{2}{3} < 1.
$$

Por lo tanto $f$ es una contracción en $[-1,1]$, luego la unicidad se tiene por el teorema anterior. $\blacksquare$
:::

### El teorema de la mejor aproximación  

::: {.callout-note}
##### Definición: Mejor aproximación

Sea $U$ un subconjunto de un espacio normado $X$ y sea $w \in X$.  
Un elemento $v \in U$ se llama **la mejor aproximación a $w$ con respecto a $U$** si:

$$
\|w - v\| \;\leq\; \inf_{u \in U} \|w - u\|,
$$

es decir, $v$ es el elemento en $U$ más cercano a $w$.
:::

::: {.callout-note}
##### Teorema

Sea $U$ un subespacio de dimensión finita de un espacio normado $X$.  
Entonces, para todo elemento $w \in X$, existe una mejor aproximación con respecto a $U$.
:::

::: {.callout-caution collapse="true"}
###### Prueba

Sea $w \in X$ y escojamos una sucesión $(u_n)$ tal que $u_n \in U$ y satisfaga lo siguiente:

$$
\lVert w - u_n \rVert \to d := \inf_{u \in U} \lVert w - u \rVert \quad \text{cuando } n \to \infty.
$$

Como 
$$
\lVert u_n \rVert \leq \lVert w - u_n \rVert + \lVert w \rVert
$$ 
entonces $(u_n)$ es una sucesión acotada.  

Por el Teorema 3 la sucesión $(u_n)$ contiene una subsucesión convergente $(u_{n(\ell)})$ con límite $v \in U$.  

Entonces:

$$
\lVert w - v \rVert = \lim_{\ell \to \infty} \lVert w - u_{n(\ell)} \rVert = d,
$$

con lo que se prueba el teorema. $\blacksquare$
:::

::: {.callout-note}
##### Teorema

Sea $U$ un subespacio vectorial de un espacio de pre-Hilbert $X$.  
Un elemento $v$ es la mejor aproximación a $w \in X$ con respecto a $U$ si y solo si:  

$$
\langle w - v, u \rangle = 0
$$  

para todo $u \in U$.  

Es decir, si y solamente si $w - v \perp U$.  
Además, para cada $w \in X$ existe a lo más una única mejor aproximación con respecto a $U$.
:::

::: {.callout-caution collapse="true"}
###### Prueba

(Ejercicio) $\blacksquare$
:::

::: {.callout-note}
##### Definición: Operador Acotado

Un operador $A : X \to Y$ donde $X$ y $Y$ son espacios normados, se llama **acotado** si existe un número positivo $C$ tal que:  

$$
\lVert Ax \rVert \leq C \lVert x \rVert
$$  

para todo $x \in X$.
:::

::: {.callout-note}
##### Teorema  

Un operador lineal $A : X \to Y$ es acotado si y solamente si:  

$$
\lVert A \rVert := \sup_{\lVert x \rVert = 1} \lVert Ax \rVert < \infty .
$$  

El número $\lVert A \rVert$ es la más pequeña cota para $A$ y se llama la **norma de $A$**.
:::

::: {.callout-caution collapse="true"}
###### Prueba  

Asuma $A$ es acotado con una cota $C$. Entonces  

$$
\sup_{\lVert x \rVert = 1} \lVert Ax \rVert < C < \infty ,
$$  

y entonces $\lVert A \rVert$ es menor o igual que cualquier otra cota para $A$.  

Inversamente, si $\lVert A \rVert < \infty$, entonces usando la linealidad de la norma se tiene que:  

$$
\lVert Ax \rVert 
= \left\lVert A\!\left(\frac{x}{\lVert x \rVert}\right) \right\rVert \lVert x \rVert 
\leq \lVert A \rVert \, \lVert x \rVert ,
$$  

para todo $x \neq 0$, por lo tanto $A$ es acotado con cota $C = \lVert A \rVert$.  

$\blacksquare$
:::

::: {.callout-note}
##### Teorema  

Sea $U$ un subespacio vectorial completo de un espacio pre-Hilbert $X$. Entonces para cada elemento $w \in X$ existe una única mejor aproximación con respecto a $U$.  

- El operador $P : X \to U$ que le asigna a $w \in X$ su mejor aproximación es un operador lineal acotado con las siguientes propiedades:  

$$
P^2 = P 
\quad \text{y} \quad 
\lVert P \rVert = 1 .
$$  

- Este operador se conoce como la **proyección ortogonal** de $X$ sobre $U$.
:::

::: {.callout-caution collapse="true"}
###### Prueba  

(ejercicio) $\blacksquare$
:::

::: {.callout-note}
##### Corolario  

Sea $U$ un subespacio vectorial de dimensión finita de un espacio pre-Hilbert $X$ con base $u_{1}, u_{2}, \ldots, u_{n}$.  
Entonces la combinación lineal  

$$
v = \sum_{k=1}^{n} \alpha_{k} u_{k}
$$  

es la mejor aproximación para $w \in X$ con respecto a $U$ si y solamente si los coeficientes $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ satisfacen las *ecuaciones normales*:  

$$
\sum_{k=1}^{n} \alpha_{k} \langle u_{k}, u_{j} \rangle = \langle w, u_{j} \rangle, 
\quad \text{para } j = 1, 2, \ldots, n.
$$
:::

::: {.callout-caution collapse="true"}
###### Prueba  

Es evidente que la ecuación (8) es equivalente a la ecuación (7).  
$\blacksquare$
:::

::: {.callout-note}
##### Corolario  

Sea $U$ un subespacio vectorial de dimensión finita de un espacio pre-Hilbert $X$ con base ortonormal $u_{1}, u_{2}, \ldots, u_{n}$.  
Entonces la proyección ortogonal está dada por:  

$$
Pw = \sum_{k=1}^{n} \langle w, u_{k} \rangle u_{k}, \quad \text{con } w \in X.
$$
:::

::: {.callout-caution collapse="true"}
###### Prueba  

Es evidente de (8) puesto que  

$$
\langle u_{k}, u_{j} \rangle =
\begin{cases}
0 & \text{si } k \neq j, \\
1 & \text{si } k = j,
\end{cases}
$$  

luego $\alpha_{k} = \langle w, u_{k} \rangle$ para $k = 1, 2, \ldots, n$.  
$\blacksquare$
:::

::: {.callout-tip collapse="true"}
###### Ejemplo  

Sea $P_{2}[0,1]$ un subespacio de dimensión finita del espacio pre–Hilbert $C[0,1]$ dotado del producto interno  

$$
\langle f,g \rangle := \int_{0}^{1} f(x)g(x)\,dx.
$$  

Es fácil verificar que  
$$
\mathcal{B} = \{\,1, \ \sqrt{3}(2x-1), \ \sqrt{5}(6x^{2}-6x+1)\,\}
$$  
es una base ortonormal de $P_{2}[0,1]$.  

Si tomamos $f(x) = e^{x}$, entonces la mejor aproximación de $f(x)$ en $P_{2}[0,1]$ es:  

$$
Pf = \langle e^{x},1 \rangle \cdot 1 
+ \langle e^{x}, \sqrt{3}(2x-1)\rangle \cdot \sqrt{3}(2x-1) 
+ \langle e^{x}, \sqrt{5}(6x^{2}-6x+1)\rangle \cdot \sqrt{5}(6x^{2}-6x+1),
$$  

es decir,  

$$
Pf = (e-1) + \sqrt{3}(3-e)\sqrt{3}(2x-1) + \sqrt{5}(7e-19)\sqrt{5}(6x^{2}-6x+1).
$$  

Por lo tanto,  

$$
Pf \approx 1.01 + 0.85x + 0.84x^{2}.
$$

```{r}
#| echo: false
#| fig-width: 7
#| fig-height: 5
#| fig-dpi: 300
#| fig-align: center
#| fig-cap: "Gráfico de $e^x$ y aproximación cuadrática"
#| out-width: "70%"

# Datos
x <- seq(0, 1, length.out = 400)
f <- exp(x)
p <- 1.01 + 0.85*x + 0.84*x^2

# Estética básica
op <- par(mar = c(5,5,3,2))  # márgenes
on.exit(par(op), add = TRUE)

# Gráfico
plot(x, f, type = "l", lwd = 3, col = "red",
     xlab = "x", ylab = "y",
     main = "Gráficamente:\nGráfico de e^x y polinomio cuadrático")

# # Título superior (opcional, estilo como en la imagen)
# mtext("Gráficamente:", side = 3, line = 1.5, cex = 1.6, font = 2)

# Segunda curva
lines(x, p, lwd = 3, col = "blue")

# Leyenda
legend("topleft",
       legend = c(expression(e^x),
                  expression(1.01 + 0.85*x + 0.84*x^2)),
       col = c("red", "blue"), lwd = 3, bty = "n")
```

:::

::: {.callout-note}
##### Teorema

En una Regresión Múltiple, encontrar los parámetros $\beta_1, \beta_2, \ldots, \beta_p$ de tal manera que:

$$
\sum_{i=1}^n e_i^2
$$

sea mínima, es equivalente a proyectar ortogonalmente el vector $y$ sobre el espacio generado por $x_1, x_2, \ldots, x_p$.

Es decir, primero se ortonormaliza la base con *Gram–Schmidt* y luego se proyecta el vector $y$ sobre el subespacio generado por la base ortonormal $\{v_1, v_2, \ldots, v_p\}$.  
O sea, primero se aplica el método de Gram–Schmidt a la base $B = \{x_1, x_2, \ldots, x_p\}$ y se obtiene una base ortonormal $B' = \{v_1, v_2, \ldots, v_p\}$.

Así, la regresión se realiza proyectando el vector $y$ sobre el subespacio generado por la base ortonormal $B'$:

$$
\hat{y} = \langle y, v_1 \rangle v_1 + \langle y, v_2 \rangle v_2 + \cdots + \langle y, v_p \rangle v_p .
$$

Donde $\langle x, y \rangle = x \cdot y$ es el producto punto.  
Lo anterior es equivalente a hacerlo de la forma clásica:

$$
\hat{y} = X \beta = X (X^T X)^{-1} X^T y .
$$

#### Gráficamente:

<!--   -->

::: {.callout-caution collapse="true"}
##### Prueba

Es claro, pues este teorema es nada más otra forma de calcular la proyección $P$ de la variable a predecir sobre el subespacio generado por las variables predictoras. $\blacksquare$

:::

:::

# Solución numérica de ecuaciones no lineales.

## El método de aproximaciones sucesivas.

### Teoremas de convergencia y del error

::: {.callout-note}
##### Teorema
Sea $U$ un subconjunto completo de un espacio normado $X$ y $A:U \to U$ una contracción. Entonces las **aproximaciones sucesivas**:

$$
x_{n+1} = Ax_n, \quad \text{para } n = 0,1,2,\ldots,
$$

con $x_0$ arbitrario en $U$, convergen al punto fijo único $x$ de $A$.
:::

::: {.callout-caution collapse="true"}
##### Prueba

Sea $x_0 \in U$ entonces definimos recursivamente la siguiente sucesión en $U$:

$$
x_{n+1} := Ax_n, \quad \text{para } n=0,1,2,\ldots
$$

De donde se tiene que:

$$
\|x_{n+1} - x_n\| = \|Ax_n - Ax_{n-1}\| \leq q \|x_n - x_{n-1}\|,
$$

luego por inducción se deduce que:

$$
\|x_{n+1} - x_n\| \leq q^n \|x_1 - x_0\|, \quad \text{para } n=0,1,2,\ldots
$$

Por lo tanto para $m > n$ se tiene que:

\begin{align*}
\|x_n - x_m\| &\leq \|x_n - x_{n+1}\| + \|x_{n+1} - x_{n+2}\| + \cdots + \|x_{m-1} - x_m\| \\
&\leq (q^n + q^{n+1} + \cdots + q^{m-1}) \|x_1 - x_0\| \\
&\leq \frac{q^n}{1-q} \|x_1 - x_0\|.
\end{align*}

Como $q^n \to 0$ cuando $n \to \infty$, entonces $(x_n)$ es una sucesión de Cauchy y como $U$ es completo existe $x \in U$ tal que $x_n \to x$ cuando $n \to \infty$.  

$\blacksquare$
:::

::: {.callout-note}
##### Corolario: Cota del Error a Priori
Con las mismas hipótesis del teorema anterior se tiene el siguiente *estimado para el error a priori*:

$$
\|x_n - x\| \leq \frac{q^n}{1-q} \, \|x_1 - x_0\|.
$$

::: {.callout-caution collapse="true"}
##### Prueba
Es evidente de la desigualdad (1.1).  

$\blacksquare$
:::
:::

::: {.callout-note}
##### Corolario: Cota del Error a Posteriori
Con las mismas hipótesis del teorema anterior se tiene el siguiente *estimado para el error a posteriori*:

$$
\|x_n - x\| \leq \frac{q}{1-q} \, \|x_n - x_{n-1}\|.
$$

::: {.callout-caution collapse="true"}
##### Prueba
Se deduce del error a priori iniciando con $x_0 = x_{n-1}$.  

$\blacksquare$
:::
:::


::: {.callout-note}
##### Teorema [Versión 1]
Sea $D \subset \mathbb{R}$ un cerrado y sea $g: D \to D$ una función continuamente diferenciable con la siguiente propiedad:

$$
q := \sup_{x \in D} |g'(x)| < 1.
$$

Entonces la ecuación $g(x) = x$ tiene solución única $x \in D$ y la sucesión de aproximaciones sucesivas:

$$
x_{n+1} := g(x_n), \quad \text{para } n = 0,1,2,\ldots
$$

con $x_0$ arbitrario en $D$ converge a esta solución. Además se tiene el siguiente *estimado para el error a priori*:

$$
|x_n - x| \leq \frac{q^n}{1-q}|x_1 - x_0|,
$$

y el siguiente *estimado para el error a posteriori*:

$$
|x_n - x| \leq \frac{q}{1-q}|x_n - x_{n-1}|.
$$

Además, si $D = [a,b]$ entonces se tiene también la siguiente cota del error:

$$
|x_n - x| \leq q^n \max\{x_0 - a, b - x_0\}.
$$

::: {.callout-caution collapse="true"}
##### Prueba

El espacio $\mathbb{R}$ equipado de la norma valor absoluto $|\cdot|$ es un espacio de Banach.  
Por el teorema del valor medio, para todo $x, y \in D$ con $x < y$ se tiene que:

$$
g(x) - g(y) = g'(\xi)(x-y)
$$

para algún punto $\xi \in ]x,y[$. Por lo tanto:

$$
|g(x) - g(y)| \leq \sup_{\xi \in D} |g'(\xi)| \cdot |x-y| = q|x-y|,
$$

lo cual también es válido para $x,y \in D$ con $x \geq y$.  
Por lo tanto $g$ es una contracción, luego aplicando el Teorema de Banach o el Teorema 1 se tiene la existencia y unicidad del punto fijo.

De los corolarios 1 y 2 se tienen obviamente las desigualdades (1.2) y (1.3).  
Para probar la cota del error (1.4) note que:


\begin{align*}
|x_n - x|
&= |g(x_{n-1}) - g(x)| = |g'(\xi_1)| \cdot |x_{n-1} - x| \leq q |x_{n-1} - x| \\[1ex]
&= q |g(x_{n-2}) - g(x)| = q |g'(\xi_2)| \cdot |x_{n-2} - x| \leq q^2 |x_{n-2} - x| \\[1ex]
&\leq \ldots \\[1ex]
&\leq q^n |x_0 - x| \\[1ex]
&\leq q^n \max\{x_0 - a, \; b - x_0\}.
\end{align*}


$\blacksquare$
:::
:::

::: {.callout-note}
##### Teorema [Versión 2]
Sea $g \in C[a,b]$, con $g:[a,b] \to [a,b]$. Entonces:

1. $g$ tiene un punto fijo en $[a,b]$.

2. Además, si $g'(x)$ existe en $]a,b[$ y $|g'(x)| \leq q < 1$, para todo $x \in ]a,b[$, entonces $g$ tiene un punto fijo único en $[a,b]$.

::: {.callout-caution collapse="true"}
##### Prueba

**I Caso**: Si $g(a) = a$ o $g(b) = b$ se tiene la prueba.  

**II Caso**: Si $g(a) \neq a$ y $g(b) \neq b \Rightarrow g(a) > a$ y $g(b) < b$  

Tome $h(x) = g(x) - x$, note que:  

- $h$ es continua en $[a,b]$,  
- $h(a) = g(a) - a > 0$,  
- $h(b) = g(b) - b < 0$.  

Luego $h(a)$ y $h(b)$ tienen signos opuestos, usando el teorema de los valores intermedios se tiene que existe $x \in [a,b]$ tal que $h(x) = 0 \Rightarrow g(x) - x = 0 \Rightarrow g(x) = x$, por lo tanto $x$ es el punto fijo de $g$.  

2. Suponga que $g$ tiene dos puntos fijos en $[a,b]$, sean estos $x$ y $y$, con $x \neq y$, entonces por el Teorema del Valor Medio existe $\xi \in ]a,b[$ tal que:  

$$
|x-y| = |g(x) - g(y)| = g'(\xi)|x-y| \leq q|x-y| < |x-y|
$$

De donde $|x-y| < |x-y|$, lo cual es una contradicción, luego se tiene que $x=y$.  

$\blacksquare$
:::
:::


::: {.callout-tip collapse="true"}
##### Ejemplo
Pruebe que 

$$
f(x) = \frac{x^2 - 2x}{6}, \quad x \in [-1,1]
$$

tiene un punto fijo en $[-1,1]$.

::: {.callout-caution collapse="true"}
##### Solución

Se debe probar que $f(x) \in [-1,1]$ para todo $x \in [-1,1]$.  
Como 
$$
f'(x) = \tfrac{1}{6}(2x-2) = \frac{x-1}{3} = 0 \;\;\Leftrightarrow\;\; x=1
$$
entonces los máximos o mínimos posibles están en $x=-1$ o $x=1$.  

Como $f(-1) = \tfrac{1}{2}$ es máximo y $f(1) = -\tfrac{1}{6}$ es mínimo entonces para todo $x \in [-1,1]$ se tiene que $f(x)\in[-1,1]$, de donde $f$ tiene un punto fijo en $[-1,1]$.

Solo se ha probado que existe por lo menos un punto fijo, ahora tenemos que probar que es único.  
Se debe probar que existe $q<1$ tal que $|f'(x)| \leq q < 1$, para todo $x \in ]-1,1[$.  

Note que:

$$
|f'(x)| = \left|\frac{x-1}{3}\right| \leq \left|\frac{-1-1}{3}\right| = \tfrac{2}{3} < 1, 
\quad \text{para todo } x \in ]-1,1[.
$$

Luego $f(x)$ tiene un punto fijo único en $[-1,1]$.  
$\blacksquare$
:::

:::




::: {.callout-note}
##### Teorema

Sea $x$ un punto fijo de una función continuamente diferenciable $g$ tal que $|g'(x)|<1$.  
Entonces el método de las aproximaciones sucesivas 
$$
x_{n+1} := g(x_n), \quad \text{para } n=0,1,2,\ldots
$$
es localmente convergente, es decir, existe un vecindario $\mathcal{B}$ del punto fijo $x$ de $g$ tal que el método de aproximaciones sucesivas converge a $x$, para $x_0 \in \mathcal{B}$.

::: {.callout-caution collapse="true"}
##### Prueba

Como $g'$ es continua y $|g'(x)|<1$ entonces existe una constante $0<q<1$ y $\delta>0$ tal que $|g'(y)|<q$ para todo $y\in \mathcal{B}:=[x-\delta,x+\delta]$.  

Entonces se tiene que:

$$
|g(y)-x| = |g(y)-g(x)| \leq q|y-x| < |y-x| \leq \delta
$$

para todo $y\in \mathcal{B}$.  

Por lo que se deduce que $g$ mapea $\mathcal{B}$ en sí mismo, o sea que $g:\mathcal{B}\to \mathcal{B}$ es una contracción, por lo que el resultado se tiene del Teorema 1.  

$\blacksquare$
:::
El Teorema se ilustra en la Figura.
![El método de aproximaciones sucesivas.](Imagenes/3 graph.png)
:::


**Algoritmo**: Método de las aproximaciones sucesivas

**Entrada:** $x_0$ (aproximación inicial), $Tol$, $N$, $g(x)$  
**Salida:** $x$ (punto fijo aproximado) o mensaje de error  

**Pasos:**

1. $i \leftarrow 1$  
2. Mientras $i \leq N$, siga los pasos 3–6:  
   - $x \leftarrow g(x_0)$  
   - Si $|x - x_0| < Tol$:  
     - Salida: $x$  
     - **Parar**  
   - $i \leftarrow i+1$  
   - $x_0 \leftarrow x$  
3. Mensaje de error: *“Número máximo de iteraciones excedido”*.  
   **Parar**
   
Una implementación en R es la siguiente:
   
```{r}
punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_tem <- p0
  while (i <= n) {
    p <- g(p0_tem)
    if (abs(p - p0_tem) < tol) {
      return(p)
    }
    i <- i + 1
    p0_tem <- p
  }
  return(Inf)
}
```

::: {.callout-tip collapse="true"}
###### Ejemplo

Sea $f(x) = e^{-x}$, es fácil probar que $f(x)$ mapea  $A = [0.5, 0.69]$ en sí mismo.

::: {.callout-caution collapse="true"}
##### Prueba

Como $f(x)$ es estrictamente decreciente y continua en $\mathbb{R}$, en particular lo es en el intervalo $A$, por lo que su imagen sobre $A$ es:
$$
f([0.5, 0.69]) = [f(0.69), f(0.5)]
$$
Calculamos:
$$
f(0.5) = e^{-0.5} \approx 0.6065, \quad f(0.69) = e^{-0.69} \approx 0.5016
$$
Por lo tanto:
$$
f([0.5, 0.69]) = [0.5016, 0.6065]
$$
Observamos que:
$$
[0.5016, 0.6065] \subseteq [0.5, 0.69]
$$
La función $f(x) = e^{-x}$ mapea el intervalo $A = [0.5, 0.69]$ en sí mismo, es decir:
$$
f(A) \subseteq A
$$
:::

Como $f$ es continuamente diferenciable, tome:

$$
q = \max_{x \in A} \, \big| f'(x) \big| 
   = \max_{x \in A} \, \big| -e^{-x} \big| 
   \approx 0.606531 < 1
$$

Si se ejecuta el programa iterativo en **R** como sigue:

```{r}
#| echo: false

punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_tem <- p0
  while (i <= n) {
    p <- g(p0_tem)
    cat(sprintf("En la iteración %d el valor de P es %.15f\n", i, p))
    if (abs(p - p0_tem) < tol) {
      return(p)
    }
    i <- i + 1
    p0_tem <- p
  }
  cat("El método no converge\n")
  return(NULL)
}
```


```{r}
g <- function(x) exp(-x)
sol3 <- punto.fijo(p0 = 0.55, tol = 1e-6, n = 30, g = g)
```

es decir, tomando $x_0 = 0.55$ como aproximación inicial, con $\varepsilon = Tol = 10^{-6}$ para el algoritmo anterior obtenemos que el “punto fijo” de $F$ es  

$$
x = x_{19} = 0.567143650676,
$$  

pues en $x_{19}$ se terminó la ejecución de la función,  
como se aprecia en la salida del programa:

```{r}
#| echo: false
cat("Solución punto.fijo:", sol3, "\n")
```

Por otro lado el error absoluto al calcular $x_{12} = 0.567124201933893$ es igual a:

$$
|x - x_{12}| \approx 1.91 \cdot 10^{-5},
$$

mientras que usando el error a priori se obtiene:

$$
|x - x_{12}| \leq \frac{q^{12}}{1-q} |x_1 - x_0| = 1.70 \cdot 10^{-4}
$$

y usando el error a posteriori se obtiene que:

$$
|x - x_{12}| \leq \frac{q}{1-q} |x_{12} - x_{11}| = 8.13 \cdot 10^{-5}
$$

que es una mejor estimación del verdadero error.  
Usando el error a priori se deduce que para obtener una precisión de $\varepsilon = 10^{-6}$ se requieren al menos:

$$
n \geq \frac{\log \left(\dfrac{\varepsilon(1-q)}{|x_1 - x_0|}\right)}{\log(q)} 
\approx 22.3 \leq 23 \text{ iteraciones},
$$

pero se observa que el programa requirió de 19 iteraciones. 

Ver la versión recursiva en el *HTML*.

```{r}
#| code-fold: true

punto.fijo.recursivo <- function(p0, tol, n, g) {
  p1 <- g(p0)
  if (abs(p0 - p1) < tol || n < 1) {
    if (n > 1) {
      return(p1)
    } else {
      return(Inf)
    }
  } else {
    return(punto.fijo.recursivo(p1, tol, n - 1, g))
  }
}
sol3 <- punto.fijo.recursivo(p0 = 0.55, tol = 1e-6, n = 30, g = g)
cat("Solución punto.fijo.recursivo:", sol3, "\n")
```

:::


::: {.callout-tip collapse="true"}
###### Ejemplo

Resuelva la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$.

**Solución:**  
Se debe plantear un problema de encontrar los puntos fijos de una función $g(x)$ que sea equivalente a resolver la ecuación $x^3 - x - 1 = 0$.  
Resolver $x^3 - x - 1 = 0$ es equivalente a resolver la ecuación $x^3 - 1 = x$, entonces se puede tratar de encontrar los puntos de $g(x) := x^3 - 1$.  

Pero $g(x)$ no cumple las hipótesis del Teorema de Punto Fijo de Banach, pues $g'(x) = 3x^2 > 0 \implies g(x)$ es creciente en $[1,2]$, luego $g(1) = 0$ y $g(2) = 7$ son el mínimo y el máximo de $g(x)$ en el intervalo $[1,2]$ respectivamente, por lo que $g(x) \notin [1,2] \quad \forall x \in [1,2]$.

Otro intento se puede hacer usando el hecho de que:

$$
x^3 - x - 1 = 0 \iff x = \pm \sqrt{1 + \frac{1}{x}},
$$

luego tome 

$$
g(x) := \sqrt{1 + \frac{1}{x}}, 
\quad g'(x) = -\frac{1}{2x^2\sqrt{1 + \frac{1}{x}}} < 0 \quad \text{en } [1,2].
$$

Esto implica que $g(x)$ es decreciente en $[1,2]$, luego:

$$
g(1) = \sqrt{2} \approx 1.41, 
\quad g(2) = \sqrt{\tfrac{3}{2}} \approx 1.22.
$$

El máximo y el mínimo respectivamente de $g(x)$ en $[1,2]$, por lo que $g(x) \in [1,2]$  
$\forall x \in [1,2]$, luego la función $g(x)$ tiene al menos un punto fijo en $[1,2]$.

Se probó que $g : [1,2] \to [1,2]$, falta probar que $g$ es una contracción en el intervalo $[1,2]$. Veamos

\begin{align*}
g'(x) = -\frac{1}{2x^2 \sqrt{1 + \frac{1}{x}}} 
\Rightarrow |g'(x)| = \frac{1}{2x^2 \sqrt{1 + \frac{1}{x}}} \leq \frac{1}{2} := q < 1
\end{align*}

de donde se puede tomar $q := \frac{1}{2}$.  
Se pueden ver las ejecuciones de las funciones en **R** de punto fijo con $x_0 = 2$ y $\epsilon = 10^{-5}$ en el archivo `punto_fijo.html`.

Luego el punto fijo de $g(x)$ y solución de la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$ es:  
$$x = 1.32471747253653$$  
Que coincide con la solución encontrada por nuestro programa. Gráficamente se ilustra en la Figura

```{r}
#| echo: false
# Definir los valores de x
x <- seq(0.01, 3, by = 0.01)

# Definir la función G(x)
G <- function(x) sqrt(1 + 1/x)

# Crear el gráfico
plot(x, G(x), type = "l", col = "blue", lwd = 2,
     ylim = c(0, 3.2), xlab = "x", ylab = "y", main = "Gráfico de G(x) y x")

# Agregar la recta y = x
lines(x, x, col = "red", lty = 2, lwd = 2)

# Agregar leyenda
legend("topright", legend = c("G(x) = sqrt(1 + 1/x)", "y = x"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```


$\blacksquare$

:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Dada la ecuación del ejemplo anterior $x^3 - x - 1 = 0$ en el intervalo $[1,2]$,  ¿cuántas iteraciones se requieren para obtener un error absoluto menor que $10^{-5}$?

**Solución:** Recuerde que $q = \frac{1}{2}$, de donde se tiene que:

\begin{align*}
|x_n - x| 
&\leq q^n \max\{x_0 - a, b - x_0\} \\
&= \left(\frac{1}{2}\right)^n \max\{1, 0\} \quad \text{(con $x_0 = 2$)} \\
&\leq \left(\frac{1}{2}\right)^n
\end{align*}

Luego:

\begin{align*}
|x_n - x| \leq 10^{-5} 
\Leftrightarrow \left(\frac{1}{2}\right)^n \leq 10^{-5} 
\Leftrightarrow n \geq 16.6
\end{align*}

Tome $n = 17$. $\blacksquare$

::: {.callout-important collapse="true"}
###### Observación

**Observación 1** En la práctica el programa requirió solamente 9 iteraciones.
:::
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Para la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$, con $x_0 = 2$, estime usando el error a priori (1.2) ¿Cuántas iteraciones se requieren para obtener un error absoluto menor que $10^{-5}$?

**Solución:** Se tiene que

$$
q = \frac{1}{2}, \quad x_0 = 2, \quad g(x) = \sqrt{1 + \frac{1}{x}}
$$

De donde se obtiene que $x_1 = \sqrt{1 + \frac{1}{2}} = \sqrt{\frac{3}{2}} \approx 1.2$, entonces:

\begin{align*}
|x_n - x| 
&\leq \frac{\left(\frac{1}{2}\right)^n}{1 - \frac{1}{2}} |2 - 1.2| \\
&= \left(\frac{1}{2}\right)^{n-1} \cdot 0.8
\end{align*}

entonces:

\begin{align*}
|x_n - x| 
&\leq 10^{-5} 
\Leftrightarrow \left(\frac{1}{2}\right)^{n-1} \cdot 0.8 \leq 10^{-5} \\
&\Leftrightarrow (n-1)(-\log(2)) \leq -5 - \log(0.8)
\end{align*}

esto implica que $n \geq 17.28$, por lo que se puede tomar $n = 18$. $\blacksquare$

::: {.callout-important collapse="true"}
###### Observación

**Observación 2** En la práctica el programa requirió solamente 9 iteraciones.
:::
:::



## Método de bisección, método regula falsi y método de la secante.

### Método de la Bisección

Las hipótesis de este método son:

- $f$ debe ser continua en el intervalo $[a, b]$.
- $f(a)$ y $f(b)$ deben tener signos opuestos.

Luego, por el Teorema de los Valores Intermedios, existe $x \in [a, b]$ tal que $f(x) = 0$, gráficamente se ilustra en la Figura .

![El Método de la Bisección](Imagenes/4 grpah.png){width=400px fig-align="center"}

La idea es encontrar una sucesión $(x_n)$ tal que $x_n \to x$ cuando $n \to \infty$ tal que $f(x) = 0$. Para encontrar la sucesión $(x_n)$, la idea es la siguiente:

- Tome $a_1 = a$, $b_1 = b$, $x_1 = \dfrac{a_1 + b_1}{2}$.
- Si $f(x_1) = 0$, entonces ya se tiene el cero de la ecuación $x = x_1$.
- Si no:
  - Si $f(x_1)$ y $f(a)$ tienen el mismo signo, se toma:
  
    $$
    a_2 = x_1, \quad b_2 = b_1, \quad x_2 = \dfrac{a_2 + b_2}{2}.
    $$
  - Si no se toma:

  $$
  a_2 = a_1, \quad b_2 = x_1, \quad x_2 = \dfrac{a_2 + b_2}{2},
  $$

  y así sucesivamente hasta que $f(x_i) \approx 0$ o hasta superar el número máximo de iteraciones. El pseudocódigo se puede escribir como sigue:


#### Algoritmo Método de la Bisección

**Entrada:** $a, b, Tol$ (tolerancia), $N$, $f$  
**Salida:** Aproximación de $x$ (cero de la ecuación) o mensaje de error

1. $i \leftarrow 1$  
2. Mientras $i \leq N$, siga los pasos 3–6  
3. $x \leftarrow \dfrac{a + b}{2}$  
4. Si $f(x) = 0$ o $|b - a| < Tol$  
  -  Salida ($x$)  
  - **Parar**  
5. $i \leftarrow i + 1$  
6. Si $f(a) \cdot f(x) > 0$  
   - $a \leftarrow x$  
   -  Si no  
   -  $b \leftarrow x$  
7. Salida (“Número máximo de iteraciones excedido”)  
    **Parar**


Este algoritmo se puede programar iterativamente en **R**, ver el archivo `biseccion.html`.

```{r}
# Versión detallada: imprime cada iteración
biseccion <- function(a, b, tol, n, G) {
  i <- 1
  a1 <- a
  b1 <- b
  if (G(a) * G(b) > 0) {
    cat("No cumple las hipótesis\n")
  } else {
    while (i <= n) {
      X <- (a1 + b1) / 2
      cat("En la iteración", i, "el valor de X es", X, "\n")
      if (G(a) * G(X) > 0) {
        a1 <- X
      } else {
        b1 <- X
      }
      if ((b1 - a1) < tol) {
        return(X)
      }
      i <- i + 1
    }
    cat("El método no converge\n")
    return(NA)
  }
}
```


::: {.callout-tip collapse="true"}
###### Ejemplo

Resuelva la ecuación $x^3 + 4x^2 - 10 = 0$ en el intervalo $[1,2]$ con una tolerancia de $\varepsilon = 10^{-6}$.

**Solución:** 
```{r}
# Definición de la función
G <- function(x) x^3 + 4*x^2 - 10
# Gráfico de la función
curve(G, from = 1, to = 2, col = "blue", lwd = 2,
      main = "Gráfico de G(x) = x^3 + 4x^2 - 10",
      xlab = "x", ylab = "G(x)")
abline(h = 0, col = "red", lty = 2)
# Llamada a la función biseccion
resultado <- biseccion(1, 2, 1e-6, 50, G)
cat("Resultado final:", resultado, "\n")
```

$\blacksquare$
:::

#### Estudio del error en el método de la bisección

::: {.callout-note}
##### Teorema

Sea $f \in C[a,b]$, con $f(a)f(b) < 0$. Entonces el algoritmo de la bisección produce una sucesión $(x_n)$ que aproxima a $x$, el cero de la ecuación $f(x) = 0$, con un error absoluto tal que:

$$
|x_n - x| < \frac{b - a}{2^n} \quad \text{para } n \geq 1.
$$

::: {.callout-caution collapse="true"}
##### Prueba

\begin{align*}
|b_1 - a_1| &= |b - a| \\
|b_2 - a_2| &= \frac{1}{2} |b - a| \\
|b_3 - a_3| &= \frac{1}{2^2} |b - a| \\
&\vdots \\
|b_n - a_n| &= \frac{1}{2^{n-1}} |b - a|
\end{align*}

Como $x \in \, ]a_n, b_n[$ y $x_n = \dfrac{a_n + b_n}{2}$, se tiene que:

\begin{align*}
|x_n - x| &\leq \frac{|b_n - a_n|}{2} = \frac{1}{2} \cdot \frac{1}{2^{n-1}} |b - a| = \frac{1}{2^n}(b - a)
\end{align*}

$\blacksquare$
:::
:::
::: {.callout-important collapse="true"}
###### Observación

Sea $|x_n - x| < \dfrac{b - a}{2^n} \Rightarrow \dfrac{|x_n - x|}{\frac{1}{2^n}} < b - a = k$,  
esto implica que la sucesión $(x_n)$ es $\mathcal{O}\left( \dfrac{1}{2^n} \right)$,  
es decir, $x_n \to x$ con rapidez $\dfrac{1}{2^n}$, lo cual es bastante rápido.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Para $f(x) = x^3 + 4x^2 - 10$, con $a = 1$ y $b = 2$; usando (1.5), hallar el $n \in \mathbb{N}$ necesario para tener un error absoluto menor a $\varepsilon = 10^{-6}$.

**Solución:**

\begin{align*}
|x_n - x| &< \frac{b - a}{2^n} < 10^{-6} \\
\Leftrightarrow \quad 2^{-n} &< 10^{-6} \\
\Leftrightarrow \quad -n \log 2 &< -6 \\
\Leftrightarrow \quad n &> \frac{6}{\log 2} \\
\Leftrightarrow \quad n &> 19.9
\end{align*}

Entonces se puede escoger $n = 20$. $\blacksquare$
:::

::: {.callout-important collapse="true"}
###### Observación

$n = 20$ fue exactamente lo que requirió el programa.
:::

### Método de Newton–Raphson

El método de Newton–Raphson es uno de los más poderosos para resolver la ecuación $f(x) = 0$.  
Si $f$ es una función de una variable y $x_0$ es una aproximación a cero de la función $f$, entonces en un vecindario de $x_0$, por la fórmula de Taylor se tiene que:

$$
f(x) \approx f(x_0) + f'(x_0)(x - x_0).
$$

Si se define $g(x) := f(x_0) + f'(x_0)(x - x_0)$, entonces el cero de la función afín $g(x)$ se puede considerar como una nueva aproximación al cero de $f(x)$, el cual denotamos por $x_1$.  
Luego se tiene que:

$$
g(x_1) = f(x_0) + f'(x_0)(x_1 - x_0) = 0,
$$

despejando se tiene:

$$
x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}.
\tag{1.6}
$$

Geométricamente, la función afín $g(x)$ representa la recta tangente a $f(x)$ en el punto $x_0$. Esto se ilustra en la siguiente Figura.

![El Método de  Newton](Imagenes/MetodoNR.png){width=400px fig-align="center"}

De la Figura también se puede deducir el método de Newton–Raphson.  
Nótese que:

$$
f'(x_i) = \frac{f(x_i) - 0}{x_i - x_{i+1}}
$$

lo cual implica que:

$$
x_i - x_{i+1} = \frac{f(x_i)}{f'(x_i)}
$$

de donde:

$$
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
$$

que es la sucesión del método de Newton–Raphson.

::: {.callout-important collapse="true"}
###### Observación

El método de Newton–Raphson consiste en diseñar un algoritmo que calcule la sucesión:

$$
x_n =
\begin{cases}
x_0 & \text{si } n = 0 \\
x_{n-1} - \dfrac{f(x_{n-1})}{f'(x_{n-1})} & \text{si } n > 0
\end{cases}
$$
:::

::: {.callout-note}
##### Algoritmo 3 – Método de Newton–Raphson

**Entrada:** $x_0$, $N$, $Tol$, $f$  
**Salida:** La solución $x$ aproximada de la ecuación $f(x) = 0$ o mensaje de error.

**Pasos:**

1 $i \leftarrow 1$  
2 Mientras $i \leq N$, hacer pasos 3–6:  
  3 $x \leftarrow x_0 - \dfrac{f(x_0)}{f'(x_0)}$  
  4 Si $|x - x_0| < Tol$  
  Salida $(x)$  
  Parar  
   5 $i \leftarrow i + 1$  
   6 $x_0 \leftarrow x$  
7 Salida (“Número máximo de iteraciones excedido”)  
 Parar
:::

El método de Newton–Raphson se puede implementar iterativamente en **R**.  
Ver el archivo `Newton_Raphson.html`.

::: {.callout-tip collapse="true"}
###### Ejemplo

**Ejemplo 8.** Ejecutando el programa iterativo para resolver la ecuación  
$e^{-x} - x = 0$ con $x_0 = 1$ como aproximación inicial y una tolerancia de $\varepsilon = 10^{-6}$,  
se obtiene la siguiente solución en el archivo `Newton_Raphson.html`.
:::

::: {.callout-note}
##### Teorema

Sea $f \in C^2[a,b]$. Si $x \in [a,b]$ con $f(x) = 0$ y $f'(x) \ne 0$, entonces existe $\varepsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que está bien definida y converge a $x$ cuando $n \to \infty$, para todo $x_0 \in [x - \varepsilon, x + \varepsilon]$.

::: {.callout-caution collapse="true"}
##### Prueba

Sea

$$
g(x) := x - \frac{f(x)}{f'(x)}.
$$

Nótese que si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, entonces $g(\tilde{x}) \in [a,b]$ y $g(\tilde{x}) = \tilde{x}$.

Se define:

$$
p_n := \begin{cases}
p_0 & \text{si } n = 0 \\
g(p_{n-1}) & \text{si } n \geq 1
\end{cases}
$$

Es claro que la $(p_n)$ es la sucesión de Newton–Raphson, entonces basta probar que $g$ cumple las hipótesis del Teorema de punto fijo de Banach, a saber:

$$
\exists \, \varepsilon > 0 \text{ y } q \in ]0,1[ \text{ tal que } \forall x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \Rightarrow |g'(x)| \leq q < 1,
$$

y que $g$ mapea el intervalo $[\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$ en sí mismo.

Como $f'(\tilde{x}) \ne 0$ y $f'$ es continua, se tiene que existe un $\varepsilon_1 > 0$ tal que $f'(x) \ne 0$ para todo $x \in [\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1] \subset [a,b]$,  
de esta manera $g$ está definida y es continua en $[\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1]$.  
También lo está:

$$
g'(x) = \frac{f(x) f''(x)}{[f'(x)]^2} \quad \text{para todo } x \in [\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1].
$$

Como $f \in C^2[a,b]$, entonces $g \in C^1[\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1]$.  
Como por hipótesis $f(\tilde{x}) = 0$, entonces:

$$
g'(\tilde{x}) = \frac{f(\tilde{x}) f''(\tilde{x})}{[f'(\tilde{x})]^2} = 0.
$$

Luego, como $g'$ es continua, esto implica que para cualquier constante $q < 1$ existe un $\varepsilon$, con $0 < \varepsilon < \varepsilon_1$, tal que:

$$
|g'(x)| \leq q < 1 \quad \text{para todo } x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon].
$$

Solo falta probar que $g : [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \to [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$,  
pero si $x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$, por el Teorema del Valor Medio para algún $\xi$ entre $x$ y $\tilde{x}$ se tiene que:

$$
|g(x) - g(\tilde{x})| = |g'(\xi)||x - \tilde{x}|,
$$

entonces:

$$
|g(x) - \tilde{x}| = |g(x) - g(\tilde{x})| = |g'(\xi)| \cdot |x - \tilde{x}| \leq q \cdot |x - \tilde{x}| < |x - \tilde{x}|.
\tag{1.7}
$$

Como $x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$, entonces $|x - \tilde{x}| < \varepsilon$,  
y luego por (1.7) se tiene que $|g(x) - \tilde{x}| < \varepsilon$,  
lo cual implica que $g(x) \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$,  
con lo que se prueba que $g : [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \to [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$.

$\blacksquare$
:::
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

**Ejemplo 9.** Hallar un método para calcular $\sqrt{A}$, con $A \geq 0$.

**Solución:** Probaremos que la sucesión $(x_n)$ definida por  
$x_{n+1} = \dfrac{1}{2} \left( x_n + \dfrac{A}{x_n} \right)$  
converge a $\sqrt{A}$.  

Nótese que calcular $\sqrt{A}$ es equivalente a resolver la ecuación:

$$
x^2 - A = 0,
$$

usando el método de Newton–Raphson con $f(x) = x^2 - A$ y $f'(x) = 2x$, se tiene que:

\begin{align*}
x_{n+1} &= x_n - \frac{f(x_n)}{f'(x_n)} \\
        &= x_n - \frac{x_n^2 - A}{2x_n} \\
        &= \frac{1}{2} \left( x_n + \frac{A}{x_n} \right).
\end{align*}

De donde es claro que la sucesión $x_n \to \sqrt{A}$ cuando $n \to \infty$. $\blacksquare$
:::

### Método de la Secante

El problema con el método de Newton–Raphson es que requiere la derivada de $f(x)$,  
la cual en muchos casos no se tiene.  

La idea del método de la secante es usar una aproximación para esta derivada,  
como se ilustra en la Figura 1.5.

![El Método de la Secante](Imagenes/MetodoSecante.png){width=400px fig-align="center"}

La deducción del método es la siguiente:

$$
f'(x_{n-1}) = \lim_{x \to x_{n-1}} \frac{f(x) - f(x_{n-1})}{x - x_{n-1}} \approx \frac{f(x_{n-2}) - f(x_{n-1})}{x_{n-2} - x_{n-1}}.
\tag{1.8}
$$

Si en el método de Newton–Raphson $x_n = x_{n-1} - \dfrac{f(x_{n-1})}{f'(x_{n-1})}$ sustituimos $f'(x_{n-1})$ por la aproximación dada en (1.8), se tiene que:

$$
x_n \approx x_{n-1} - \frac{f(x_{n-1})}{\dfrac{f(x_{n-2}) - f(x_{n-1})}{x_{n-2} - x_{n-1}}},
$$

simplificando se obtiene el Método de la Secante:

$$
x_n \approx x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}.
$$

Entonces el método de la secante consiste en calcular la sucesión $(x_n)$ definida por:

$$
x_n =
\begin{cases}
x_0 & \text{si } n = 0 \\
x_1 & \text{si } n = 1 \\
x_{n-1} - \dfrac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})} & \text{si } n \geq 2
\end{cases}
$$

::: {.callout-note}
##### Algoritmo 4 – Método de la Secante

**Entrada:** $N$, $Tol$, $x_0$, $x_1$  
**Salida:** Aproximación de $x$, con $f(x) = 0$, o mensaje de error.

 Paso 1. $i \leftarrow 2$  

  Paso 2. Mientras $i \leq N$, hacer pasos 3–6:

  Paso 3. $x \leftarrow x_1 - \dfrac{(x_0 - x_1)f(x_1)}{f(x_0) - f(x_1)}$

  Paso 4. Si $|x - x_0| < Tol$  

    Salida $(x)$  

   Parar  

  Paso 5. $x_0 \leftarrow x_1$ 

     $x_1 \leftarrow x$  
  Paso 6. $i \leftarrow i + 1$  
  Paso 7. Salida (“Número máximo de iteraciones excedido”)  
 Parar
:::

Una implementación iterativa y otra recursiva en **R** se pueden ver en el archivo `secante.html`.

::: {.callout-tip collapse="true"}
###### Ejemplo

**Ejemplo 10.** Ejecutando el programa iterativo para resolver la ecuación  
$e^{-x} - x = 0$ con $x_0 = 1$ y $x_1 = \dfrac{1}{2}$ como aproximaciones iniciales  
y con una tolerancia de $\varepsilon = 10^{-6}$, se obtienen las siguientes soluciones  
en el archivo `secante.html`.
:::

::: {.callout-note}
##### Teorema

Sea $f$ una función de clase $C^2[a,b]$ con $a < b$.  
Si existe un punto $x$ tal que $f(x) = 0$ y $f'(x) \ne 0$,  
entonces existe un número $\varepsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$,  
la sucesión generada por el método de la secante

$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

está bien definida dentro del intervalo $[x - \varepsilon, x + \varepsilon]$  
y converge a $x$ (cero de la ecuación $f(x) = 0$).
:::

::: {.callout-caution collapse="true"}
##### Prueba

Ejercicio. $\blacksquare$
:::

### Orden de convergencia

::: {.callout-note}
##### Definición

Sea $(x_n)$ una sucesión que converge a $x$, se denota $e_n := x_n - x$ para $n \geq 0$.  
Si existen constantes $\alpha$ y $\lambda$ positivas, tal que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - x|}{|x_n - x|^\alpha} = 
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|^\alpha} = \lambda,
$$

entonces se dice que la sucesión $(x_n)$ converge a $x$ con **orden $\alpha$**  
y con **constante asintótica** $\lambda$.
:::

::: {.callout-important collapse="true"}
###### Observación

- Entre mayor sea el orden de convergencia, o sea entre mayor sea $\alpha$, mayor será la “velocidad” de convergencia.
- Si $\alpha = 1$, se dice que el método tiene **orden lineal**.
- Si $\alpha = 2$, se dice que el método tiene **orden cuadrático**.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Suponga que tenemos dos esquemas (sucesiones) $x_n$ y $\tilde{x}_n$ tal que:

- $\displaystyle \lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} = 0.75$ (método lineal),
- $\displaystyle \lim_{n \to \infty} \frac{|\tilde{e}_{n+1}|}{|\tilde{e}_n|^2} = 0.75$ (método cuadrático),
- Se supone además que $e_0 = 0.5$ y $\tilde{e}_0 = 0.5$.

**¿Cuántas iteraciones requieren $x_n$ y $\tilde{x}_n$ para converger con un error absoluto menor a $10^{-8}$?**

**Solución:**

**1. Analicemos primero la sucesión (esquema) $x_n$:**

\begin{align*}
|x_{n+1} - x| < 10^{-8} &\Leftrightarrow |e_{n+1}| < 10^{-8} \\
\text{Pero } \frac{|e_{n+1}|}{|e_n|} &\approx 0.75 \Rightarrow |e_{n+1}| \approx 0.75 |e_n| \approx 0.75^2 |e_{n-1}| \approx \cdots \\
&\approx 0.75^n |e_0| = 0.75^n \cdot 0.5 \\
\Rightarrow |e_{n+1}| < 10^{-8} &\Leftrightarrow 0.75^n \cdot 0.5 < 10^{-8} \\
\Leftrightarrow \log(0.75^n \cdot 0.5) &< \log(10^{-8}) \\
n \log(0.75) + \log(0.5) &< -8 \\
n &> \frac{-8 - \log(0.5)}{\log(0.75)} \approx 61.62
\end{align*}

Se puede tomar $n = 62$, por lo tanto $x_n$ requiere aproximadamente **62 iteraciones** para converger a $x$.

---

**2. Analicemos ahora la sucesión $\tilde{x}_n$:**

\begin{align*}
|\tilde{x}_{n+1} - x| < 10^{-8} &\Leftrightarrow |\tilde{e}_{n+1}| < 10^{-8} \\
\text{Pero } \frac{|\tilde{e}_{n+1}|}{|\tilde{e}_n|^2} &\approx 0.75 \Rightarrow |\tilde{e}_{n+1}| \approx 0.75 |\tilde{e}_n|^2 \\
&\approx 0.75 \left[ 0.75 |\tilde{e}_{n-1}|^2 \right]^2 = 0.75^3 |\tilde{e}_{n-1}|^4 \\
&\approx 0.75^3 \left[ 0.75 |\tilde{e}_{n-2}|^2 \right]^4 = 0.75^7 |\tilde{e}_{n-2}|^8 \\
&\approx \cdots \approx 0.75^{2^n - 1} |\tilde{e}_0|^{2^n}
\end{align*}
de donde
\begin{align*}
|\tilde{e}_{n+1}| &< 10^{-8} \\
0.75^{2^{n+1}-1} |\tilde{e}_0|^{2^{n+1}} &< 10^{-8} \\
0.75^{2^{n+1}-1} \cdot 0.5^{2^{n+1}} &< 10^{-8} \\
0.75^{-1} \cdot 0.375^{2^{n+1}} &< 10^{-8} \\
2^{n+1} \log(0.375) &< -8 + \log(0.75) \\
2^{n+1} &> \frac{-8 + \log(0.5)}{\log(0.375)} \\
(n + 1) \log(2) &> \log(19.07) \\
n &> \frac{\log(19.07)}{\log(2)} - 1 \approx 3.24
\end{align*}

Luego $n = 4$, por lo que $\tilde{x}_n$ requiere solamente de **4 iteraciones** para converger a $x$, mientras que el esquema lineal requirió aproximadamente **62 iteraciones** para converger a $x$.

$\blacksquare$

:::

::: {.callout-note}
##### Teorema

Sea $x_n = \begin{cases}
x_0 & \text{si } n = 0 \\
g(x_{n-1}) & \text{si } n \geq 1
\end{cases}$ el esquema (la sucesión) de punto fijo visto en la sección 1.1.  

Si además se supone que:

- $g : [a,b] \to [a,b]$
- $g \in C^2[a,b]$
- Existe $q$ tal que $0 \leq q < 1$ y $|g'(x)| \leq q < 1$ para todo $x \in ]a,b[$
- $g'(x) \ne 0$ para el punto fijo de $g$

Entonces $(x_n)$ converge al menos **linealmente a $x$**, cuando $n \to \infty$.
:::

::: {.callout-caution collapse="true"}
##### Prueba

\begin{align*}
|e_{n+1}| &= |x_{n+1} - x| = |g(x_n) - g(x)| = |g'(\xi_n)(x_n - x)| = |g'(\xi_n)||e_n|
\end{align*}

con $\xi_n$ entre $x_n$ y $x$.  
Como $x_n \to x$ cuando $n \to \infty$, y $\xi_n$ está entre $x_n$ y $x$, entonces la sucesión $(\xi_n)$ también converge a $x$ cuando $n \to \infty$.  

Luego:

$$
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} 
= \lim_{n \to \infty} |g'(\xi_n)| 
= |g'\left( \lim_{n \to \infty} \xi_n \right)| = |g'(x)|
$$

Por lo tanto:

$$
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} = g'(x) := \lambda < 1,
$$

de donde la convergencia es lineal.  
$\blacksquare$
:::

::: {.callout-note}
##### Teorema

Sea $\tilde{x}$ un punto fijo de $g$, además $g'(\tilde{x}) = 0$, $g''$ es continua en un intervalo abierto $I$ que contiene a $x$ y $g''(\tilde{x}) \ne 0$.  
Entonces existe $\epsilon > 0$ tal que para $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ la sucesión  
$x_n = \begin{cases}
x_0 & \text{si } n = 0 \\
g(x_{n-1}) & \text{si } n \geq 1
\end{cases}$  
converge al menos **cuadráticamente** a $\tilde{x}$.

::: {.callout-caution collapse="true"}
##### Prueba

Escoja $\epsilon$ tal que el intervalo $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ esté contenido en $I$,  
$|g'(x)| \leq q < 1$ y $g''$ sea continua.  

Como $|g'(x)| \leq q < 1$, se tiene que los términos de la sucesión $(x_n)$ están contenidos en $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.  

Expandiendo $g(x)$ en su polinomio de Taylor para $x \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$, se tiene que:

\begin{align*}
g(x) &= g(\tilde{x}) + g'(\tilde{x})(x - \tilde{x}) + \frac{g''(\xi)}{2}(x - \tilde{x})^2
\end{align*}

con $\xi$ entre $x$ y $\tilde{x}$. Por hipótesis $g(\tilde{x}) = \tilde{x}$ y $g'(\tilde{x}) = 0$, esto implica que:

\begin{align*}
g(x) = \tilde{x} + \frac{g''(\xi)}{2}(x - \tilde{x})^2
\end{align*}

En particular si se toma $x = x_n$, entonces:

\begin{align*}
x_{n+1} = g(x_n) = \tilde{x} + \frac{g''(\xi_n)}{2}(x_n - \tilde{x})^2
\end{align*}

con $\xi_n$ entre $x_n$ y $\tilde{x}$. Luego:

\begin{align}
x_{n+1} - \tilde{x} = \frac{g''(\xi_n)}{2}(x_n - \tilde{x})^2 \tag{1.9}
\end{align}

Como $|g'(x)| \leq q < 1$ y $g$ mapea $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ en sí mismo, es claro que $(x_n)$ converge a $\tilde{x}$ punto fijo de $g$,  
entonces como $\xi_n$ está entre $x_n$ y $\tilde{x}$, la sucesión $(\xi_n)$ converge también a $\tilde{x}$.  
Luego usando (1.9) se deduce que:

\begin{align*}
\lim_{n \to \infty} \frac{|x_{n+1} - \tilde{x}|}{|x_n - \tilde{x}|^2}
= \left| \frac{g''(\xi_n)}{2} \right| = \frac{|g''(\tilde{x})|}{2} = \lambda \ne 0
\end{align*}

Entonces la sucesión $(x_n)$ converge cuadráticamente a $\tilde{x}$.  
$\blacksquare$
:::
:::


::: {.callout-note}
##### Corolario

Sea $f \in C^3[a,b]$. Si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$,  
entonces existe $\epsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que **converge al menos cuadráticamente a $\tilde{x}$** cuando $n \to \infty$,  
para todo $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.
:::

::: {.callout-note}
##### Corolario

Sea $f \in C^3[a,b]$.  
Si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$,  
entonces existe $\epsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que converge al menos cuadráticamente a $\tilde{x}$  
cuando $n \to \infty$, para todo $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.


::: {.callout-caution collapse="true"}
##### Prueba

La convergencia de $(x_n)$ se demostró en el teorema anterior.  
Usando el teorema anterior se concluye el corolario, pues si se toma $g(x) := x - \frac{f(x)}{f'(x)}$  
entonces:

\begin{align*}
g(\tilde{x}) = \tilde{x} - \frac{f(\tilde{x})}{f'(\tilde{x})} = \tilde{x} \quad \text{pues } f(\tilde{x}) = 0 \text{ y } f'(\tilde{x}) \ne 0.
\end{align*}

Además,

\begin{align*}
g'(x) &= 1 - \frac{f'(x)f'(x) - f(x)f''(x)}{(f'(x))^2} = 1 - 1 - \frac{f(x)f''(x)}{[f'(x)]^2} = -\frac{f(x)f''(x)}{[f'(x)]^2}
\end{align*}

de donde $g'(\tilde{x}) = 0$, pues $f(\tilde{x}) = 0$.

Como:

\begin{align*}
g''(x) = \frac{[f'(x)]^2 f''(x) + f(x)f'(x)f^{(3)}(x) - 2f(x)[f''(x)]^2}{[f'(x)]^3}
\end{align*}

entonces:

\begin{align*}
g''(\tilde{x}) = \frac{f''(\tilde{x})}{f'(\tilde{x})} \ne 0.
\end{align*}

Es claro que $g''$ es continua en un intervalo que contiene a $\tilde{x}$ dado que $f \in C^3[a,b]$.  
Entonces $g(x)$ cumple todas las hipótesis del teorema anterior por lo que $(x_n)$ converge cuadráticamente a $\tilde{x}$.  
$\blacksquare$
:::
:::

::: {.callout-note}
##### Teorema

Sea $f$ una función de clase $C^2[a,b]$.  
Si existe un punto $\tilde{x}$ tal que $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$  
entonces existe un número $\epsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \epsilon, x + \epsilon]$  
la sucesión generada por el método de la secante:

$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1})f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

converge a $\tilde{x}$ con orden de al menos:

$$
\frac{1 + \sqrt{5}}{2} \approx 1.618.
$$


::: {.callout-caution collapse="true"}
##### Prueba

(Ejercicio)  
Sugerencias: Pruebe que

\begin{align}
|e_{n+1}| \approx C \cdot |e_n| \cdot |e_{n-1}| \tag{1.10}
\end{align}

con $C := \left| \frac{f''(\tilde{x})}{2f'(\tilde{x})} \right|$.  
Por la definición 1 se busca una solución aproximada a la ecuación:

\begin{align}
|e_n| = \lambda |e_{n-1}|^\alpha \tag{1.11}
\end{align}

con $\lambda > 0$ y $\alpha \geq 1$.  
Pero considerando (1.10) como una ecuación y sustituyendo en (1.11), se tiene que:

\begin{align}
\lambda |e_n|^\alpha = \lambda \lambda^\alpha |e_{n-1}|^{\alpha^2} = C \lambda |e_{n-1}|^{\alpha + 1} \tag{1.12}
\end{align}

La ecuación (1.12) es válida solamente para $n$ suficientemente grandes si:

\begin{align*}
\lambda^\alpha = C \quad \text{y} \quad \alpha^2 = \alpha + 1
\end{align*}

de donde:

\begin{align*}
\alpha = \frac{1 + \sqrt{5}}{2}, \quad \text{y} \quad \lambda = C^{1/\alpha}.
\end{align*}

$\blacksquare$
:::
:::
## Aceleración de la convergencia y método de Steﬀensen.


### Método $\triangle^2$ de Aitken

El método $\triangle^2$ de *Aitken* tiene las siguientes hipótesis:

1. $(x_n)$ converge linealmente a $x$ con constante asintótica $\lambda$ tal que $0 < \lambda < 1$.

2. Los signos de $x_n - x$, $x_{n+1} - x$ y $x_{n+2} - x$ son iguales.

3. Se asume que para $n$ suficientemente grande:

$$
\frac{x_{n+1} - x}{x_n - x} \approx \frac{x_{n+2} - x}{x_{n+1} - x}.
$$

La idea es encontrar una sucesión $(\tilde{x}_n)$ la cual converge “más rápidamente” a $x$ que la sucesión $(x_n)$. Note que de la expresión anterior:

\begin{align*}
&\Rightarrow (x_{n+1} - x)(x_n - x) \approx (x_n - x)(x_{n+2} - x) \\
&\Rightarrow x_{n+1}^2 - 2x_nx_{n+1} + x^2 \approx x_nx_{n+2} - xx_{n+2} - xx_n + x^2 \\
&\Rightarrow -2x_nx_{n+1} + xx_{n+1} + xx_n \approx -x_{n+1}^2 + x_nx_{n+2} \\
&\Rightarrow (-2x_nx_{n+1} + x_nx_{n+2} + x_nx_{n+1})x \approx -x_{n+1}^2 + x_nx_{n+2} \\
&\Rightarrow x \approx \frac{-x_{n+1}^2 + x_nx_{n+2}}{(-2x_nx_{n+1} + x_nx_{n+2} + x_n)} \\
&\Rightarrow x \approx \frac{x_nx_{n+2} - x_{n+1}^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx \frac{x_n^2 + x_nx_{n+2} - 2x_nx_{n+1} - x_{n+1}^2 + 2x_nx_{n+1} - x_{n+1}^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx \frac{x_n(x_n + x_{n+2} - 2x_{n+1}) - (x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n}
\end{align*}

El método de $\triangle^2$ de Aitken se basa en el hecho de que la sucesión $(\tilde{x}_n)$ definida por:

$$
\tilde{x}_{n+3} := x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n},
$$

bajo ciertas condiciones, converge “más rápidamente” a $x$ que la sucesión $(x_n)$.


::: {.callout-note}
###### Notación: Diferencia Progresiva

- $\triangle(x_n) := x_{n+1} - x_n$ para $n \ge 0$.
- $\triangle^k(x_n) := \triangle^{k-1}(\triangle x_n)$ para $k \ge 2$.
:::
::: {.callout-important collapse="true"}
###### Observación

Nótese que con esta notación se tiene:

\begin{align*}
\triangle^2(x_n) &= \triangle(\triangle x_n) \\
              &= \triangle x_{n+1} - \triangle x_n \\
              &= x_{n+2} - x_{n+1} - (x_{n+1} - x_n) \\
              &= x_{n+2} - 2x_{n+1} + x_n.
\end{align*}

Por lo que el método $\triangle^2$ de Aitken se puede escribir como:

\begin{equation}
\tilde{x}_{n+3} := x_n - \frac{(\triangle x_n)^2}{\triangle^2 x_n}\tag{1.14}
\end{equation}
:::

::: {.callout-note}
##### Definición

Sean $(x_n)$ y $(\tilde{x}_n)$ dos sucesiones que convergen a $x$, se dice que la sucesión $(\tilde{x}_n)$ **converge más rápido a** $x$ que la sucesión $(x_n)$ si:

$$
\lim_{n \to \infty} \frac{\tilde{x}_n - x}{x_n - x} = 0.
$$
:::

::: {.callout-note}
##### Teorema

Sea $(x_n)$ una sucesión que converge a $x$ con orden lineal con constante asintótica $\lambda < 1$, además se asume que $e_n = x_n - x \ne 0$ para todo $n$. Entonces la sucesión $(\tilde{x}_n)$, definida como en (1.14), converge a $x$ más rápido que $(x_n)$.

::: {.callout-caution collapse="true"}
##### Prueba

Ejercicio. $\blacksquare$
:::
:::

### El método de Steffensen

Por el Teorema 8 la sucesión de aproximaciones sucesivas $(x_n)$ converge linealmente a $x$, cuando $n \to \infty$ con constante asintótica $\lambda < 1$, entonces tiene sentido aplicar el método $\triangle^2$ de Aitken a esta sucesión. Es así como una combinación entre el método de aproximaciones sucesivas y el método $\triangle^2$ de Aitken produce un método conocido como el **Método de Steffensen**, el cual consiste en calcular la sucesión:

$$
x_{n+1} := x_n - \frac{[g(x_n) - x_n]^2}{g(g(x_n)) - 2g(x_n) + x_n}.
$$

Algorítmicamente, el Método de Steffensen para acelerar el método de punto fijo se puede expresar de la siguiente manera. Sea $x_0$ la aproximación inicial en el método de punto fijo, entonces se toma:

\begin{align*}
x_0^{(0)} &= x_0 \\
x_1^{(0)} &= g\left(x_0^{(0)}\right) \\
x_2^{(0)} &= g\left(x_1^{(0)}\right) \\
x_0^{(1)} &= \Delta^2\left(x_0^{(0)}\right) = x_0^{(0)} - \frac{\left(x_1^{(0)} - x_0^{(0)}\right)^2}{x_2^{(0)} - 2x_1^{(0)} + x_0^{(0)}} \\
x_1^{(1)} &= g\left(x_0^{(1)}\right) \\
x_2^{(1)} &= g\left(x_1^{(1)}\right) \\
x_0^{(2)} &= \Delta^2\left(x_0^{(1)}\right) = x_0^{(1)} - \frac{\left(x_1^{(1)} - x_0^{(1)}\right)^2}{x_2^{(1)} - 2x_1^{(1)} + x_0^{(1)}} \\
\vdots &
\end{align*}


::: {.callout-note}
##### Algoritmo [Método de Steffensen]

**Entrada**: $N$, $Tol$, $x_0$, $g$  
**Salida**: Aproximación de $x$ o mensaje de error

\begin{align*}
&\text{Paso 1. } i \gets 2 \\
&\text{Paso 2. Mientras } i \leq N, \text{ siga los pasos 3–6} \\
&\quad \text{Paso 3. } x_1 = g(x_0), \quad x_2 = g(x_1) \\
&\qquad x = x_0 - \frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} \\
&\quad \text{Paso 4. Si } |x - x_0| < Tol \\
&\qquad \text{Salida } (x) \\
&\qquad \text{Parar} \\
&\quad \text{Paso 5. } i \gets i + 1 \\
&\quad \text{Paso 6. } x_0 \gets x \\
&\text{Paso 7. Salida (``Número máximo de iteraciones excedido'')} \\
&\text{Parar}
\end{align*}

Este algoritmo se puede programar iterativamente y recursivamente en **R**, como se muestra en el archivo `steffensen.html`.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Resolver la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1, 2]$ con $\epsilon = 10^{-5}$.

**Solución**: Como se mostró en el ejemplo 3, utilizando $g(x) = \sqrt{1 + \frac{1}{x}}$, el método de punto fijo requirió de 9 iteraciones, con $x_0 = 2$, para resolver esta ecuación, con una tolerancia de $\epsilon = 10^{-5}$, mientras que el Método de Steffensen requiere solamente de 3 iteraciones, como se muestra en el archivo `steffensen.html`. $\blacksquare$
:::
::: {.callout-note}
##### Teorema

Si el método de punto fijo $x_{n+1} = g(x_n)$ converge linealmente, entonces el orden de convergencia del método de Steffensen es al menos dos.

::: {.callout-caution collapse="true"}
##### Prueba

(Ejercicio) Suponga que $g(x)$ es un número suficientemente de veces derivable, luego pruebe que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - \widetilde{x}|}{|x_n - \widetilde{x}|^2} 
= \frac{1}{2} \left| \frac{g'(\widetilde{x})g''(\widetilde{x})}{g'(\widetilde{x}) - 1} \right| := \lambda \neq 0.
$$

$\blacksquare$
:::
:::

# Interpolación

## Interpolación y aproximaciones polinómicas

### Polinomios de Bernstein

::: {.callout-note}
##### Teorema: [Weierstrass]

Sea $f$ continua en $[a, b]$; entonces dado $\varepsilon > 0$, existe $n \in \mathbb{N}$ y $P_n(x) \in P_n$ tal que

$$
\left| f(x) - P_n(x) \right| < \varepsilon, \quad \forall x \in [a, b].
$$


![Polinomio de Bernstein](Imagenes/Polinomio de Bernstein.png){width=400px fig-align="center"}

::: {.callout-caution collapse="true"}
##### Prueba

Sin pérdida de generalidad, suponga que $a = 0$ y $b = 1$. Sea:


$$
B_n(x) = \sum_{k=0}^n \binom{n}{k} x^k (1 - x)^{n-k} f\left( \frac{k}{n} \right),
$$

se puede probar que $B_n(x) \to f(x)$ uniformemente en $[0,1]$ (ejercicio). $\blacksquare$
:::
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Sea $f(x) = e^x$ en $[0, 1]$, entonces:

\begin{align*}
B_2(x) 
&= \sum_{k=0}^2 \binom{2}{k} x^k (1 - x)^{2-k} e^{\frac{k}{2}} \\
&= \binom{2}{0} x^0 (1 - x)^2 e^0 
   + \binom{2}{1} x^1 (1 - x)^1 e^{\frac{1}{2}} 
   + \binom{2}{2} x^2 (1 - x)^0 e^1 \\
&= (1 - x)^2 + 2x(1 - x) e^{\frac{1}{2}} + x^2 e.
\end{align*}

:::

```{r}
#| code-fold: true

# Polinomio de Bernstein en [0,1]
bernstein <- function(n, F, x) {
  resultado <- numeric(length(x))
  for (i in seq_along(x)) {
    xi <- x[i]
    suma <- 0
    for (k in 0:n) {
      coef <- choose(n, k) * (xi^k) * ((1 - xi)^(n - k))
      suma <- suma + coef * F(k / n)
    }
    resultado[i] <- suma
  }
  return(resultado)
}

# Ejemplo: F(x) = exp(x) en [0,1]
F <- function(x) exp(x)
xs <- seq(0, 1, length.out = 400)

# Aproximación con n = 10
vals <- bernstein(10, F, xs)

# Graficar
plot(xs, F(xs), type = "l", col = "red", lwd = 2,
     main = "Aproximación de Bernstein de exp(x) en [0,1]",
     ylab = "f(x)", xlab = "x")
lines(xs, vals, col = "blue", lwd = 2)
legend("topleft", legend = c("exp(x)", "Bernstein n=10"),
       col = c("red", "blue"), lwd = 2)
```

**Observación.** El polinomio de Bernstein tiene solo valor teórico y no práctico.

### Existencia y unicidad del polinomio de interpolación

::: {.callout-note}
##### Problema

Sean $(x_i, y_i)$, para $i = 0, 1, 2, \ldots, n$, una secuencia de $n + 1$ puntos (nodos). Se busca un polinomio de grado $n$:

$$
P_n(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n,
$$

tal que satisfaga las condiciones de interpolación:

$$
P_n(x_i) = y_i \qquad i = 0, 1, 2, \ldots, n.
$$
:::

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 7

# Paquetes necesarios
library(ggplot2)

# Datos de los nodos
x <- c(-2, -1, 0, 1, 2)
y <- c(-1.2, 0.5, 1, 2.8, 3.5)

# Ajuste del polinomio interpolante de grado 4
modelo <- lm(y ~ poly(x, 4, raw = TRUE))

# Datos para graficar el polinomio
x_vals <- seq(min(x) - 0.5, max(x) + 0.5, length.out = 500)
y_vals <- predict(modelo, newdata = data.frame(x = x_vals))

# Crear un data frame con los nodos para etiquetas
df_nodos <- data.frame(x = x, y = y,
                       etiqueta = paste0("(", x, ", ", y, ")"))

# Gráfico
ggplot() +
  geom_line(aes(x = x_vals, y = y_vals), color = "darkred", linewidth = 1.3) +
  geom_point(data = df_nodos, aes(x = x, y = y), size = 3, shape = 21, fill = "white") +
  geom_text(data = df_nodos, aes(x = x, y = y, label = etiqueta),
            vjust = -1.2, size = 3.5) +
  labs(
    title = "Interpolación polinómica con nodos: (-2,-1.2), (-1,0.5), (0,1), (1,2.8), (2,3.5)",
    subtitle = "Polinomio P[n](x) tal que  P_n(xᵢ) = yᵢ",
    x = "x",
    y = "y"
  ) +
  theme_minimal(base_size = 14)
```

::: {.callout-note}
##### Teorema

Sean $(x_i, y_i)$ una secuencia de $n + 1$ puntos, con $i = 0, 1, 2, \ldots, n$, tal que $x_i \ne x_j$, $\forall i \ne j$, entonces existe un único polinomio (de interpolación) que satisface la condición de interpolación:

$$
P_n(x_i) = y_i, \qquad i = 0, 1, \ldots, n,
$$

el cual tiene a lo más grado $n$.

::: {.callout-caution collapse="true"}
##### Prueba

**Existencia:**

Sea

$$
L_i(x) := \prod_{\substack{j = 0 \\ j \ne i}}^n \frac{x - x_j}{x_i - x_j}
= \frac{(x - x_0)\cdots(x - x_{i-1})(x - x_{i+1})\cdots(x - x_n)}
{(x_i - x_0)\cdots(x_i - x_{i-1})(x_i - x_{i+1})\cdots(x_i - x_n)}.
$$

Note que:

$$
L_i(x_k) =
\begin{cases}
1 & \text{si } i = k \\
0 & \text{si } i \ne k
\end{cases}
= \delta_{ik}.
$$

Entonces el polinomio

$$
P_n(x) := \sum_{i = 0}^n y_i L_i(x),
$$

tiene la propiedad de interpolación, pues:

$$
P_n(x_k) = \sum_{i = 0}^n y_i L_i(x_k) = \sum_{i = 0}^n y_i \delta_{ik} = y_k, \qquad \text{para } k = 0, 1, \ldots, n.
$$

Además, el grado de $P_n(x)$ es menor o igual a $n$, pues es combinación lineal de polinomios de grado $n$.

**Unicidad:**

Sean $P_n(x)$ y $Q_n(x)$ dos polinomios de grado $n$ que satisfacen las condiciones de interpolación,

$$
P_n(x_k) = Q_n(x_k) = y_k, \qquad k = 0, 1, \ldots, n.
$$

Sea $D(x) := P_n(x) - Q_n(x)$, note que $D(x)$ es un polinomio de grado a lo más $n$ y tiene $n + 1$ raíces $x_0, x_1, \ldots, x_n$, por lo que de acuerdo al Teorema Fundamental del Álgebra se tiene que:

$$
D(x) \equiv 0 \Rightarrow P_n(x) - Q_n(x) = 0 \Rightarrow P_n(x) = Q_n(x). \quad \blacksquare
$$
:::
:::

### Interpolación de Lagrange


::: {.callout-note}
##### Definición: Polinomio de interpolación de Lagrange

El polinomio:

$$
P_n(x) = \sum_{i=0}^n y_i L_i(x),
$$

con $y_i = f(x_i)$ y $L_i(x)$ definido por:

$$
L_i(x) = \frac{(x - x_0)(x - x_1)\cdots(x - x_{i-1})(x - x_{i+1})\cdots(x - x_n)}
{(x_i - x_0)(x_i - x_1)\cdots(x_i - x_{i-1})(x_i - x_{i+1})\cdots(x_i - x_n)},
$$

se llama el **polinomio de interpolación de Lagrange**.
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Sea $f(x) = e^x$, $0 \leq x \leq 2$. Calcule $P_2(x)$ con $x_0 = 0$, $x_1 = 1$, y $x_2 = 2$.

**Solución:**

\begin{align*}
P_2(x) &= \frac{(x - 1)(x - 2)}{(0 - 1)(0 - 2)} e^0 + \frac{(x - 0)(x - 2)}{(1 - 0)(1 - 2)} e^1 + \frac{(x - 1)(x - 0)}{(2 - 0)(2 - 1)} e^2 \\
&= \frac{(x - 1)(x - 2)}{2} - x(x - 2)e + \frac{(x - 1)x}{2}e^2.
\end{align*}

$\blacksquare$
:::

![Polinomio de interpolación de Lagrange](Imagenes/Polinomio de interpolación de Lagrange.png){width=400px fig-align="center"}

#### Estudio del error

Recordemos el Teorema de Rolle Generalizado:

::: {.callout-note}
##### Teorema

Sea $f \in C[a, b]$ y $f \in C^n[a, b]$, si $f$ se anula en $n + 1$ puntos distintos $x_0, x_1, \ldots, x_n$ en $[a, b]$, entonces $\exists\, c \in ]a, b[$ tal que $f^{(n)}(c) = 0$.

::: {.callout-caution collapse="true"}
##### Prueba

Se omite. $\blacksquare$
:::
:::

::: {.callout-note}
##### Teorema: Error en el método de Lagrange

Sean $x_0, x_1, \ldots, x_n \in [a, b]$ y sea $f \in C^{n+1}[a, b]$. Entonces $\forall\, x \in [a, b]$, $\exists\, \xi_x \in ]a, b[$ tal que:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x - x_0)(x - x_1) \cdots (x - x_n).
$$

Es decir, el error absoluto es:

$$
\left|f(x) - P_n(x)\right| = \left| \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x - x_0)(x - x_1) \cdots (x - x_n) \right|.
$$

::: {.callout-caution collapse="true"}
##### Prueba

**I caso**  
Si $x = x_k$, entonces $f(x_k) = P_n(x_k)$ y el error es cero, por lo tanto cualquier $\xi_x$ funciona.

**II caso**  
Si $x \ne x_k$, se define:

$$
g(t) = f(t) - P_n(t) - \left[ f(x) - P_n(x) \right] \frac{(t - x_0)(t - x_1)\cdots(t - x_n)}{(x - x_0)(x - x_1)\cdots(x - x_n)}.
$$

Vamos a probar que $g(t)$ cumple las hipótesis del Teorema Generalizado de Rolle para $n + 1$.

- $g$ es $n + 1$ veces derivable pues $f \in C^{n+1}[a, b]$ y $P \in C^\infty[a, b]$.
- $g$ se anula en $n + 2$ puntos, a saber: $t = x_0, x_1, \ldots, x_n$ y $t = x$.


Como $g$ cumple las hipótesis del Teorema Generalizado del Rolle, entonces:

$$
\exists \, \xi_x \in ]a, b[ \text{ tal que } g^{(n+1)}(\xi_x) = 0.
$$

Vamos a calcular $g^{(n+1)}(t)$:

\begin{align*}
\frac{d^{n+1}}{dt^{n+1}} g(t) 
&= f^{(n+1)}(t) - 0 - [f(x) - P_n(x)] \cdot \frac{d^{n+1}}{dt^{n+1}} \prod_{i=0}^n \frac{t - x_i}{x - x_i} \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{1}{\prod_{i=0}^n (x - x_i)} \cdot \frac{d^{n+1}}{dt^{n+1}} \prod_{i=0}^n (t - x_i) \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{1}{\prod_{i=0}^n (x - x_i)} \cdot \frac{d^{n+1}}{dt^{n+1}} \left( t^{n+1} + \text{términos de grado } \leq n \right) \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{(n+1)!}{\prod_{i=0}^n (x - x_i)}.
\end{align*}

De donde se concluye que:

$$
g^{(n+1)}(\xi_x) = f^{(n+1)}(\xi_x) - [f(x) - P_n(x)] \cdot \frac{(n+1)!}{\prod_{i=0}^n (x - x_i)} = 0,
$$

por lo tanto:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!} \prod_{i=0}^n (x - x_i).
$$

$\blacksquare$
:::
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Sea $f(x) = e^x$, con $x \in [0, 2]$ y sean $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, tenemos que:

$$
P_2(x) = \frac{(x - 1)(x - 2)}{2} - x(x - 2)e + \frac{(x - 1)x}{2}e^2,
$$

así el error absoluto de aproximar $f(0.25)$ por $P_2(0.25)$ es:

$$
\text{Error Absoluto} = |f(0.25) - P_2(0.25)| = 0.1312511.
$$

Mientras que el error teórico es:

$$
\text{Error Teórico} = \left| \frac{e^{\xi_x}}{3!}(x - 0)(x - 1)(x - 2) \right| 
\leq \left| \frac{e^2}{3!}x(x - 1)(x - 2) \right|,
$$

luego con $x = 0.25$:

$$
\text{Error Teórico} = \left| \frac{e^2}{3!}(0.25)(0.25 - 1)(0.25 - 2) \right| = 0.404089.
$$

$\blacksquare$
:::

::: {.callout-note}
##### Método 1: Calcula el Polinomio de Lagrange retornando una función para ser evaluada en un $x$.
```{r}
#| code-fold: true

PLagrange <- function(nodos, f, tol = 1e-12) {
  n  <- length(nodos)
  yi <- vapply(nodos, f, numeric(1))
  #  w_i = 1 / ∏_{j≠i} (x_i - x_j)
  pesos <- numeric(n)
  for (i in seq_len(n)) {
    pesos[i] <- 1 / prod(nodos[i] - nodos[-i])
  }
  # Función que evalúa el polinomio de interpolación
  polinomio <- function(x) {
    x <- as.numeric(x)
    salida <- numeric(length(x))
    for (k in seq_along(x)) {
      diferencias <- abs(x[k] - nodos)
      j <- which.min(diferencias)
      if (diferencias[j] < tol) {
        # Caso: x coincide con un nodo
        salida[k] <- yi[j]
      } else {
        denominador <- sum(pesos / (x[k] - nodos))
        numerador   <- sum((pesos * yi) / (x[k] - nodos))
        salida[k]   <- numerador / denominador
      }
    }
    salida
  }
  return(polinomio)
}
```
:::
::: {.callout-note}
##### Método 2: Calcula los coeficientes del Polinomio de Lagrange
Salida: $c(a_1, a_2,\ldots, a_{n-1})$ donde $$P_n(x) = \sum_{k=0}^{n-1}a_kx^k$$
```{r}
#| code-fold: true
PLagrangeCoeficientes <- function(nodos, f) {
  n     <- length(nodos)
  valores <- vapply(nodos, f, numeric(1))
  # Multiplicación de polinomios: coeficientes en orden ascendente
  multiplicar_polinomios <- function(a, b) {
    resultado <- numeric(length(a) + length(b) - 1)
    for (i in seq_along(a)) {
      for (j in seq_along(b)) {
        resultado[i + j - 1] <- resultado[i + j - 1] + a[i] * b[j]
      }
    }
    resultado
  }
  # ∏ (x - r_j), devuelve vector de coeficientes
  polinomio_desde_raices <- function(raices) {
    polinomio <- 1
    for (r in raices) {
      polinomio <- multiplicar_polinomios(polinomio, c(-r, 1))
    }
    polinomio
  }
  coeficientes <- numeric(n)  # grado ≤ n-1 → longitud n
  for (i in seq_len(n)) {
    raices_i     <- nodos[-i]
    numerador    <- polinomio_desde_raices(raices_i)   # longitud n
    denominador  <- prod(nodos[i] - raices_i)
    coeficientes <- coeficientes + valores[i] * (numerador / denominador)
  }
  return(coeficientes)
}
```

:::

::: {.callout-tip collapse="true"}
###### Ejemplo 1
1) Polinomio de Lagrange
```{r}
#| code-fold: true
## Nodos y función
nodos <- c(0, 1, 2)
f     <- function(x) exp(x)

## 1) Polinomio de Lagrange
P <- PLagrange(nodos, f)

## Evalua en algunos puntos
x_vals <- seq(0, 2, length.out = 5)
data.frame(x = x_vals,
           exp_x = exp(x_vals),
           P_x   = P(x_vals))
```
2) Obtener coeficientes del polinomio de Lagrange
```{r}
#| echo: false
## 2) Obtener coeficientes del polinomio de Lagrange
coefs <- PLagrangeCoeficientes(nodos, f)
coefs
```
3) Graficar
```{r}
#| echo: false

# --- Preparar datos para graficar ---
xi <- seq(-1, 3, length.out = 400)
df <- data.frame(
  x   = xi,
  expx = exp(xi),
  Lx   = P(xi)
)

# Nodos para marcar los puntos de interpolación
df_nodes <- data.frame(x = nodos, y = f(nodos))

# --- Gráfico ---
ggplot(df, aes(x = x)) +
  geom_line(aes(y = expx, color = "exp(x)"), linewidth = 1) +
  geom_line(aes(y = Lx,   color = "L(x)"),  linewidth = 1, linetype = "dashed") +
  geom_point(data = df_nodes, aes(x = x, y = y), shape = 21, size = 3, fill = "white") +
  scale_color_manual(values = c("exp(x)" = "blue", "L(x)" = "red")) +
  labs(title = "Interpolación de Lagrange",
       subtitle = "Nodos {0,1,2}, f(x) = exp(x)",
       y = "Valor", color = "Función") +
  theme_minimal(base_size = 14)
```
:::
::: {.callout-tip collapse="true"}
###### Ejemplo 2
1) Polinomio de Lagrange
```{r}
# --- Nodos y función ---
nodos <- c(0, 3, 6, 9, 12, 15)
f     <- function(x) exp(x)

# Polinomio de Lagrange
P <- PLagrange(nodos, f)

# --- Datos para graficar ---
xi <- seq(0, 15, length.out = 600)   # intervalo que cubre todos los nodos
df <- data.frame(
  x    = xi,
  expx = exp(xi),
  Lx   = P(xi)
)
df_nodes <- data.frame(x = nodos, y = f(nodos))
```
2) Obtener coeficientes del polinomio de Lagrange
```{r}
#| code-fold: true
## 2) Obtener coeficientes del polinomio de Lagrange
coefs <- PLagrangeCoeficientes(nodos, f)
coefs
```
3) Grafico
```{r}
# --- Gráfico ---

ggplot(df, aes(x = x)) +
  geom_line(aes(y = expx, color = "exp(x)"), linewidth = 1) +
  geom_line(aes(y = Lx,   color = "L(x)"),  linewidth = 1, linetype = "dashed") +
  geom_point(data = df_nodes, aes(x = x, y = y), shape = 21, size = 3, fill = "white") +
  scale_color_manual(values = c("exp(x)" = "blue", "L(x)" = "red")) +
  labs(title = "Interpolación de Lagrange",
       subtitle = "Nodos {0, 3, 6, 9, 12, 15},  f(x) = exp(x)",
       y = "Valor", color = "Función") +
  theme_minimal(base_size = 14)
```
:::

#### Interpolación iterada

::: {.callout-note}
##### Definición: Polinomio de Lagrange en subconjuntos de nodos

Sea $f$ una función definida en $x_0, x_1, \ldots, x_n$ y sean $0 \le m_i \le n$, para $i = 1, 2, \ldots, k$, entonces el polinomio de Lagrange de grado $\le (k - 1)$ que coincide con $f$ en $x_{m_1}, x_{m_2}, \ldots, x_{m_k}$ se denota por:
$$
P_{m_1, m_2, \ldots, m_k}(x).
$$
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Sea $f(x) = x^3$, $x_0 = 1$, $x_1 = 2$, $x_2 = 3$, $x_3 = 4$, $x_4 = 6$, calcule $P_{0,3,4}(x)$.

**Solución:** $P_{0,3,4}(x)$ es el polinomio que coincide con $f$ en $x_0 =  = 1$, $x_3 = 4$, $x_4 = 6$, de donde:
\begin{align*}
P_{0,3,4}(x) &= \frac{(x - 4)(x - 6)}{(1 - 4)(1 - 6)} 1^3 + \frac{(x - 1)(x - 6)}{(4 - 1)(4 - 6)} 4^3 + \frac{(x - 1)(x - 4)}{(6 - 1)(6 - 4)} 6^3 \\
&= 11x^2 - 34x + 24.
\end{align*}
$\blacksquare$
:::
::: {.callout-tip collapse="true"}
###### Ejemplo

Calcule $P_{1,2,4}(x)$:

**Solución:**

\begin{align*}
P_{1,2,4}(x) &= \frac{(x - 3)(x - 6)}{(2 - 4)(2 - 6)} 2^3 
+ \frac{(x - 2)(x - 6)}{(3 - 2)(3 - 6)} 3^3 
+ \frac{(x - 2)(x - 3)}{(6 - 2)(6 - 3)} 6^3 \\
&= 10x^2 - 27x + 18.
\end{align*}
$\blacksquare$

:::

Los siguientes párrafos se dedican a encontrar un método para calcular los polinomios de Lagrange en forma recursiva.

::: {.callout-note}
##### Teorema

Sea $f$ una función definida en $x_0, x_1, \ldots, x_k$ y sea $x_i \ne x_j$ con $i,j \in \{0, 1, 2, \ldots, k\}$. Entonces el polinomio de Lagrange que coincide con $f$ en $x_0, x_1, \ldots, x_k$ se puede escribir como:

\begin{align*}
P(x) = \frac{(x - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x) - (x - x_i) P_{0,1,\ldots,(i-1),(i+1),\ldots,k}(x)}{x_i - x_j}.
\end{align*}

::: {.callout-caution collapse="true"}
##### Prueba

Hay que probar que $P(x_s) = f(x_s)$, $\ \forall \ s = 0, 1, 2, \ldots, k$.

**Primer caso:**  
Sea $x_r \ne x_i$ y $x_r \ne x_j$ un nodo:

\begin{align*}
P(x_r) &= \frac{(x_r - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x_r) - (x_r - x_i) P_{0,1,\ldots,(i-1),(i+1),\ldots,k}(x_r)}{x_i - x_j} \\
&= \frac{(x_r - x_j) f(x_r) - (x_r - x_i) f(x_r)}{x_i - x_j} \\
&= \frac{(-x_j + x_i) f(x_r)}{x_i - x_j} \\
&= f(x_r).
\end{align*}

**Segundo caso:**  
Sea $x_r = x_i$:

\begin{align*}
P(x_r) &= \frac{(x_i - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x_i) - 0}{x_i - x_j} \\
&= \frac{(x_i - x_j)}{(x_i - x_j)} f(x_i) \\
&= f(x_i).
\end{align*}

Es análogo si $x_r = x_j$, por lo tanto $P(x)$ es el polinomio de Lagrange que coincide con $f$ en $x_0, x_1, \ldots, x_k$ pues este es único. $\blacksquare$
:::
:::

#### Método de Neville

Se desea aproximar $f(x^*)$ dada la siguiente tabla de valores para $f$:

\begin{align*}
\begin{array}{c|c}
x & f(x) \\
\hline
x_0 & f(x_0) \\
x_1 & f(x_1) \\
\vdots & \vdots \\
x_n & f(x_n)
\end{array}
\end{align*}

Se genera la tabla de $f(x^*)$:

$$
\begin{array}{ccccccc}
x_0 & P_0 \\
x_1 & P_1 & P_{0,1} \\
x_2 & P_2 & P_{1,2} & P_{0,1,2} \\
x_3 & P_3 & P_{2,3} & P_{1,2,3} & P_{0,1,2,3} \\
x_4 & P_4 & P_{3,4} & P_{2,3,4} & P_{1,2,3,4} & P_{0,1,2,3,4} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & P_n & P_{n-1,n} & P_{n-2,n-1,n} & P_{n-3,n-2,n-1,n} & \cdots & P_{0,1,\ldots,n}
\end{array}
$$

Con $P_i(x) = f(x_i)$ una función constante, polinomio de Lagrange de grado 0.  
Esta tabla puede ser calculada usando el Teorema anterior, veamos algunos ejemplos:


\begin{align*}
P_{0,1}(x) &= \frac{(x - x_0)P_1 - (x - x_1)P_0}{x_1 - x_0} \\
P_{1,2}(x) &= \frac{(x - x_1)P_2 - (x - x_2)P_1}{x_2 - x_1} \\
&\vdots \\
P_{n-1,n}(x) &= \frac{(x - x_{n-1})P_n - (x - x_n)P_{n-1}}{x_n - x_{n-1}} \\
P_{0,1,2}(x) &= \frac{(x - x_0)P_{1,2} - (x - x_2)P_{0,1}}{x_2 - x_0} \\
P_{1,2,3}(x) &= \frac{(x - x_1)P_{2,3} - (x - x_3)P_{1,2}}{x_3 - x_1} \\
&\vdots \\
P_{n-2,n-1,n}(x) &= \frac{(x - x_{n-2})P_{n-1,n} - (x - x_n)P_{n-2,n-1}}{x_n - x_{n-2}} \\
P_{0,1,2,3}(x) &= \frac{(x - x_0)P_{1,2,3} - (x - x_3)P_{0,1,2}}{x_3 - x_0} \\
P_{1,2,3,4}(x) &= \frac{(x - x_1)P_{2,3,4} - (x - x_4)P_{1,2,3}}{x_4 - x_1} \\
&\vdots
\end{align*}

::: {.callout-tip collapse="true"}
###### Ejemplo

Aproxime $f(2.5)$ dada la siguiente tabla:

$$
\begin{array}{c|c}
x & f(x) \\
\hline
x_0 = 2.0 & 0.5103757 \\
x_1 = 2.2 & 0.5207843 \\
x_2 = 2.4 & 0.5104147 \\
x_3 = 2.6 & 0.4813306 \\
x_4 = 2.8 & 0.4359160
\end{array}
$$

**Solución:**

Construimos la tabla de Neville:

\begin{align*}
x_0 &: P_0 \\
x_1 &: P_1 \quad P_{0,1} \\
x_2 &: P_2 \quad P_{1,2} \quad P_{0,1,2} \quad f(2.5) \\
x_3 &: P_3 \quad P_{2,3} \quad P_{1,2,3} \quad P_{0,1,2,3} \\
x_4 &: P_4 \quad P_{3,4} \quad P_{2,3,4} \quad P_{1,2,3,4} \quad P_{0,1,2,3,4}
\end{align*}

La tabla de Neville es:

$$
\begin{array}{cccccc}
x_0 & 0.5103757 \\
x_1 & 0.5207843 & \boxed{0.5363972} & \hookleftarrow P_{0,1}\\
x_2 & 0.5104147 & 0.5052299 & 0.4974380 \\
x_3 & 0.4813306 & 0.4958726 & 0.4982119 & 0.4980829 \\
x_4 & 0.4359160 & 0.5040379 & 0.4979139 & 0.4980629 & 0.49807047 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
$$

De donde $f(2.5) \approx 0.49807047$. Un ejemplo del cálculo en la matriz anterior es:

\begin{align*}
P_{0,1}(x) 
&= \frac{(x - x_0)P_1 - (x - x_1)P_0}{x_1 - x_0} \\
&= \frac{(2.5 - 2.0) \cdot 0.5207843 - (2.5 - 2.2) \cdot 0.5103757}{2.2 - 2.0} \\
&= 0.5363972.
\end{align*}

$\blacksquare$

:::


::: {.callout-note}
###### Notación

Se denota por $Q_{ij}$ el polinomio interpolante de Lagrange de grado $j$ que pasa por los $j + 1$ nodos siguientes:

$$
x_{i-j},\ x_{i-j+1},\ \ldots,\ x_{i-1},\ x_i
$$

es decir,

$$
Q_{ij} = P_{i-j,i-j+1,\ldots,i-1,i}(x).
$$

Ahora, usando el método de Neville (teorema anterior):

\begin{align*}
Q_{ij} 
&= \frac{(x - x_i)P_{i-j,i-j+1,\ldots,i-1}(x) - (x - x_{i-j})P_{i-j+1,\ldots,i}(x)}{x_i - x_{i-j}} \\
&= \frac{(x - x_{i-j})P_{i-j+1,\ldots,i}(x) - (x - x_i)P_{i-j,\ldots,i-1}(x)}{x_i - x_{i-j}} \\
&= \frac{(x - x_{i-j})Q_{i,j-1} - (x - x_i)Q_{i-1,j-1}}{x_i - x_{i-j}}.
\end{align*}

Pues:

$$
P_{i-j+1,i-j+2,\ldots,i-1,i} = Q_{i,j-1} \quad \text{dado que } (i - (j - 1)) = i - j + 1,
$$

$$
P_{i-j,i-j+1,\ldots,i-1} = Q_{i-1,j-1} \quad \text{dado que } (i - 1 - (j - 1)) = i - j.
$$

Note que:

$$
Q_{i0} = P_i = f(x_i), \quad \forall i = 0,1,\ldots,n.
$$

Con esta nueva notación, la tabla de Neville se puede escribir como:

$$
\begin{array}{cccccccc}
x_0 & Q_{00} \\
x_1 & Q_{10} & Q_{11} \\
x_2 & Q_{20} & Q_{21} & Q_{22} \\
x_3 & Q_{30} & Q_{31} & Q_{32} & Q_{33} \\
x_4 & Q_{40} & Q_{41} & Q_{42} & Q_{43} & Q_{44} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & Q_{n0} & Q_{n1} & Q_{n2} & Q_{n3} & \cdots & Q_{nn}
\end{array}
$$

$$
Q_{22} = P_{0,1,2}, \quad Q_{nn} = P_{0,1,\ldots,n}
$$
:::

::: {.callout-note}
##### Algoritmo: Para calcular la tabla de Neville y aproximar $f(x^*) \approx P_n(x^*)$

**Entrada:**  
Los nodos $x_0, x_1, \ldots, x_n$.  
Sus imágenes $f(x_0), f(x_1), \ldots, f(x_n)$ como primera columna de la matriz $Q$, es decir $Q_{00}, Q_{10}, Q_{n0}$.

**Salida:**  
La tabla o matriz $Q$, donde $f(x^*) \approx Q_{nn}$.

**Paso 1:** Para $i = 1$ hasta $n$  
\quad Para $j = 1, 2, \ldots, i$
$$
Q_{ij} = \frac{(x - x_{i-j})Q_{i,j-1} - (x - x_i)Q_{i-1,j-1}}{x_i - x_{i-j}}.
$$

**Paso 2:** Salida $Q_{nn}$, parar.  
FIN
:::

#### Diferencias divididas de Newton

La ventaja de este método es que permite calcular el polinomio de Lagrange en cualquier punto $x$.

::: {.callout-note}
###### Notación: Recursiva
\begin{align*}
f[x_i] &:= f(x_i) \\
f[x_i, x_{i+1}] &:= \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i} \\
f[x_i, x_{i+1}, x_{i+2}] &:= \frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i} \\
&\vdots \\
f[x_i, x_{i+1}, \ldots, x_{i+k}] &:= \frac{f[x_{i+1}, x_{i+2}, \ldots, x_{i+k}] - f[x_i, x_{i+1}, \ldots, x_{i+k-1}]}{x_{i+k} - x_i}
\end{align*}
:::

::: {.callout-note}
##### Teorema

Si $P_n(x)$ es el polinomio de Lagrange que coincide con $f(x)$ en $x_0, x_1, \ldots, x_n$, entonces:

\begin{align*}
P_n(x) &= f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) \\
&\quad + \cdots + f[x_0, x_1, \ldots, x_n](x - x_0)(x - x_1) \cdots (x - x_{n-1}) \\
&= f[x_0] + \sum_{k=1}^n f[x_0, \ldots, x_k](x - x_0) \cdots (x - x_{k-1}).
\end{align*}

::: {.callout-caution collapse="true"}
##### Prueba

Si $P_n(x)$ se escribe de la forma

\begin{align*}
P_n(x) &= a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \cdots + \\
&\quad a_n(x - x_0)(x - x_1)\cdots(x - x_{n-1}),
\end{align*}

entonces:

$$
P_n(x_0) = a_0, \quad \text{como } P_n(x_0) = f(x_0) \Rightarrow a_0 = f(x_0) = f[x_0].
$$

Además:

$$
P_n(x_1) = a_0 + a_1(x_1 - x_0), \quad \text{como } P_n(x_1) = f(x_1) \text{ y } a_0 = f[x_0]
\Rightarrow f[x_0] + a_1(x_1 - x_0) = f(x_1)
$$

$$
\Rightarrow a_1 = \frac{f[x_1] - f[x_0]}{x_1 - x_0} = f[x_0, x_1].
$$

Luego, por inducción se puede probar fácilmente que $a_k = f[x_0, x_1, \ldots, x_k]$ (ejercicio). $\blacksquare$
:::
:::

La **Tabla de diferencias divididas de Newton** es la siguiente:

$$
\begin{array}{cccccc}
x_0 & f[x_0] \\
x_1 & f[x_1] & f[x_0, x_1] \\
x_2 & f[x_2] & f[x_1, x_2] & f[x_0, x_1, x_2] \\
\vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & f[x_n] & f[x_{n-1}, x_n] & f[x_{n-2}, x_{n-1}, x_n] & \cdots & f[x_0, x_1, \ldots, x_n]
\end{array}
$$

En la diagonal de la matriz anterior están los coeficientes del polinomio de Lagrange $P_n(x)$, según la forma presentada en el teorema anterior. El siguiente algoritmo calcula el polinomio de Lagrange usando la Tabla de diferencias divididas de Newton.

::: {.callout-note}
##### Algoritmo

**Objetivo**: Calcular el polinomio de Lagrange usando la Tabla de diferencias divididas de Newton.

**Entrada**: Los nodos $x_0, x_1, \ldots, x_n$ y los valores $f(x_0), f(x_1), \ldots, f(x_n)$ como primera columna de la matriz $F$.

**Salida**: $F_{00}, F_{11}, \ldots, F_{nn}$, los coeficientes de $P_n(x)$, donde:  
$$
P_n(x) = \sum_{i=0}^n F_{ii} \prod_{k=0}^{i-1} (x - x_k)
$$


**Paso 1**: Para $i = 1, \ldots, n$

\quad\quad Para $j = 1, 2, \ldots, i$

$$
F_{ij} = \frac{F_{i,j-1} - F_{i-1,j-1}}{x_i - x_{i-j}}
$$

**Paso 2**: Salida $(F_{00}, F_{11}, \ldots, F_{nn})$  
Parar.
:::


### Interpolación de Hermite


Los *Polinomios Osculantes* generalizan a los Polinomios de Taylor y a los Polinomios de Lagrange como veremos más adelante. 
La idea del polinomio de Hermite (que es un caso particular de los polinomios osculantes) es que si se tienen $x_0, x_1, \ldots, x_n$, $n+1$ nodos, entonces $f$ y $f'$ coincidan con $P(x)$ y $P'(x)$ en los nodos respectivamente.

::: {.callout-note}
##### Definición: Polinomio osculante

- Sean $x_0, x_1, \ldots, x_n$, $n + 1$ nodos distintos en $[a,b]$.
- Sea $m_i$ un entero no negativo, con $m_i$ asociado a $x_i$; para $i = 0,1,\ldots,n$.
- Sea $m = \max_{0 \leq i \leq n} m_i$.
- Sea $f \in C^m[a,b]$.

Entonces el **Polinomio Osculante** que aproxima a $f$ es el polinomio de grado menor tal que:

$$
\frac{d^k P(x_i)}{dx^k} = \frac{d^k f(x_i)}{dx^k},
$$

para $i = 0,1,\ldots,n$ y $k = 1,\ldots,m_i$. Es decir, en el $i$-ésimo nodo el polinomio y la función $f$ coinciden hasta la derivada $m_i$.
:::

::: {.callout-important collapse="true"}
###### Observación

1. Si $n = 0$, el polinomio osculante es el polinomio de Taylor de grado $m_0$ para $f$ en $x_0$.

Para ver esto, sea $P(x)$ polinomio de Taylor de grado $m_0$ para $f$ en $x_0$, entonces:

$$
P(x) = f(x_0) + f'(x_0)(x - x_0) + f''(x_0) \frac{(x - x_0)^2}{2!} + \cdots + f^{(m_0)}(x_0) \frac{(x - x_0)^{m_0}}{m_0!}
$$

De donde se deduce que: $P(x_0) = f(x_0)$.

Además:

$$
P'(x) = f'(x_0) + 2f''(x_0) \frac{(x - x_0)}{2!} + \cdots + m_0 f^{(m_0)}(x_0) \frac{(x - x_0)^{m_0 - 1}}{m_0!},
$$

lo cual implica que: $P'(x_0) = f'(x_0)$ y así sucesivamente se puede probar que $P^{(k)}(x_0) = f^{(k)}(x_0)$ para todo $k \leq m_0$.

2. Si $m_i = 0$, e $i = 0,1,\ldots,n$, entonces el polinomio osculante es el polinomio de Lagrange que interpola a $f$ en $x_0, x_1, \ldots, x_n$.  
Pues se está pidiendo que solamente coincida con $f$ en $x_0, x_1, \ldots, x_n$ (no en sus derivadas) y se probó que el polinomio de menor grado que hace esto es el de Lagrange, además se probó que es único.
:::

Cuando $P(x)$ coincide con $f$ y $f'$ en $x_0, x_1, \ldots, x_n$, se dice que $P(x)$ tiene "la misma apariencia" que $f$ en los nodos y se denomina el **Polinomio de Hermite**.

El siguiente Teorema da un método para calcular el Polinomio de Hermite.

::: {.callout-note}
##### Teorema: Polinomio de Hermite (osculante con derivadas)

- Sea $f \in C[a,b]$.
- Sean $x_0, x_1, \ldots, x_n$, $(n+1)$ nodos distintos en $[a,b]$.

Entonces el polinomio de grado menor que coincide con $f$ y $f'$ en $x_0, x_1, \ldots, x_n$:

- Tiene grado $2n+1$.
- Está dado por

$$
H_{2n+1}(x) = \sum_{j=0}^{n} f(x_j) H_{nj}(x) + \sum_{j=0}^{n} f'(x_j) \widetilde{H}_{nj}(x),
$$

donde

$$
H_{nj}(x) = \big[1 - 2(x - x_j)L'_{nj}(x_j)\big]L_{nj}^2(x),
$$

y

$$
\widetilde{H}_{nj}(x) = (x - x_j) L_{nj}^2(x).
$$

- Además, el error absoluto es:

$$
\left| f(x) - H_{2n+1}(x) \right| = \left| \frac{(x - x_0)^2 \cdots (x - x_n)^2}{(2n+2)!} f^{(2n+2)}(\xi) \right|, \quad \text{con } \xi \in ]a, b[.
$$

::: {.callout-caution collapse="true"}
##### Prueba

- Se debe demostrar que $H_{2n+1}(x_i) = f(x_i)$ para todo $i = 0, 1, \ldots, n$. Para ver esto, recordemos que:

$$
L_{nj}(x_i) =
\begin{cases}
0 & \text{si } i \ne j \\
1 & \text{si } i = j
\end{cases}
$$

de donde, cuando $i \ne j$:

$$
H_{nj}(x_i) = 0 \quad \text{y} \quad \widetilde{H}_{nj}(x_i) = 0.
$$

Mientras que:

$$
H_{ni}(x_i) = [1 - 2(x_i - x_i)L'_{ni}(x_i)] \cdot 1 = 1,
$$

$$
\widetilde{H}_{ni}(x_i) = (x_i - x_i) \cdot 1^2 
$$

Luego:

$$
H_{2n+1}(x_i) = \sum_{\substack{j = 0 \\ j \ne i}}^n f(x_j) \cdot 0 + f(x_i) \cdot 1 + \sum_{j=0}^n f'(x_j) \cdot 0 = f(x_i).
$$

Por lo tanto:

- $H_{2n+1}(x_i) = f(x_i)$ para $i = 0, 1, 2, \ldots, n$.

- Se debe demostrar que $H'_{2n+1}(x_i) = f'(x_i)$ para todo $i = 0, 1, \ldots, n$.

Nótese que $L_{nj}(x)$ es un factor de $H'_{nj}(x)$, lo cual implica que $H'_{nj}(x_i) = 0$ cuando $i \ne j$.

Además, si $i = j$:

\begin{align*}
H'_{ni}(x_i) &= -2L'_{ni}(x_i)L^2_{ni}(x_i) + [1 - 2(x_i - x_i)L'_{ni}(x_i)] \cdot 2 \cdot L_{ni}(x_i)L'_{ni}(x_i) \\
&= -2L'_{ni}(x_i) + 2L'_{ni}(x_i) \\
&= 0.
\end{align*}

Por lo tanto, $H'_{nj}(x_i) = 0$ para todo $i = 0, 1, 2, \ldots, n$ y para todo $j = 0, 1, 2, \ldots, n$.

Además:

$$
\widetilde{H}_{nj}(x_i) = L^2_{nj}(x_i) + (x_i - x_j)L'_{nj}(x_j) \cdot 2 \cdot L_{nj}(x_i)L'_{nj}(x_i),
$$

de donde:

$$
\widetilde{H}'_{nj}(x_i) =
\begin{cases}
0 & \text{si } i \ne j \\
1 & \text{si } i = j
\end{cases}
$$

por lo tanto:

$$
\widetilde{H}'_{2n+1}(x_i) = \sum_{j=0}^n f(x_j) \cdot 0 + \sum_{\substack{j = 0 \\ j \ne i}}^n f'(x_j) \cdot 0 + f'(x_i) \cdot 1 = f'(x_i).
$$

es decir:

$$
H'_{2n+1}(x_i) = f'(x_i) \quad \text{para } i = 0, 1, 2, \ldots, n.
$$

- La unicidad queda de ejercicio al lector.

$\blacksquare$

:::
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Calcule $H_5(x)$ que aproxima a $f(x) = e^x$ en $x_0 = 0$, $x_1 = 1$, $x_2 = 2$.

**Solución:**

| $k$ | $x_k$ | $f(x_k)$     | $f'(x_k)$     |
|----:|:-----:|:------------:|:-------------:|
|  0  |   0   | 1            | 1             |
|  1  |   1   | 2.7182818    | 2.7182818     |
|  2  |   2   | 7.3890561    | 7.3890561     |

$$
H_5 = \sum_{j=0}^{2} f(x_j) H_{nj}(x) + \sum_{j=0}^{2} f'(x_j) \widetilde{H}_{nj}(x).
$$

Calculemos cada uno de los términos:

$$
H_{20} = [1 - 2(x - x_0) L'_{20}(x_0)] L_{20}^2(x),
$$

con

\begin{align*}
L_{20}(x) &= \frac{(x - 1)(x - 2)}{(0 - 1)(0 - 2)} \\
          &= \frac{x^2 - x - 2x + 2}{2} \\
          &= \frac{x^2 - 3x + 2}{2},
\end{align*}

esto implica que:

$$
L'_{20}(x) = \frac{2x - 3}{2},
$$

de donde:

$$
L'_{n0}(0) = -\frac{3}{2},
$$

luego:

$$
H_{20} = (1 - 3x) \left( \frac{x^2 - 3x + 2}{2} \right)^2.
$$


\begin{align*}
L_{21}(x) &= \frac{x(x - 2)}{(1 - 0)(1 - 2)} \\
         &= \frac{x(x - 2)}{-1} \\
         &= -x(x - 2) \\
         &= -x^2 + 2x.
\end{align*}

Esto implica que:

$$
L'_{21}(x) = -2x + 2,
$$

de donde:

$$
L'_{21}(x_1) = 0,
$$

por lo que:

\begin{align*}
H_{21} &= (1 - 2(x - 1) \cdot 0)(-x(x - 2))^2 \\
      &= x^2 (x - 2)^2.
\end{align*}

\begin{align*}
L_{22}(x) &= \frac{x(x - 1)}{(2 - 0)(2 - 1)} \\
         &= \frac{x(x - 1)}{2} \\
         &= \frac{x^2 - x}{2},
\end{align*}

de donde:

$$
L'_{22}(x) = \frac{2x - 1}{2},
$$

luego:

$$
L'_{22}(x_2) = \frac{3}{2},
$$

por lo que:


\begin{align*}
H_{22}(x) &= \left(1 - 2(x - 2) \cdot \frac{3}{2} \right) \left( \frac{x^2 - x}{2} \right)^2 \\
         &= (1 - 3(x - 2)) \left( \frac{x^2 - x}{2} \right)^2.
\end{align*}

Por otra parte,

\begin{align*}
\widehat{H}_{20}(x) &= (x - x_0) L_{20}^2(x) \\
                    &= x \left( \frac{x^2 - 3x + 2}{2} \right)^2.
\end{align*}

Análogamente:

$$
\widehat{H}_{21}(x) = (x - 1)x^2(x - 2)^2,
$$

$$
\widehat{H}_{22}(x) = (x - 2) \left( \frac{x^2 - x}{2} \right)^2.
$$

Finalmente:

\begin{align*}
H_5(x) =\; & (1 - 3x) \left( \frac{x^2 - 3x + 2}{2} \right)^2 + 2.7182818\,x^2(x - 2)^2 \\
          & +\; 7.3890561 (1 - 3(x - 2)) \left( \frac{x^2 - x}{2} \right)^2 \\
          & +\; x \left( \frac{x^2 - 3x + 2}{2} \right)^2 + 2.7182818 (x - 1)x^2(x - 2)^2 \\
          & +\; 7.3890561 (x - 2) \left( \frac{x^2 - x}{2} \right)^2.
\end{align*}

$\blacksquare$

---

2. Aproximando $f(0.25) \cong H(0.25)$ se tiene que:

$$
H_5(0.25) = 1.28364
$$

con el siguiente error absoluto:

Aproximando $e^{0.25} - H_5(0.25)$:

\begin{align*}
\left| e^{0.25} - H_5(0.25) \right| &= \left| 1.28402 - 1.28364 \right| \\
                                   &= 3.8 \times 10^{-5} \\
                                   &\cong 0.3 \times 10^{-4},
\end{align*}

mientras que utilizando el polinomio de Lagrange el error fue:

$$
\left| P(0.25) - e^{0.25} \right| \cong 0.1312511,
$$

que es mucho mayor.

---

3. Calculando una cota para el error teórico se tiene que:

$$
|e^x - H_5(x)| = \left| \frac{x^2(x - 1)^2(x - 2)^2}{(2 \cdot 2 + 2)!} e^{\xi} \right|, \quad \text{con } \xi \in [0, 2],
$$

esto implica que:

\begin{align*}
|e^{0.25} - H_5(0.25)| &\leq \frac{(0.25)^2 (0.75)^2 (1.75)^2}{6!} e^2 \\
                       &= \frac{0.7955}{720} \\
                       &= 1.1 \times 10^{-3} \\
                       &= 0.1 \times 10^{-2}.
\end{align*}

Como se ha visto, el cálculo del polinomio de Hermite es sumamente tedioso, por esto en la siguiente sección se propone un algoritmo que facilite dicho cálculo.
:::

#### Algoritmo para el polinomio de Hermite

Sabemos que:

\begin{align*}
P_n(x) = f[x_0] + \sum_{k=1}^{n} f[x_0, x_1, \dots, x_k](x - x_0) \cdots (x - x_{k-1}),
\end{align*}

además utilizaremos el siguiente lema.

::: {.callout-note}
##### Lema: Generalización del Teorema del valor medio

Si $f \in C^n[a, b]$ y $x_0, x_1, \dots, x_n$ son los $(n+1)$ nodos distintos en $[a,b]$, entonces:  
existe $\xi \in ]a, b[$ tal que:

$$
f[x_0, x_1, \dots, x_n] = \frac{f^{(n)}(\xi)}{n!}.
$$

::: {.callout-caution collapse="true"}
##### Prueba

Ejercicio al lector. $\blacksquare$
:::
:::

Suponga que se conocen

\begin{align*}
\begin{array}{ccc}
x_0 & f(x_0) & f'(x_0) \\
x_1 & f(x_1) & f'(x_1) \\
\vdots & \vdots & \vdots \\
x_n & f(x_n) & f'(x_n)
\end{array}
\end{align*}

Definimos la sucesión

$$
\{z_n\}_{n \in \mathbb{N}}, \; z_{2i} = z_{2i+1} = x_i, \quad \text{para } i = 0, 1, \dots, n.
$$

Luego se forma la tabla de Hermite como sigue:

\begin{align*}
\begin{array}{lllll}
z_0 = x_0 & f[z_0] = f(x_0) \\
z_1 = x_0 & f[z_1] = f(x_0) & f[z_0,z_1] \approx f'(z_0) \\
z_2 = x_1 & f[z_2] = f(x_1) & f[z_1,z_2] & f[z_0, z_1, z_2] \\
z_3 = x_1 & f[z_3] = f(x_1) & f[z_2,z_3] \approx f'(z_1) & f[z_1,z_2,z_3] & f[z_0,z_1,z_2,z_3] \\
z_4 = x_2 & f[z_4] = f(x_2) & f[z_3,z_4] & f[z_2,z_3,z_4] & f[z_1,z_2,z_3,z_4] \\
z_5 = x_2 & f[z_5] = f(x_2) & f[z_4,z_5] \approx f'(z_2) & f[z_3,z_4,z_5] & \cdots \\
\quad\;\vdots & \quad\;\vdots & \quad\;\vdots & \quad\;\vdots & \quad\;\vdots \\
\end{array}
\end{align*}

A partir de la tercera columna de la matriz anterior el cálculo se hace exactamente igual que en el método de Neville.  
Además, nótese que $f[z_0, z_1]$ no se puede calcular usando la definición, pues daría $\frac{0}{0}$, pero resulta “razonable” tomar $f[z_0, z_1] \approx f'(x_0)$. ¿Por qué?

Luego:

\begin{align*}
H_{2n+1}(x) 
&= f[z_0] + f[z_0, z_1]\overbrace{(x - z_0)}^{(z-z_0)} + f[z_0, z_1, z_2]\overbrace{(x - z_0)^2}^{(z-z_0)(z-z_1)} \\
&\quad + f[z_0, z_1, z_2, z_3](x - z_0)^2(x - z_1) \\
&\quad + f[z_0, z_1, z_2, z_3, z_4](x - z_0)^2(x - z_1)^2 + \cdots
\end{align*}

::: {.callout-note}
##### Algoritmo: Para obtener los coeficientes del polinomio de Hermite

**Entrada:** $x_0, x_1, \dots, x_n;\; f(x_0), f(x_1), \dots, f(x_n)\;$ y $\;f'(x_0), f'(x_1), \dots, f'(x_n)$  
**Salida:** Los números $Q_{0,0}, Q_{1,1}, \dots, Q_{(2n+1),(2n+1)}$ coeficientes de:

\begin{align*}
H_{2n+1}(x) &= Q_{0,0} + \\
&\quad Q_{1,1}(x - x_0) + \\
&\quad Q_{2,2}(x - x_0)^2 + \\
&\quad Q_{3,3}(x - x_0)^2(x - x_1) + \\
&\quad Q_{4,4}(x - x_0)^2(x - x_1)^2 + \cdots + \\
&\quad Q_{(2n+1),(2n+1)}(x - x_0)^2 \cdots (x - x_{n-1})^2(x - x_n)
\end{align*}

**Paso 1**: Para $i = 1, \dots, n$ siga pasos 2–3

**Paso 2**: Tomar  
\begin{align*}
z_{2i} &= x_i \\
z_{2i+1} &= x_i \\
Q_{2i,0} &= f(x_i) \\
Q_{2i+1,0} &= f(x_i) \\
Q_{2i+1,1} &= f'(x_i)
\end{align*}

**Paso 3**: Si $i \ne 0$, tome  
$$
Q_{2i,1} = \frac{Q_{2i,0} - Q_{2i-1,0}}{z_{2i} - z_{2i-1}}
$$

**Paso 4**: Para $i = 2,3,\dots, 2n+1$  
\quad Para $j = 2,3,\dots,i$ tomar  
$$
Q_{i,j} = \frac{Q_{i,j-1} - Q_{i-1,j-1}}{z_i - z_{i-j}}
$$

**Paso 5**: Salida $\big(Q_{0,0}, Q_{1,1}, \dots, Q_{(2n+1),(2n+1)}\big)$.  
**Parar**
:::

::: {.callout-tip collapse="true"}
###### Ejemplo

Si se corre el algoritmo con $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, entonces:

\begin{align*}
Q_{0,0} &= 1 \\
Q_{1,1} &= 1 \\
Q_{2,2} &= 0.71828183 \\
Q_{3,3} &= 0.28171817 \\
Q_{4,4} &= 0.09726402 \\
Q_{5,5} &= 0.02375378
\end{align*}

Así se obtiene el siguiente polinomio:

\begin{align*}
H_5(x) &= 1 + x + 0.71828183x^2 + 0.28171817x^2(x - 1) \\
&\quad + 0.09726402x^2(x - 1)^2 + 0.02375378x^2(x - 1)^2(x - 2)
\end{align*}
:::

::: {.callout-important}
##### NOTA

Ver `hermite.nb`.
:::

### Interpolación por Splines Cúbicos

#### Presentación geométrica

![](Imagenes/Interpolación por Splines Cúbicos.png)

Es encontrar polinomios cúbicos tales que:

- Coincidan con $f(x)$ en los nodos y $P(x)$ sea continuo.
- $P(x)$ tenga primera derivada en los nodos internos.
- $P(x)$ tenga segunda derivada en los nodos internos.

Así, se deben encontrar $n$ polinomios con cuatro coeficientes cada uno. Por lo tanto, se tienen $4n$ incógnitas. Para encontrarlas, se deben establecer $4n$ ecuaciones.

¿Cómo se encuentran tales ecuaciones?

1. Condición de continuidad en los nodos internos

 Como $P(x)$ debe coincidir con $f$ en los nodos internos y $P(x)$ debe ser continuo, se tienen las siguientes ecuaciones:

$$
\left\{
\begin{aligned}
a_{i-1}x_{i-1}^3 + b_{i-1}x_{i-1}^2 + c_{i-1}x_{i-1} + d_{i-1} &= f(x_{i-1}) \\
a_i x_{i-1}^3 + b_i x_{i-1}^2 + c_i x_{i-1} + d_i &= f(x_{i-1})
\end{aligned}
\right\}
\quad \text{para } i = 2, \dots, n
$$

 De aquí se tienen $2(n - 1) = 2n - 2$ ecuaciones.

2. Condición en los extremos

 Como $P(x)$ debe coincidir con $f$ en los extremos, se tienen las siguientes ecuaciones:

$$
\left\{
\begin{aligned}
a_1 x_0^3 + b_1 x_0^2 + c_1 x_0 + d_1 &= f(x_0) \\
a_n x_n^3 + b_n x_n^2 + c_n x_n + d_n &= f(x_n)
\end{aligned}
\right.
$$

 De aquí se tienen 2 ecuaciones.

3. Como las primeras derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:

\begin{align*}
3a_{i-1}x_{i-1}^2 + 2b_{i-1}x_{i-1} + c_{i-1} &= 3a_i x_{i-1}^2 + 2b_i x_{i-1} + c_i,
\quad \text{con } i = 2, \ldots, n.
\end{align*}

 De aquí se tienen $(n - 1)$ ecuaciones.

4. Como las segundas derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:

\begin{align*}
6a_{i-1}x_{i-1} + 2b_{i-1} &= 6a_i x_{i-1} + 2b_i,
\quad \text{con } i = 2, \ldots, n.
\end{align*}

 De aquí se tienen $(n - 1)$ ecuaciones.

En total tenemos: $2n - 2 + 2 + (n - 1) + (n - 1) = 4n - 2$ por lo que faltan todavía 2 ecuaciones.

5. Asumiendo que las segundas derivadas en los nodos extremos deben ser 0, se obtienen 2 ecuaciones más:

\begin{align*}
\left\{
\begin{aligned}
6a_1 x_0 + 2b_1 &= 0 \\
6a_{n-1} x_n + 2b_{n-1} &= 0
\end{aligned}
\right.
\end{align*}

::: {.callout-tip collapse="true"}
#### Ejemplo

Para $f(x)$ dada por la siguiente tabla:

$$
\begin{array}{c|c}
x & f(x) \\
\hline
3.0 & 2.5 \\
4.5 & 1.0 \\
7.0 & 2.5 \\
9.0 & 0.5
\end{array}
$$

Calcule el polinomio de interpolación usando Splines Cúbicos.


**Solución:** Se requieren $3 \cdot 4 = 12$ ecuaciones, las cuales se obtienen como sigue:

1. Como $P(x)$ debe coincidir con $f$ en los nodos internos y $P(x)$ debe ser continuo, se tienen las siguientes ecuaciones:

\begin{align*}
91.125a_1 + 20.25b_1 + 4.5c_1 + d_1 &= 1 \\
91.125a_2 + 20.25b_2 + 4.5c_2 + d_2 &= 1 \\
343a_2 + 49b_2 + 7c_2 + d_2 &= 2.5 \\
343a_3 + 49b_3 + 7c_3 + d_3 &= 2.5
\end{align*}

2. Como $P(x)$ debe coincidir con $f$ en los extremos, se tienen las siguientes ecuaciones:

\begin{align*}
27a_1 + 9b_1 + 3c_1 + d_1 &= 2.5 \\
729a_3 + 81b_3 + 9c_3 + d_3 &= 0.5
\end{align*}

 Igualdad de derivadas en nodos internos

 Como las primeras derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:
	- con $x = 4.5$:

\begin{align*}
60.75a_1 + 9b_1 + 3c_1 - 60.75a_2 - 9b_2 - c_2 &= 0,
\end{align*}
	- con $x = 7$:

\begin{align*}
147a_2 + 14b_2 + c_2 - 147a_3 - 14b_3 - c_3 &= 0,
\end{align*}
	- con $x = 4.5$:

\begin{align*}
27a_1 + 2b_1 - 27a_2 - 2b_2 &= 0,
\end{align*}
	- con $x = 7$:

\begin{align*}
42a_2 + 2b_2 - 42a_3 - 2b_3 &= 0.
\end{align*}

Condición de derivadas segundas nulas en extremos

Asumiendo que las segundas derivadas en los nodos extremos deben ser $0$, se obtienen 2 ecuaciones más:
	- con $x = 3$:

\begin{align*}
18a_1 + 2b_1 &= 0,
\end{align*}
	- con $x = 9$:

\begin{align*}
54a_3 + 2b_3 &= 0.
\end{align*}

Sistema completo de ecuaciones

De donde, resolviendo el siguiente sistema de ecuaciones:

\begin{align*}
&91.125a_1 + 20.25b_1 + 4.5c_1 + d_1 &&= 1 \\
&91.125a_2 + 20.25b_2 + 4.5c_2 + d_2 &&= 1 \\
&343a_2 + 49b_2 + 7c_2 + d_2 &&= 2.5 \\
&343a_3 + 49b_3 + 7c_3 + d_3 &&= 2.5 \\
&27a_1 + 9b_1 + 3c_1 + d_1 &&= 2.5 \\
&729a_3 + 81b_3 + 9c_3 + d_3 &&= 0.5 \\
&60.75a_1 + 9b_1 + 3c_1 - 60.75a_2 - 9b_2 - c_2 &&= 0 \\
&147a_2 + 14b_2 + c_2 - 147a_3 - 14b_3 - c_3 &&= 0 \\
&27a_1 + 2b_1 - 27a_2 - 2b_2 &&= 0 \\
&42a_2 + 2b_2 - 42a_3 - 2b_3 &&= 0 \\
&18a_1 + 2b_1 &&= 0 \\
&54a_3 + 2b_3 &&= 0
\end{align*}

se obtiene la siguiente solución: 

\begin{align*}
a_1 &= 0.187 & a_2 &= -0.214 & a_3 &= 0.128 \\
b_1 &= -1.679 & b_2 &= 3.73 & b_3 &= -3.449 \\
c_1 &= 3.617 & c_2 &= -20.726 & c_3 &= 29.534 \\
d_1 &= 1.722 & d_2 &= 38.237 & d_3 &= -79.035
\end{align*}

por lo que el spline cúbico es:

$$
P(x) =
\begin{cases}
0.183x^3 - 1.679x^2 + 3.617x + 1.722 & \text{si } 3 \leq x \leq 4.5, \\
-0.214x^3 + 3.73x^2 - 20.726x + 38.237 & \text{si } 4.5 \leq x \leq 7, \\
0.128x^3 - 3.499x^2 + 29.53x - 79.035 & \text{si } 7 \leq x \leq 9.
\end{cases}
$$

$\blacksquare$

:::

#### Presentación algorítmica
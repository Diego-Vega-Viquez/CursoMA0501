---
title: "Apuntes Teoría MA0501"
author: 
  - name: "Diego Alberto Vega Víquez"
    email: "diegovv13@gmail.com"
date: today
lang: es
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    linestretch: 1.3
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
      - headheight=15pt
      - footskip=1.25cm
    toc: true
    toc-depth: 3
    number-sections: false
    classoption:
      - oneside
      - titlepage 
    openany: true
    colorlinks: false   
    top-level-division: section
    pdf-engine: xelatex
    include-in-header:
      text: |
        \usepackage[most]{tcolorbox}
        \usepackage[hidelinks]{hyperref}
        \usepackage{setspace}
        \AtBeginDocument{\setstretch{1.0}} % ← interlineado
  html:
    code-annotations: hover
    execute:
      code-fold: true
      message: false
      warning: false
    theme: flatly 
    toc: true
    toc-depth: 3
    toc-location: left
    html-math-method: katex
    css: styles.css
    embed-resources: true
    page-layout: full
---
\newpage
# Fundamentos de la Programación en Lenguaje R

## Funciones

A continuación un ejemplo de una función en **R**

```{r}
#| code-fold: true
area.triangulo <- function(base,altura) {   # <1>
  area <- (base*altura)/2
  return(area)
}

area.triangulo(2,5)
```
1. Comunmente usamos el punto para separar las palabras en el nombre de las variables

### Usando For, If y While

```{r}
#| code-fold: true
encuentra.cero.f <- function(v) {
  s <- -1
  for(i in 1:length(v)) {
    if((v[i]==0) && (s == -1)) {
      s<-i
    }
  }
  if (s != -1) {
    return(s)
  }
  else {
    return("¡El vector no tiene ningún cero!")
  }
}

vec<-c(4,-7,2,1,9)
encuentra.cero.f(vec)
```

La siguiente función calcula la suma del valor absoluto de las entradas de un vector, es de decir, la norma 1 del vector dada por

$$|v[1]|+|v[2]|+\ldots+|v[n]|$$

```{r}
#| code-fold: true
norma1 <- function(v) {
   suma <- 0
   i = 1
   while (i <= length(v)) {
      suma <- suma + abs(v[i])
      i <- i + 1
   }
   return(suma)
}

vec0<-c(4,-7,2,1)
norma1(vec0)
```


# Algoritmos, aproximaciones y error

## Aproximaciones y Error

### Aritmética punto flotante


#### Punto flotante

::: {.callout-note title="Definición: Forma Punto Flotante"}

Un número real $x$ está en **forma punto flotante** si se escribe de la forma

$$
0.d_1 d_2 \cdots d_k \times 10^{n},
$$

donde $0 \le d_i \le 9$, $d_1 \ne 0$, $i = 1,2,\ldots,k$.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

$$
\pi = 0.31415 \times 10^{1}.
$$
:::

::: {.callout-important collapse="true" title="Observación"}

- En análisis numérico todas las respuestas deben darse en **notación de punto flotante**.
- Si $x \in \mathbb{R}$, entonces se escribe como  
  $$x = 0.d_1 d_2 d_3 \cdots \times 10^{n},$$
  pero el computador solo puede **almacenar una cantidad finita de dígitos** por limitaciones de memoria.

Por lo tanto, quedan **dos posibilidades**:

1. **Cortar**
   - $$x \approx {fl}(x) = 0.d_1 d_2 \cdots d_k \times 10^{n},$$
     es decir, **cortar a partir del dígito $k+1$**.

2. **Redondear y cortar**
   - Si $d_{k+1} \ge 5$ entonces **sume 1 a $d_k$** y corte.
   - Si no, **solamente corte**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

$$
\pi = 3.14159265\cdots, \quad \text{con } k = 5 \text{ tenemos:}
$$

$$
fl(\pi) = 0.31415 \times 10^{1} \quad (\textbf{Cortando})
$$

$$
fl(\pi) = 0.31416 \times 10^{1} \quad (\textbf{Redondeando})
$$
:::

::: {.callout-note title="Definición: Error de Redondeo"}

El error que resulta de reemplazar $x$ por $fl(x)$ se denomina **Error de Redondeo.**
:::

#### ¿Cómo medir, controlar, la propagación del error?

::: {.callout-note title="Definición: Error absoluto & Error relativo"}

Si $P^*$ es una aproximación de $P$ se llama:

$$
\text{Error absoluto} \;=\; |P - P^*|
$$

$$
\text{Error relativo} \;=\; \frac{|P - P^*|}{|P|}, \quad \text{para } P \neq 0
$$
:::

::: {.callout-note title="Teorema"}

Sea $x \in \mathbb{R}$, $x \neq 0$, entonces:

1. Si $fl(x)$ se obtiene usando $k$-dígitos de $x$ **cortando**, entonces:

$$
\left| \frac{x - fl(x)}{x} \right| \leq 10^{-k+1}.
$$

2. Si $fl(x)$ se obtiene usando $k$-dígitos de $x$ **redondeado**, entonces:

$$
\left| \frac{x - fl(x)}{x} \right| \leq 0.5 \times 10^{-k+1} = 5 \times 10^{-k}.
$$

::: {.callout-caution collapse="true" title="Prueba"}

**1.**  
\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_1 d_2 \cdots \times 10^n - 0.d_1 d_2 \cdots d_k \times 10^n}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots \times 10^{n-k}}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
&\le \frac{1}{0.1} \times 10^{-k} \\[1ex]
&= 10^{-k+1}.
\end{align*}

**2.** 


Ya que ${fl}(x)$ es una aproximación de $x$ con redondeo a $k$ dígitos eso significa que podemos escribir ${fl}(x)$ de la siguiente forma

$${fl}(x) = 0.d_1 d_2 \cdots d_k \times 10^{n}$$

Sea $x\in\mathbb{R}$ que escribiremos como 

$$x=0.d_1 d_2 \cdots \times 10^{n}$$

De esta forma

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_1 d_2 \cdots \times 10^n - 0.d_1 d_2 \cdots d_k \times 10^n}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots \times 10^{n-k}}{0.d_1 d_2 \cdots \times 10^n} \right| \\[1ex]
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
\end{align*}

Aquí hay que analizar por casos:

- Suponga que $d_{k+1} < 5$

En este caso basta con cortar en $d_k$ así:

\begin{align*}
\left| \frac{x - fl(x)}{x} \right|
&= \left| \frac{0.d_{k+1} d_{k+2} \cdots}{0.d_1 d_2 \cdots} \right| \times 10^{-k} \\[1ex]
&\leq 0.5\cdot\left| \frac{1}{0.1} \right| \times 10^{-k} \\[1ex]
&= 0.5\times 10^{-k+1}
\end{align*}

- Suponga que $d_{k+1} \ge 5$.

Recuerde que para este caso en ${fl}(x)$ pasa que $d_k$ es una unidad mayor que el $d_k$ de $x$. De esta forma se va a cumplir que $d_{j_\text{real}}$$=d_{j_\text{aproximado}}$ para $j=\{1,2,\ldots,k-1\}$ así se tiene que 

$$
|x - fl(x)| = 10^{1-k}\cdot(1-0.d_{k+1}\cdots)
$$
$$
\left| \frac{x - fl(x)}{x} \right| = \frac{10^{1-k}\cdot(1-0.d_{k+1}\cdots)}{|0.d_1 d_2 \cdots\times 10^{n}|}
$$
Vea que 
\begin{align*}
d_{k+1} \ge 5 &\implies 0.d_{k+1}\cdots\ge\frac{1}{2}\\
&\implies 1-0.d_{k+1}\cdots\le\frac{1}{2}\\
&\implies 1-0.d_{k+1}\cdots\le\frac{1}{2}
\end{align*}
Así
$$
\left| \frac{x - fl(x)}{x} \right| \le \frac{10^{1-k} \cdot 0.5}{|0.d_1 d_2 \cdots\times 10^{n}|} \le 10^{1-k} \cdot 0.5
$$
Luego, concluya que 
$$
\left| \frac{x - {fl}(x)}{x} \right| \leq 0.5 \times 10^{-k+1} \qquad \blacksquare
$$

:::

:::


::: {.callout-tip collapse="true" title="Ejemplo"}

Si $P = 0.3000 \times 10^{1}$ y $P^* = 0.3100 \times 10^{1}$ entonces:

$$
|P - P^*| = 0.1 \times 10^{0}, 
\qquad \frac{|P - P^*|}{|P|} = 0.33 \times 10^{-1}
$$

Pero si $P = 0.3000 \times 10^{4}$ y $P^* = 0.3100 \times 10^{4}$ entonces:

$$
|P - P^*| = 0.1 \times 10^{3}, 
\qquad \frac{|P - P^*|}{|P|} = 0.33 \times 10^{-1}
$$
:::

::: {.callout-note title="Definición: Dígitos significativos"}

Se dice que un número $P^*$ aproxima a $P$ con $t$ **dígitos significativos** si $t \in \mathbb{N}$ es el número más grande tal que:

$$
\frac{|P - P^*|}{|P|} < 5 \times 10^{-t} \;=\; 0.5 \times 10^{-t+1}.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


$$
x \;=\; \pi \;=\; 0.\underbrace{3141}_{\text{4 dígitos}}59265 \cdots \times 10^{1}.
$$

$$
x^* \;=\; \frac{22}{7} \;=\; 0.\underbrace{3142}_{\text{dígitos distintos}}8517 \times 10^{1}.
$$

$$
\frac{|x - x^*|}{|x|} = 0.402 \times 10^{-3} < 0.5 \times 10^{-4+1}.
$$

Por lo que $x^*$ aproxima $x$ con **4 dígitos significativos**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


¿Qué valores puede tomar $x^*$ para aproximar 1000 con **4 dígitos significativos**?  

$$
x = 1000 = 0.1000 \times 10^{4}, \quad \text{luego tenemos que:}
$$

$$
\left| \frac{x^* - 1000}{1000} \right|
< 5 \times 10^{-4}
\;\;\;\;\;\;\;\; \Rightarrow \;\;\;\;\;\;\;\;
-5 \times 10^{-4}
< \frac{x^* - 1000}{1000}
< 5 \times 10^{-4},
$$

$$
\Rightarrow \;\; 999.5 < x^* < 1000.5.
$$

Note que 
$$x^* = 0.9996 \times 10^{3}$$ 
aproxima a $x$ con **4 dígitos significativos**,  

pero 
$$y = 0.1001 \times 10^{4}$$ 
**no** aproxima a $x$ con 4 dígitos significativos.
:::

### Problemas con la aritmética punto flotante


- **División:**  
  Si $x \cong x + \varepsilon$ y se divide entre $\delta$ muy pequeño se tiene:

  $$
  \frac{x}{\delta} \cong \frac{x + \varepsilon}{\delta} = \frac{x}{\delta} + \underbrace{\frac{\varepsilon}{\delta}}_{(*)}
  $$
$(*)$: Nuevo error enorme


- **Resta de dos números casi iguales:**

\begin{align*}
  fl(x) \; &= \; 0.d_1 d_2 \cdots d_p \alpha_{p+1} \alpha_{p+2} \cdots \alpha_k \times 10^n\\
  fl(y) \; &= \; 0.d_1 d_2 \cdots d_p \beta_{p+1} \beta_{p+2} \cdots \beta_k \times 10^n\\
  x - y \; &\cong \; fl(x) - fl(y) \;=\; 0.\underbrace{\gamma_{p+1}\gamma_{p+2}\cdots\gamma_k}_{(**)} \times 10^{n-p}
\end{align*}

$(**)$: Podría ser basura.

::: {.callout-note title="Notación"}

\begin{align*}
\Delta_x \;&=\; |x - x^*|.\\
\delta_x \;&=\; \frac{|x - x^*|}{|x|} \;=\; \frac{\Delta_x}{|x|}.
\end{align*}
:::

---

::: {.callout-note title="Teorema"}


Si $x = x_1 + x_2 + \cdots + x_n$ y $x^* = x_1^* + x_2^* + \cdots + x_n^*$ con $x_i \geq 0$ entonces:

\begin{align*}
\Delta_x \;&\leq\; \sum_{i=1}^n \Delta_{x_i}. \\
\delta_x \;&\leq\; \max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\}.
\end{align*}
:::

::: {.callout-caution collapse="true" title="Prueba"}

\begin{align*}
\Delta_x &= |x - x^*|
= \left| \sum_{i=1}^n x_i - \sum_{i=1}^n x_i^* \right|
\leq \sum_{i=1}^n |x_i - x_i^*|
= \sum_{i=1}^n \Delta_{x_i}.
\end{align*}


\begin{align*}
\text{Sabemos que } \quad
\delta_x &= \frac{\Delta_x}{|x|}
\leq \frac{\Delta_{x_1} + \Delta_{x_2} + \cdots + \Delta_{x_n}}
{|x_1 + x_2 + \cdots + x_n|},\\[1ex]
\delta_{x_i} &= \frac{\Delta_{x_i}}{|x_i|}
\;\Rightarrow\; \Delta_{x_i} = \delta_{x_i}\,|x_i|,\\[2ex]
\Rightarrow \quad
\delta_x &\leq \frac{|x_1|\delta_{x_1} + |x_2|\delta_{x_2} + \cdots + |x_n|\delta_{x_n}}
{|x_1 + x_2 + \cdots + x_n|}\\[2ex]
&\leq
\frac{\max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\}\, (|x_1| + |x_2| + \cdots + |x_n|)}
{|x_1 + x_2 + \cdots + x_n|}\\[2ex]
&\leq \max\{\delta_{x_1}, \delta_{x_2}, \ldots, \delta_{x_n}\},
\qquad \text{pues } x_i \ge 0 \ \forall i .
\end{align*}

:::

::: {.callout-important collapse="true" title="Observación"}


Se debe evitar la pérdida de dígitos significativos:

Por ejemplo, al evaluar:  

$$
f(x) = 1 - \cos(x),
$$  

con $x$ cercano a $0$ se producirá una pérdida de dígitos significativos.  
Esto se puede evitar racionalizando, como sigue:

$$
f(x) = 1 - \cos(x) \;=\; \frac{\sin^{2}(x)}{1 + \cos(x)}.
$$

:::

### Algoritmos y convergencia

#### ¿Qué es un algoritmo?

::: {.callout-note title="Definición: Algoritmo"}

Un algoritmo es un procedimiento que describe, sin ninguna ambigüedad, una sucesión finita de pasos a realizar en orden específico, con el propósito de resolver un problema.

:::

**Características:**

- Finito.  
- Definido (no ambiguo).  
- Entrada.  
- Salida.  
- Efectivo.  
- Eficiente.  

Para representar las instrucciones utilizaremos pseudocódigo.

::: {.callout-tip collapse="true" title="Ejemplo"}


Para calcular  
$$
\sum_{k=a}^{\infty} f(x,k)
$$
tenemos:

**Entrada:** $\varepsilon, f, a, x$.  

**Salida:** Valor aproximado de  
$$
\sum_{k=a}^{\infty} f(x,k).
$$

---

```{python}
#| eval: false
#| message: false
k <- a
s <- 0
T <- f(x,k)
while |T| < ε do
    s <- s + t
    t <- t * (f(x,k+1) / f(x,k))
    k <- k + 1
end while
return s
```
:::

::: {.callout-note title="Definición: Algoritmos Estables"}

Un algoritmo se dice **estable** si pequeños cambios en la entrada producen pequeños cambios en la salida.  
En caso contrario, es decir, pequeños cambios en la entrada producen grandes cambios en la salida, entonces el algoritmo se dice **inestable (caótico)**.
:::

::: {.callout-note title="Notación"}

\begin{align*}
E \;&=\; \text{Error inicial.}\\
E_n \;&=\; \text{Error luego de }n\text{ pasos.}
\end{align*}
:::

::: {.callout-note title="Definición: Crecimiento lineal del error "}

Si  
$$|E_n| = CnE,$$  
con $C$ constante, entonces el crecimiento del error es **lineal**.  
:::

::: {.callout-note title="Definición: Crecimiento exponencial del error  "}

Si  
$$|E_n| = K^n E, \quad K > 1,$$  
entonces el crecimiento del error es **exponencial**.  
:::

::: {.callout-important collapse="true" title="Observación"}
- Crecimiento del error lineal $\;\Leftrightarrow\;$ estable.  
- Crecimiento del error exponencial $\;\Leftrightarrow\;$ inestable.  
:::

::: {.callout-note title="Definición: Rapidez de convergencia"}

Sea $\{ \alpha_n \}_{n \in \mathbb{N}}$ una sucesión que converge a $\alpha$, se dice que $\{ \alpha_n \}_{n \in \mathbb{N}}$ converge con una rapidez $\mathcal{O}(\beta_n)$, donde $\{ \beta_n \}_{n \in \mathbb{N}}$ es otra sucesión ($\beta_n \neq 0 \ \forall n$) si:  

$$
\frac{|\alpha_n - \alpha|}{|\beta_n|} < K
$$  

para $n$ suficientemente grande y $K$ constante que no depende de $n$.  
:::

::: {.callout-note title="Notación"}

$$
\alpha_n = \alpha + \mathcal{O}(\beta_n).
$$  

$$
\alpha_n \;\to\; \alpha \;\;\; \text{con rapidez } \mathcal{O}(\beta_n).
$$  
:::

::: {.callout-note title="Observación"}
$$
\frac{|\alpha_n - \alpha|}{|\beta_n|} < K 
\;\;\;\Longleftrightarrow\;\;\;
- K \beta_n + \alpha < \alpha_n < K \beta_n + \alpha.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Si $\alpha_n = \dfrac{n+3}{n^3}$, entonces  

$$
\alpha_n = 0 + \mathcal{O}\!\left(\frac{1}{n^2}\right)
$$  

pues:  

$$
\left| \frac{\alpha_n - \alpha}{\beta_n} \right|
= \left| \frac{\tfrac{n+3}{n^3} - 0}{\tfrac{1}{n^2}} \right|
= \left| \frac{n^3 + 3n^2}{n^3} \right|
= \left| 1 + \frac{3}{n} \right|
\leq 4, \quad \text{si } n \to \infty.
$$  

Es decir, $\dfrac{n+3}{n^3}$ converge a $0$ tan rápido como $\dfrac{1}{n^2}$ converge a $0$.  
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Si 
$$
\alpha_n = \frac{\sin(n)}{n},
$$  
entonces  
$$
\alpha_n = 0 + \mathcal{O}\!\left(\frac{1}{n}\right), \quad \text{cuando } n \to \infty.
$$
:::

::: {.callout-note title="Observación: Velocidad de convergencia de los ciclos  "}

1\. $\texttt{For[i=1, i++, i<=N, \ldots]}$

tiene una velocidad de convergencia $\mathcal{O}(N)$.  

2\. $\texttt{For[i=1, i++, i<=N,} \quad$

  $\quad\texttt{For[j=1, j++, j<=M, \ldots]]}$
   
tiene una velocidad de convergencia $\mathcal{O}(N^2)$.  

3\. $\texttt{For[i=1, i++, i<=N,} \quad$

  $\qquad\texttt{For[j=1, j++, j<=M,} \quad$
   
  $\qquad\qquad\texttt{For[k=1, k++, k<=L, \ldots]]]}$
      
tiene una velocidad de convergencia $\mathcal{O}(N^3)$.  
:::

### Origen del Error

#### Tipos de Error

Existen dos tipos de error: El error en los datos y el error computacional

![Tipos de Error](Imagenes/Grafico Error.png){width=400px fig-align="center"}

::: {.callout-note title="Definición: Propagación del error"}

Sea $a \in \mathbb{R}$ y sea $\hat{a}$ una aproximación de $a$ y $f$ una función o procedimiento,  
entonces 

$$
f(\hat{a}) - f(a)
$$  

se llama **propagación del error** o **error propagado**.
:::
Esto se ilustra en el siguiente gráfico

AGRAGAR GRAFICO ANDREY

::: {.callout-tip collapse="true" title="Ejemplo"}


Suponga que se desea calcular $c$ en el siguiente triángulo:

![](Imagenes/Triangulo.png){width=300px fig-align="center"}

Supongamos que $a$ tiene un error y usamos $100.1$.  
Entonces el error se propaga como se muestra en la siguiente tabla:

| Expresión | Exacto | Aproximado | Error relativo |
|-----------|--------|-------------|----------------|
| $a$ | $100$ | $100.1$ | $0.1 \,\%$ |
| $b-a$ | $1$ | $0.9$ | $-10 \,\%$ |
| $(b-a)^2$ | $1$ | $0.81$ | $-19 \,\%$ |
| $4ab \sin^2\!\left(\tfrac{\gamma}{2}\right)$ | $3.0765\ldots$ | $3.0796\ldots$ | $0.1 \,\%$ |
| $(b-a)^2 + 4ab \sin^2\!\left(\tfrac{\gamma}{2}\right)$ | $4.0765\ldots$ | $3.8896\ldots$ | $-4.6 \,\%$ |
| $c$ | $2.0190\ldots$ | $1.9722\ldots$ | $-2.3 \,\%$ |
| **Error relativo inicial:** | $0.1 \,\%$  | **Error relativo final:** | $-2.3 \,\%$ |

: {tbl-colwidths="[35,20,25,20]"}

:::

::: {.callout-tip collapse="true" title="Ejemplo: Inestabilidad Numérica"}


Suponga que se desea calcular  

$$
z = 1.000 - \frac{1.208}{x}
$$  

en una computadora que solamente utiliza 4 dígitos, con $x = 1.209$.

**Algoritmo 1.**  
Calcule primero $y := \dfrac{1.208}{x}$ y luego $z := 1.000 - y$  

![](Imagenes/1 graph.png){width=350px fig-align="center"}

**Algoritmo 2.**  
Calcule primero $y := x - 1.208$ y luego $z := \dfrac{y}{x}$  

![](Imagenes/2 graph.png){width=350px fig-align="center"}
:::

#### Principio Básico:

Si es posible, evite operaciones sensibles con operandos contaminados por la propagación del error.

::: {.callout-tip collapse="true" title="Ejemplo: Raíces cuadráticas y propagación del error"}

Usualmente, para calcular las raíces de una ecuación cuadrática se usa:

$$
x_{1,2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},
$$

pero si $b > 0$ es mejor usar:

$$
x_1 = \frac{-2c}{-b - \sqrt{b^2 - 4ac}},
$$

y si $b < 0$ es mejor usar:

$$
x_2 = \frac{2c}{-b + \sqrt{b^2 - 4ac}}.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo: Evitar cancelación numérica"}

Otros ejemplos que evitan problemas de cancelación cuando $x \approx y$ son:

\begin{align*}
x^2 - y^2 \;\;&\longrightarrow\;\; (x-y)(x+y),\\
\cos(x) - 1 \;\;&\longrightarrow\;\; 2 \sin^2\!\left(\tfrac{x}{2}\right), \quad \text{para } x \to 0,\\
\ln(x) - \ln(y) = \ln\!\left(\tfrac{x}{y}\right)&\longrightarrow\; 2 \tanh^{-1}\!\left(\tfrac{x-y}{x+y}\right),\\
e^x - e^y \;\;&\longrightarrow\;\; 2 \sinh\!\left(\tfrac{x-y}{2}\right) e^{\tfrac{x+y}{2}}.
\end{align*}
:::  

# Elementos de análisis funcional

## Elementos de Análisis Funcional para Análisis Numérico

### Espacios Normados  

::: {.callout-note title="Definición: Espacio normado"}

Sea $X$ un espacio vectorial complejo (o real).  
Una función $\|\cdot\| : X \to \mathbb{R}$ con las siguientes propiedades:

1. $\|x\| \geq 0,$  
2. $\|x\| = 0$ si y solamente si $x = 0,$  
3. $\|\alpha x\| = |\alpha| \, \|x\|,$  
4. $\|x + y\| \leq \|x\| + \|y\|,$  

para todo $x, y \in X$ y para todo $\alpha \in \mathbb{C}$ (o $\mathbb{R}$),  
se llama **norma** en $X$.  

El espacio vectorial $X$ provisto de una norma se llama **espacio normado**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Algunas normas de $\mathbb{R}^n$ y $\mathbb{C}^n$ son:

$$
\|x\|_1 := \sum_{i=1}^n |x_i|, 
\qquad
\|x\|_2 := \left( \sum_{i=1}^n |x_i|^2 \right)^{1/2}, 
\qquad
\|x\|_\infty := \max\limits_{i=1,2,\ldots,n} |x_i|,
$$

para $x = (x_1, x_2, \ldots, x_n)^t$.

Las normas anteriores son conocidas como las normas $\ell_1$, $\ell_2$ y $\ell_\infty$ respectivamente.  
Las tres son casos particulares de la **norma $\ell_p$**:

$$
\|x\|_p := \left( \sum_{i=1}^n |x_i|^p \right)^{1/p},
\qquad p \geq 1.
$$

La norma $\ell_\infty$ es el límite de $\ell_p$ cuando $p \to \infty$.
:::

::: {.callout-note title="Proposición"}

Para toda norma se tiene la **segunda desigualdad triangular**:

$$
\big| \|x\| - \|y\| \big| \leq \|x - y\|, 
$$

para todo $x, y \in X$.
:::

::: {.callout-caution collapse="true" collapsed="true" title="Prueba"}

De la desigualdad triangular se tiene que

$$
\|x\| = \|x - y + y\| \leq \|x - y\| + \|y\|,
$$

por lo tanto

$$
\|x\| - \|y\| \leq \|x - y\|.
$$

Análogamente, cambiando el rol de $x$ y $y$, se obtiene

$$
\|y\| - \|x\| \leq \|y - x\|,
$$

y por lo tanto se cumple la desigualdad.
:::

::: {.callout-note title="Definición: Norma"}

Para dos elementos $x,y$ en un espacio normado $X$ la norma $\|x-y\|$ se llama **distancia** entre $x$ y $y$.
:::

::: {.callout-note title="Definición: Sucesión Convergente"}

Una sucesión $(x_n)$ de elementos en un espacio normado $X$ se llama **convergente** si existe un elemento $x \in X$ tal que:

$$
\lim_{n \to \infty} \|x_n - x\| = 0,
$$

es decir, para todo $\varepsilon > 0$ existe un entero $N(\varepsilon)$ tal que $\|x_n - x\| < \varepsilon$ para todo $n > N(\varepsilon)$.  

El elemento $x$ es llamado el **límite** de la sucesión $(x_n)$ y se escribe:

$$
\lim_{n \to \infty} x_n = x.
$$

Si una sucesión no converge se llama **divergente**.
:::

::: {.callout-note title="Proposición"}

El límite de una sucesión convergente es **único**.
:::

::: {.callout-caution collapse="true" title="Prueba"}

*(Ejercicio)* $\blacksquare$
:::

::: {.callout-note title="Definición: Normas Equivalentes"}
Dos normas en un espacio vectorial se llaman **equivalentes** si tienen el mismo conjunto de sucesiones convergentes.
:::

::: {.callout-note title="Teorema"}

Dos normas $\|\cdot\|_{a}$ y $\|\cdot\|_{b}$ en un espacio vectorial $X$ son equivalentes si y solamente si existen números positivos $c$ y $C$ tal que:
$$
c \|x\|_{a} \;\leq\; \|x\|_{b} \;\leq\; C \|x\|_{a}, \quad \forall x \in X.
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sean las normas $\|\cdot\|_{a}$ y $\|\cdot\|_{b}$ equivalentes. Suponga que no existe $C>0$ tal que 
$\|x\|_{b} \leq C \|x\|_{a}$ para todo $x \in X$, entonces existe una sucesión $(x_{n})$ con $\|x_{n}\|_{a} = 1$ y $\|x_{n}\|_{b} \geq n^{2}$.  
Luego la sucesión $y_{n} := x_{n}/n$ converge a cero con respecto a $\|\cdot\|_{a}$ pero no con respecto a $\|\cdot\|_{b}$, porque $\|y_{n}\|_{b} \geq n$.  

Recíprocamente, si se tiene (2), entonces si $\|x_{n}-x\|_{a}\to 0$ es claro que $\|x_{n}-x\|_{b}\to 0$ y viceversa.  
$\blacksquare$
:::

::: {.callout-note title="Teorema"}

En un espacio vectorial de dimensión finita todas las normas son equivalentes.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Ejercicio. $\blacksquare$
:::

::: {.callout-note title="Definición: Subconjunto cerrado"}
- Un subconjunto $U$ de un espacio normado se llama **cerrado** si este contiene el límite de todas las sucesiones convergentes de $U$.  

- La **clausura** $\overline{U}$ de un conjunto $U$ de un espacio normado $X$ es el conjunto de todos los límites de sucesiones convergentes de $U$.  

- Un subconjunto $U$ de $X$ se llama **abierto** si su complemento $X \setminus U$ es cerrado.  

- Un conjunto $U$ se llama **denso** en otro conjunto $V$ si $V \subset \overline{U}$, es decir, si cada elemento de $V$ es el límite de una sucesión convergente de $U$.  

- Para cada $x_{0} \in X$ y $r > 0$ el conjunto  
  $$
  B[x_{0}, r] := \{x \in X \;:\; \|x - x_{0}\| \leq r\}
  $$  
  es cerrado y se llama la **bola cerrada** de radio $r$ y centro $x_{0}$.  

  El conjunto  
  $$
  B(x_{0}, r) := \{x \in X \;:\; \|x - x_{0}\| < r\}
  $$  
  es abierto y se llama la **bola abierta** de radio $r$ y centro $x_{0}$.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

- **Cerrado.** $[0,1] \subset \mathbb{R}$ es cerrado: toda sucesión convergente de puntos en $[0,1]$ converge a un punto en $[0,1]$.

- **Clausura.** Si $U = ]0,1[$, entonces $\overline{U} = [0,1]$.  
  Si $U = \mathbb{Q}$, entonces $\overline{U} = \mathbb{R}$.

- **Abierto.** $]0,1[ \subset \mathbb{R}$ es abierto porque su complemento es  
  $\mathbb{R} \setminus ]0,1[ = ]-\infty,0] \cup [1,\infty[$.

- **Denso en $V$.** $\mathbb{Q}$ es denso en $\mathbb{R}$ ($\overline{\mathbb{Q}} = \mathbb{R}$).  
  También $]0,1[$ es denso en $[0,1]$ pues $\overline{]0,1[} = [0,1]$.

- **Bolas (norma usual).**  
  En $\mathbb{R}$, con $x_0 = 2$, $r = 3$:  
  $$
  B[2,3] = \{ x \in \mathbb{R} : |x-2| \leq 3 \} = [-1,5],
  $$
  $$
  B(2,3) = \{ x \in \mathbb{R} : |x-2| < 3 \} = ]-1,5[.
  $$

  En $\mathbb{R}^2$, con $x_0 = (1,2)$, $r = 2$:  
  $$
  B[(1,2),2] = \{ (x,y) \in \mathbb{R}^2 : \sqrt{(x-1)^2+(y-2)^2} \leq 2 \},
  $$
  $$
  B((1,2),2] = \{ (x,y) \in \mathbb{R}^2 : \sqrt{(x-1)^2+(y-2)^2} < 2 \}.
  $$
:::

::: {.callout-note title="Definición"}
Un conjunto $U$ se llama **acotado** si existe un número positivo $C$ tal que  
$$
\|x\| < C \quad \text{para todo } x \in U.
$$
:::

::: {.callout-note title="Teorema"}

Toda sucesión acotada en un espacio normado de dimensión finita $X$ contiene una subsucesión convergente.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $u_{1}, u_{2}, \ldots, u_{n}$ una base de $X$ y sea $(x_{\nu})$ una sucesión acotada. Entonces se puede escribir:
$$
x_{\nu} = \sum_{j=1}^{n} \alpha_{j\nu} u_{j}.
$$

Como $x_{\nu}$ es una sucesión acotada y usando la norma
$$
\|x_{\nu}\|_{\infty B} := \max\limits_{j=1,2,\ldots,n} |\alpha_{j\nu}|,
$$
se tiene que cada una de las sucesiones $(\alpha_{j\nu})$ es acotada en $\mathbb{C}$ para cada $j=1,2,\ldots,n$.  

Por lo tanto, usando el teorema de Bolzano–Weierstrass se puede seleccionar una subsucesión $\alpha_{j\nu(\ell)} \to \alpha_{j}$ cuando $\ell \to \infty$ para cada $j=1,2,\ldots,n$.  

Esto implica que:
$$
x_{\nu(\ell)} \to \sum_{j=1}^{n} \alpha_{j} u_{j} \in X, \quad \text{cuando } \ell \to \infty.
$$
$\blacksquare$
:::

### Productos escalares  

::: {.callout-note title="Definición: Producto interno y espacio pre-Hilbert"}

Sea $X$ un espacio vectorial complejo (o real).  
Una función $\langle \cdot,\cdot \rangle : X \times X \to \mathbb{C}$ (o $\mathbb{R}$) con las siguientes propiedades:

1. $\langle x, x \rangle \ge 0,$
2. $\langle x, x \rangle = 0 \quad \text{si y sólo si } x = 0,$
3. $\langle x, y \rangle = \overline{\langle y, x \rangle},$
4. $\langle \alpha x + \beta y, z \rangle = \alpha \langle x, z \rangle + \beta \langle y, z \rangle, \quad \forall\, x,y,z \in X,\; \alpha,\beta \in \mathbb{C}\; (\text{o } \mathbb{R}),$

se llama **producto interno** en $X$.  
Un espacio vectorial $X$ provisto de un producto interno se llama **espacio pre-Hilbert**.
:::

::: {.callout-important collapse="true" title="Observación"}

Una consecuencia inmediata de 3. y 4. es la **antilinealidad**:
$$
\langle x, \alpha y + \beta z \rangle = \overline{\alpha} \langle x, y \rangle + \overline{\beta} \langle x, z \rangle.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Un ejemplo de producto interno en $\mathbb{C}^n$ (o $\mathbb{R}^n$) está dado por:
$$
\langle x, y \rangle := \sum_{i=1}^{n} x_i \overline{y_i},
$$
donde $x := (x_1, x_2, \ldots, x_n)^t$ y $y := (y_1, y_2, \ldots, y_n)^t$.
:::

::: {.callout-note title="Teorema"}

Para todo producto interno se tiene la desigualdad de Cauchy–Schwarz:
$$
|\langle x, y \rangle|^2 \leq \langle x, x \rangle \langle y, y \rangle,
$$
para todo $x, y \in X$. Además se tiene igualdad si para todo $x, y$ son linealmente dependientes.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Si $x = 0$ la desigualdad es trivial.  
Si $x \neq 0$, tome
$$
z = y - \frac{\langle y, x \rangle}{\|x\|^2}x,
$$
luego es claro que $\langle z, x \rangle = 0$ y que:
$$
0 \leq \|z\|^2
= \left\langle y - \frac{\langle y, x \rangle}{\|x\|^2}x , \; y - \frac{\langle y, x \rangle}{\|x\|^2}x \right\rangle
= \langle y, y \rangle - \frac{\langle y, x \rangle \langle x, y \rangle}{\|x\|^2}
= \|y\|^2 - \frac{|\langle x, y \rangle|^2}{\|x\|^2},
$$
de donde se tiene la desigualdad. Además se tiene igualdad si para todo $x, y$ son linealmente dependientes (ejercicio).
$\blacksquare$
:::

::: {.callout-note title="Teorema"}

Sea $X$ un espacio vectorial complejo (o real).  
Entonces la función:
$$
\|x\| := \langle x, x \rangle^{1/2}
$$
define una norma en $X$, es decir, un espacio pre–Hilbert es siempre un espacio normado.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Ejercicio (use la desigualdad de Cauchy–Schwarz para probar la desigualdad triangular).  
$\blacksquare$
:::

::: {.callout-note title="Definición: Elementos Ortogonales"}

- Dos elementos $x$ y $y$ de un espacio pre–Hilbert se llaman **ortogonales** si:
$$
\langle x, y \rangle = 0.
$$

- Dos subconjuntos $U$ y $V$ se llaman **ortogonales** si $\langle u, v \rangle = 0$ para todo $u \in U$ y $v \in V$.  

- Si dos elementos son ortogonales se denota $x \perp y$ y si dos conjuntos son ortogonales se denota $U \perp V$.  

- Un subconjunto $U \subseteq X$ se llama un **sistema ortogonal** si $\langle x, y \rangle = 0$ para todo $x, y \in U$ con $x \neq y$.  

- Un sistema ortogonal $U$ de $X$ se llama **ortonormal** si $\|x\| = 1$ para todo $x \in U$.
:::

::: {.callout-note title="Teorema"}

Los elementos de un sistema ortogonal son linealmente independientes.
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $\{q_{1},q_{2},\ldots,q_{n}\}$ un sistema ortogonal.  
Si 
$$
\sum_{k=1}^{n} \alpha_{k} q_{k} = 0,
$$
y se multiplica a ambos lados por $q_{j}$, es inmediato que $\alpha_{j} = 0$ para todo $j=1,2,\ldots,n$.  
$\blacksquare$
:::

::: {.callout-note title="Teorema  [Gram–Schmidt]"}

Sea $\{u_{0},u_{1},\ldots\}$ un conjunto finito o numerable de elementos linealmente independientes de un espacio de pre–Hilbert.  
Entonces existe un sistema ortogonal único $\{q_{0},q_{1},\ldots\}$ de la forma:

$$
q_{n} = u_{n} + r_{n}, \quad \text{para } n = 0,1,\ldots,
\tag{3}
$$

con $r_{0}=0$ y $r_{n} \in \operatorname{gen}\{u_{0},u_{1},\ldots,u_{n-1}\}$ para $n=1,2,\ldots$.  
Además se tiene que:

$$
\operatorname{gen}\{u_{0},u_{1},\ldots,u_{n}\} 
= 
\operatorname{gen}\{q_{0},q_{1},\ldots,q_{n}\},
\quad \text{para } n=0,1,\ldots
\tag{4}
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}


Asumamos que hemos construido elementos de la forma (3) con la propiedad (4) hasta $q_{n-1}$.  
Por la propiedad (4) los elementos $\{q_{0},q_{1},\ldots,q_{n-1}\}$ son linealmente independientes y por lo tanto $\|q_{k}\|\neq 0$ para $k=0,1,\ldots,n-1$.  
Por lo tanto:

$$
q_{n} = u_{n} - \sum_{k=0}^{n-1}\frac{\langle u_{n},q_{k}\rangle}{\langle q_{k},q_{k}\rangle}q_{k},
$$

está bien definido, y usando la hipótesis de inducción es fácil notar que  

$$
\langle q_{n},q_{k}\rangle = 0, \quad \text{para } k=0,1,\ldots,n-1.
$$

Es claro que  

$$
r_{n}=\sum_{k=0}^{n-1}\frac{\langle u_{n},q_{k}\rangle}{\langle q_{k},q_{k}\rangle}q_{k}
\;\;\in\;\;
\operatorname{gen}\{q_{0},q_{1},\ldots,q_{n-1}\}
=
\operatorname{gen}\{u_{0},u_{1},\ldots,u_{n-1}\}.
$$

La unicidad queda de ejercicio al lector. $\blacksquare$
:::

### Completitud  


::: {.callout-note title="Definición: Sucesión de Cauchy"}

Una sucesión de elementos en un espacio normado $X$ se llama **sucesión de Cauchy** si para todo $\varepsilon > 0$ existe un $N(\varepsilon) \in \mathbb{N}$ tal que:

$$
\|x_n - x_m\| < \varepsilon,
$$

para todo $n, m \geq N(\varepsilon)$, es decir, si  

$$
\lim_{n \to \infty, \; m \to \infty} \|x_n - x_m\| = 0.
$$
:::

::: {.callout-note title="Teorema"}

Toda sucesión convergente es de Cauchy
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $x_n \to x$ cuando $n \to \infty$. Entonces, para todo $\varepsilon > 0$ existe un $N(\varepsilon) \in \mathbb{N}$ tal que  

$$
\|x_n - x\| < \tfrac{\varepsilon}{2}, \quad \text{para todo } n \geq N(\varepsilon).
$$

Ahora, usando la desigualdad triangular:  

$$
\|x_n - x_m\| 
= \|x_n - x + x - x_m\|
\leq \|x_n - x\| + \|x_m - x\| < \varepsilon,
$$

para todo $n, m \geq N(\varepsilon)$. $\;\blacksquare$
:::

::: {.callout-important collapse="true" title="Observación"}

El recíproco del teorema anterior no es válido en general, por esto tiene sentido dar la siguiente definición.
:::

::: {.callout-note title="Definición: Subconjunto Completo"}
Un subconjunto $U$ de un espacio normado $X$ se llama **completo** si toda sucesión de Cauchy de elementos de $U$ converge a un elemento en $U$.  

Un espacio normado completo se llama **espacio de Banach**.  

Un espacio pre–Hilbert se llama **espacio de Hilbert** si este es un espacio completo.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

El espacio vectorial $C[a,b]$ provisto con la norma:

$$
\|f\|_\infty := \max\limits_{x \in [a,b]} |f(x)|
$$

es un espacio de Banach.

::: {.callout-caution collapse="true" title="Prueba"}

(Ejercicio) $\blacksquare$
:::

:::

::: {.callout-tip collapse="true" title="Ejemplo"}

El espacio vectorial $C[a,b]$ provisto con la norma $L_1$:

$$
\|f\|_1 := \int_a^b |f(x)| \, dx
$$

NO es un espacio de Banach.

::: {.callout-caution collapse="true" title="Prueba"}

Es evidente que $\|f\|_1$ es una norma.  
Sin pérdida de generalidad se toma $[a,b] = [0,2]$ y se escoge:

$$
f_n(x) := 
\begin{cases}
x^n & \text{si } 0 \leq x \leq 1, \\
1   & \text{si } 1 < x \leq 2.
\end{cases}
$$

Para todo $m > n$ se tiene que:

$$
\| f_n - f_m \|_1 
= \int_0^1 \big( x^n - x^m \big) dx 
= \frac{1}{n+1} + \frac{1}{m+1} \to 0
\quad \text{cuando } n,m \to \infty,
$$

por lo tanto $(f_n)$ es una sucesión de Cauchy. 

Ahora, supongamos que la sucesión $(f_n)$ converge a una función continua $f$ con respecto a la norma $L_1$, es decir:

$$
\| f_n - f \|_1 \to 0 \quad \text{cuando } n \to \infty.
$$

Entonces:

$$
\int_0^1 |f(x)| \, dx 
\leq \int_0^1 |f(x) - x^n| \, dx + \int_0^1 x^n \, dx 
\leq \| f - f_n \|_1 + \frac{1}{n+1} \to 0
$$

cuando $n \to \infty$, de donde $f(x) = 0$ para $0 \leq x \leq 1$.

Además se tiene que

$$
\int_1^2 |f(x)-1| \, dx 
= \int_1^2 |f(x)-f_n(x)| \, dx 
\leq \| f - f_n \|_1 \to 0 \quad \text{cuando } n \to \infty,
$$

esto implica que $f(x) = 1$ para $1 \leq x \leq 2$.  
Por lo tanto $f$ no es continua, lo cual es una contradicción. $\blacksquare$
:::

:::

::: {.callout-tip collapse="true" title="Ejemplo"}

El espacio vectorial $C[a,b]$ provisto con la norma $L_2$:

$$
\| f \|_2 := \left( \int_a^b |f(x)|^2 \, dx \right)^{2}
$$

NO es un espacio de Banach.

::: {.callout-caution collapse="true" title="Prueba"}

Hay que probar que el espacio vectorial $C[a,b]$ normado provisto con la norma $L_2$ **NO** es completo. Es decir,
hay que encontrar una sucesión de Cauchy de elementos de $U$ que **NO** converge a un elemento en $U$, siendo $U$ un subconjunto del espacio vectorial $C[a,b]$

Sin pérdida de generalidad se toma $[a,b] = [0,2]$ y se escoge:

$$
f_n(x) := 
\begin{cases}
x^n & \text{si } 0 \leq x \leq 1, \\
1   & \text{si } 1 < x \leq 2.
\end{cases}
$$

Para todo $m > n$ se tiene que:

\begin{align*}
\| f_n - f_m \|_1
&= \left( \int_0^2 |f_n(x) - f_m(x)|^2 \, dx \right)^2\\[1ex]
&= \left( \int_0^1 |x^n - x^m|^2 \, dx \right)^2\\[1ex]
&= \left( \int_0^1 \big(x^{2n} - 2x^{n+m} + x^{2m}\big) \, dx \right)^2\\[1ex]
&= \left( \frac{1}{2n+1} - \frac{2}{n+m+1} + \frac{1}{2m+1} \right)^2 \to 0
\quad \text{cuando } n,m \to \infty,
\end{align*}

por lo tanto $(f_n)$ es una sucesión de Cauchy. 

Ahora, supongamos que la sucesión $(f_n)$ converge a una función continua $f$ con respecto a la norma $L_1$, es decir:

$$
\| f_n - f \|_1 \to 0 \quad \text{cuando } n \to \infty.
$$
Entonces,

\begin{align*}
\|f\|_{1} 
  &= \|(f-x^{n})+x^{n}\|_{1}
\\
  &\le \|f-x^{n}\|_{1} + \|x^{n}\|_{1}
  \qquad\text{(desigualdad triangular)}
\\
  &= \|f-f_{n}\|_{1} + \left(\int_{0}^{1} |x^{n}|^{2}\,dx\right)^{2}
\\
  &= \|f-f_{n}\|_{1} + \left(\int_{0}^{1} x^{2n}\,dx\right)^{2}
\\
  &= \|f-f_{n}\|_{1} + \left(\frac{1}{2n+1}\right)^{2}
  \xrightarrow[n\to\infty]{} 0
\end{align*}
de donde $f(x) = 0$ para $0 \leq x \leq 1$.

Vea además que 

\begin{align*}
\Biggl(\int_{1}^{2}\!\bigl|f(x)-1\bigr|^{2}\,dx\Biggr)^{2} 
&= \Biggl(\int_{1}^{2}\!\bigl|f(x)-f_n(x)\bigr|^{2}\,dx\Biggr)^{2} 
\qquad\text{(pues $f_n(x)=1$ en $(1,2]$)}\\[4ex]
&\le \Biggl(\int_{0}^{2}\!\bigl|f(x)-f_n(x)\bigr|^{2}\,dx\Biggr)^{2} 
= \,\|f-f_n\|_{1}\;\xrightarrow[n\to\infty]{}\;0 .
\end{align*}

esto implica que $f(x) = 1$ para $1 \leq x \leq 2$.  
Por lo tanto $f$ no es continua, lo cual es una contradicción. $\qquad\blacksquare$
:::
:::

::: {.callout-note title="Teorema"}

Todo espacio normado de dimensión finita es un espacio de Banach.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $X$ un espacio normado con base $u_1, u_2, \ldots, u_n$ y sea $(x_\nu)$ una sucesión de Cauchy en $X$.  
Se puede escribir:  

$$
x_\nu = \sum_{j=1}^n \alpha_{j\nu} u_j
$$

usando el Teorema 1 se tiene que existe un $C > 0$ tal que:  

$$
\max\limits_{j=1,2,\ldots,n} |\alpha_{j\nu} - \alpha_{j\mu}| \leq C \, \| x_\nu - x_\mu \|
$$

para todo $\nu, \mu \in \mathbb{N}$.

Por lo tanto $(\alpha_{j\nu})$ es una sucesión de Cauchy en $\mathbb{C}$, entonces existen $\alpha_1, \alpha_2, \ldots, \alpha_n$ tales que $\alpha_{j\nu} \to \alpha_j$ cuando $\nu \to \infty$ para cada $j = 1,2,\ldots,n$.  
Por lo tanto $(x_\nu)$ converge a:  

$$
x_\nu \to x := \sum_{j=1}^n \alpha_j u_j \in X \quad \text{cuando } \nu \to \infty.
$$
$\blacksquare$
:::

### El teorema de punto fijo de Banach  

::: {.callout-note title="Definición: Contracción  "}

Sea $U$ un subconjunto de un espacio normado $X$.  
Un operador (una función) $A: U \to X$ se llama **una contracción** si existe una constante $q \in [0,1[$ tal que:  

$$
\|Ax - Ay\| \leq q \|x - y\|, \quad \forall\, x,y \in U.
$$

:::

::: {.callout-note title="Definición: Continuidad de un operador"}

Sea $U$ un subconjunto de un espacio normado $X$, y sea $Y$ un espacio normado.  

- Una función $A: U \to Y$ se llama **continua en $x \in U$** si para toda sucesión $(x_n)$ de $U$ tal que $\lim\limits_{n \to \infty} x_n = x$, se cumple que  

$$
\lim\limits_{n \to \infty} Ax_n = Ax.
$$

- Un operador $A: U \to Y$ se llama **continuo** si es continuo en $x$ para todo $x \in U$.
:::

::: {.callout-note title="Proposición"}

Toda contracción es un operador continuo.
:::

::: {.callout-caution collapse="true" title="Prueba"}

La prueba es evidente, puesto que si $\|x_n - x\| \to 0$ cuando $n \to \infty$ entonces  
$\|Ax_n - Ax\| \to 0$ cuando $n \to \infty$ ya que  

$$
\|Ax_n - Ax\| \leq q \|x_n - x\| \to 0 \quad \text{cuando } n \to \infty.
$$

$\blacksquare$
:::

::: {.callout-note title="Definición: Operador Lipschitz"}
Un operador $A : U \to X$ se llama *Lipschitz* con constante de Lipschitz $L$ si existe una constante positiva $L$ tal que:  

$$
\|Ax - Ay\| \leq L \|x - y\|
$$

para todo $x, y \in U$.  
Es decir, una contracción es un operador *Lipschitz* con constante menor que uno.
:::

::: {.callout-note title="Definición: Operador lineal"}
Un operador $A : X \to Y$ donde $X$ y $Y$ son espacios normados se llama **lineal** si:  

$$
A(\alpha x + \beta y) = \alpha Ax + \beta Ay
$$  

para todo $x, y \in X$ y $\alpha, \beta \in \mathbb{C}$ (o $\mathbb{R}$).
:::

::: {.callout-note title="Teorema"}

Un operador lineal es continuo si y solo si es continuo en un elemento.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $A : X \to Y$ un operador lineal continuo en $x_0 \in X$.  
Entonces, para todo $x \in X$ y para toda sucesión $(x_n) \to x$ cuando $n \to \infty$, se tiene que:

$$
Ax_n = A(x_n - x + x_0) + A(x - x_0) \;\to\; A(x_0) + A(x - x_0) = Ax,
\quad \text{cuando } n \to \infty.
$$  

::: {.callout-important collapse="true" title="Observación"}

Como $x_n - x + x_0 \to x_0$, entonces se puede aplicar el límite dentro de la expresión.
:::

$\blacksquare$
:::

::: {.callout-note title="Definición: Punto fijo"}

Un elemento $x \in X$ (espacio normado) se llama **punto fijo** de un operador 
$A : U \subseteq X \to X$ si:
$$
Ax = x.
$$
:::

::: {.callout-note title="Teorema"}

Toda contracción tiene a lo más un único punto fijo.
:::

::: {.callout-caution collapse="true" title="Prueba"}
Supongamos que $x$ y $y$ son puntos fijos de una contracción $A$, entonces
$$
0 \neq \|x - y\| = \|Ax - Ay\| \leq q \|x - y\|,
$$
lo que implica que $q \geq 1$, lo cual es una contradicción. $\blacksquare$
:::

::: {.callout-note title="Teorema: (Banach)"}

Sea $U$ un subconjunto completo de un espacio normado $X$ y sea $A : U \to U$ una contracción. Entonces $A$ tiene un punto fijo único.
:::

::: {.callout-caution collapse="true" title="Prueba"}

Sea $x_0 \in U$ entonces definimos recursivamente la siguiente sucesión en $U$:
$$
x_{n+1} := A x_n, \quad \text{para } n = 0,1,2,\ldots
$$

De donde se tiene que:
$$
\|x_{n+1} - x_n\| = \|Ax_n - Ax_{n-1}\| \leq q \|x_n - x_{n-1}\|,
$$

luego, por inducción se deduce que:
$$
\|x_{n+1} - x_n\| \leq q^n \|x_1 - x_0\|, \quad \text{para } n = 0,1,2,\ldots
$$

Por lo tanto para $m > n$ se tiene que:
$$
\|x_n - x_m\| \leq \|x_n - x_{n+1}\| + \|x_{n+1} - x_{n+2}\| + \cdots + \|x_{m-1} - x_m\|
$$
$$
\leq (q^n + q^{n+1} + \cdots + q^{m-1}) \|x_1 - x_0\|
$$
$$
\leq \frac{q^n}{1-q} \|x_1 - x_0\|.
$$

Como $q^n \to 0$ cuando $n \to \infty$, entonces $(x_n)$ es una sucesión de Cauchy y como $U$ es completo entonces existe $x \in U$ tal que $x_n \to x$ cuando $n \to \infty$. Finalmente, por la continuidad de $A$ se tiene que:
$$
x = \lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} A x_n = A x.
$$

La unicidad se tiene por el teorema anterior. $\blacksquare$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Pruebe que la función $f(x) = \dfrac{x^2 - 2x}{6}, \; x \in [-1,1]$ tiene un punto fijo único en $[-1,1]$.

**Solución:** Como $\mathbb{R}$ es un espacio de Banach con la norma valor absoluto, se debe probar que $f(x) \in [-1,1] \quad \forall x \in [-1,1]$.

Como 
$$
f'(x) = \tfrac{1}{6}(2x-2) = \dfrac{x-1}{3} = 0 \iff x=1
$$ 
se tiene que los máximos y mínimos posibles están en $x=-1$ o $x=1$, así el valor máximo es 
$$
f(-1)=\tfrac{1}{2}
$$ 
y el valor mínimo es 
$$
f(1)=-\tfrac{1}{6}.
$$  

Por lo que para todo $x \in [-1,1]$ se tiene que $f(x) \in [-1,1]$, o sea que $f$ tiene un punto fijo en $[-1,1]$.

Para probar la unicidad, por el teorema del valor medio la constante $L$ de Lipschitz está dada por:
$$
L = \max\limits_{x \in [-1,1]} |f'(x)| = \max\limits_{x \in [-1,1]} \left|\frac{x-1}{3}\right| 
= \left|\frac{-1-1}{3}\right| = \frac{2}{3} < 1.
$$

Por lo tanto $f$ es una contracción en $[-1,1]$, luego la unicidad se tiene por el teorema anterior. $\blacksquare$
:::

### El teorema de la mejor aproximación  

::: {.callout-note title="Definición: Mejor aproximación"}

Sea $U$ un subconjunto de un espacio normado $X$ y sea $w \in X$.  
Un elemento $v \in U$ se llama **la mejor aproximación a $w$ con respecto a $U$** si:

$$
\|w - v\| \;\leq\; \inf_{u \in U} \|w - u\|,
$$

es decir, $v$ es el elemento en $U$ más cercano a $w$.
:::

::: {.callout-note title="Teorema"}

Sea $U$ un subespacio de dimensión finita de un espacio normado $X$.  
Entonces, para todo elemento $w \in X$, existe una mejor aproximación con respecto a $U$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $w \in X$ y escojamos una sucesión $(u_n)$ tal que $u_n \in U$ y satisfaga lo siguiente:

$$
\lVert w - u_n \rVert \to d := \inf_{u \in U} \lVert w - u \rVert \quad \text{cuando } n \to \infty.
$$

Como 
$$
\lVert u_n \rVert \leq \lVert w - u_n \rVert + \lVert w \rVert
$$ 
entonces $(u_n)$ es una sucesión acotada.  

Por el Teorema 3 la sucesión $(u_n)$ contiene una subsucesión convergente $(u_{n(\ell)})$ con límite $v \in U$.  

Entonces:

$$
\lVert w - v \rVert = \lim_{\ell \to \infty} \lVert w - u_{n(\ell)} \rVert = d,
$$

con lo que se prueba el teorema. $\blacksquare$
:::

::: {.callout-note title="Teorema"}

Sea $U$ un subespacio vectorial de un espacio de pre-Hilbert $X$.  
Un elemento $v$ es la mejor aproximación a $w \in X$ con respecto a $U$ si y solo si:  

$$
\langle w - v, u \rangle = 0
$$  

para todo $u \in U$.  

Es decir, si y solamente si $w - v \perp U$.  
Además, para cada $w \in X$ existe a lo más una única mejor aproximación con respecto a $U$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


(Ejercicio) $\blacksquare$
:::

::: {.callout-note title="Definición: Operador Acotado"}

Un operador $A : X \to Y$ donde $X$ y $Y$ son espacios normados, se llama **acotado** si existe un número positivo $C$ tal que:  

$$
\lVert Ax \rVert \leq C \lVert x \rVert
$$  

para todo $x \in X$.
:::

::: {.callout-note title="Teorema"}

Un operador lineal $A : X \to Y$ es acotado si y solamente si:  

$$
\lVert A \rVert := \sup_{\lVert x \rVert = 1} \lVert Ax \rVert < \infty .
$$  

El número $\lVert A \rVert$ es la más pequeña cota para $A$ y se llama la **norma de $A$**.
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

Asuma $A$ es acotado con una cota $C$. Entonces  

$$
\sup_{\lVert x \rVert = 1} \lVert Ax \rVert < C < \infty ,
$$  

y entonces $\lVert A \rVert$ es menor o igual que cualquier otra cota para $A$.  

Inversamente, si $\lVert A \rVert < \infty$, entonces usando la linealidad de la norma se tiene que:  

$$
\lVert Ax \rVert 
= \left\lVert A\!\left(\frac{x}{\lVert x \rVert}\right) \right\rVert \lVert x \rVert 
\leq \lVert A \rVert \, \lVert x \rVert ,
$$  

para todo $x \neq 0$, por lo tanto $A$ es acotado con cota $C = \lVert A \rVert$.  

$\blacksquare$
:::

::: {.callout-note title="Teorema"}

Sea $U$ un subespacio vectorial completo de un espacio pre-Hilbert $X$. Entonces para cada elemento $w \in X$ existe una única mejor aproximación con respecto a $U$.  

- El operador $P : X \to U$ que le asigna a $w \in X$ su mejor aproximación es un operador lineal acotado con las siguientes propiedades:  

$$
P^2 = P 
\quad \text{y} \quad 
\lVert P \rVert = 1 .
$$  

- Este operador se conoce como la **proyección ortogonal** de $X$ sobre $U$.
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

(ejercicio) $\blacksquare$
:::

::: {.callout-note title="Corolario"}

Sea $U$ un subespacio vectorial de dimensión finita de un espacio pre-Hilbert $X$ con base $u_{1}, u_{2}, \ldots, u_{n}$.  
Entonces la combinación lineal  

$$
v = \sum_{k=1}^{n} \alpha_{k} u_{k}
$$  

es la mejor aproximación para $w \in X$ con respecto a $U$ si y solamente si los coeficientes $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ satisfacen las *ecuaciones normales*:  

$$
\sum_{k=1}^{n} \alpha_{k} \langle u_{k}, u_{j} \rangle = \langle w, u_{j} \rangle, 
\quad \text{para } j = 1, 2, \ldots, n.
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

Es evidente que la ecuación (8) es equivalente a la ecuación (7).  
$\blacksquare$
:::

::: {.callout-note title="Corolario"}

Sea $U$ un subespacio vectorial de dimensión finita de un espacio pre-Hilbert $X$ con base ortonormal $u_{1}, u_{2}, \ldots, u_{n}$.  
Entonces la proyección ortogonal está dada por:  

$$
Pw = \sum_{k=1}^{n} \langle w, u_{k} \rangle u_{k}, \quad \text{con } w \in X.
$$
:::

::: {.callout-caution collapse="true" title="Prueba"}
  

Es evidente de (8) puesto que  

$$
\langle u_{k}, u_{j} \rangle =
\begin{cases}
0 & \text{si } k \neq j, \\
1 & \text{si } k = j,
\end{cases}
$$  

luego $\alpha_{k} = \langle w, u_{k} \rangle$ para $k = 1, 2, \ldots, n$.  
$\blacksquare$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}
  

Sea $P_{2}[0,1]$ un subespacio de dimensión finita del espacio pre–Hilbert $C[0,1]$ dotado del producto interno  

$$
\langle f,g \rangle := \int_{0}^{1} f(x)g(x)\,dx.
$$  

Es fácil verificar que  
$$
\mathcal{B} = \{\,1, \ \sqrt{3}(2x-1), \ \sqrt{5}(6x^{2}-6x+1)\,\}
$$  
es una base ortonormal de $P_{2}[0,1]$.  

Si tomamos $f(x) = e^{x}$, entonces la mejor aproximación de $f(x)$ en $P_{2}[0,1]$ es:  

$$
Pf = \langle e^{x},1 \rangle \cdot 1 
+ \langle e^{x}, \sqrt{3}(2x-1)\rangle \cdot \sqrt{3}(2x-1) 
+ \langle e^{x}, \sqrt{5}(6x^{2}-6x+1)\rangle \cdot \sqrt{5}(6x^{2}-6x+1),
$$  

es decir,  

$$
Pf = (e-1) + \sqrt{3}(3-e)\sqrt{3}(2x-1) + \sqrt{5}(7e-19)\sqrt{5}(6x^{2}-6x+1).
$$  

Por lo tanto,  

$$
Pf \approx 1.01 + 0.85x + 0.84x^{2}.
$$

```{r}
#| echo: false
#| fig-width: 7
#| fig-height: 5
#| fig-dpi: 300
#| fig-align: center
#| fig-cap: "Gráfico de $e^x$ y aproximación cuadrática"
#| out-width: "70%"

# Datos
x <- seq(0, 1, length.out = 400)
f <- exp(x)
p <- 1.01 + 0.85*x + 0.84*x^2

# Estética básica
op <- par(mar = c(5,5,3,2))  # márgenes
on.exit(par(op), add = TRUE)

# Gráfico
plot(x, f, type = "l", lwd = 3, col = "red",
     xlab = "x", ylab = "y",
     main = "Gráficamente:\nGráfico de e^x y polinomio cuadrático")

# # Título superior (opcional, estilo como en la imagen)
# mtext("Gráficamente:", side = 3, line = 1.5, cex = 1.6, font = 2)

# Segunda curva
lines(x, p, lwd = 3, col = "blue")

# Leyenda
legend("topleft",
       legend = c(expression(e^x),
                  expression(1.01 + 0.85*x + 0.84*x^2)),
       col = c("red", "blue"), lwd = 3, bty = "n")
```

:::

::: {.callout-note title="Teorema"}

En una Regresión Múltiple, encontrar los parámetros $\beta_1, \beta_2, \ldots, \beta_p$ de tal manera que:

$$
\sum_{i=1}^n e_i^2
$$

sea mínima, es equivalente a proyectar ortogonalmente el vector $y$ sobre el espacio generado por $x_1, x_2, \ldots, x_p$.

Es decir, primero se ortonormaliza la base con *Gram–Schmidt* y luego se proyecta el vector $y$ sobre el subespacio generado por la base ortonormal $\{v_1, v_2, \ldots, v_p\}$.  
O sea, primero se aplica el método de Gram–Schmidt a la base $B = \{x_1, x_2, \ldots, x_p\}$ y se obtiene una base ortonormal $B' = \{v_1, v_2, \ldots, v_p\}$.

Así, la regresión se realiza proyectando el vector $y$ sobre el subespacio generado por la base ortonormal $B'$:

$$
\hat{y} = \langle y, v_1 \rangle v_1 + \langle y, v_2 \rangle v_2 + \cdots + \langle y, v_p \rangle v_p .
$$

Donde $\langle x, y \rangle = x \cdot y$ es el producto punto.  
Lo anterior es equivalente a hacerlo de la forma clásica:

$$
\hat{y} = X \beta = X (X^T X)^{-1} X^T y .
$$

#### Gráficamente:

<!--   -->

::: {.callout-caution collapse="true" title="Prueba"}


Es claro, pues este teorema es nada más otra forma de calcular la proyección $P$ de la variable a predecir sobre el subespacio generado por las variables predictoras. $\blacksquare$

:::

:::

# Solución numérica de ecuaciones no lineales.

## El método de aproximaciones sucesivas.

### Teoremas de convergencia y del error

::: {.callout-note title="Teorema"}

Sea $U$ un subconjunto completo de un espacio normado $X$ y $A:U \to U$ una contracción. Entonces las **aproximaciones sucesivas**:

$$
x_{n+1} = Ax_n, \quad \text{para } n = 0,1,2,\ldots,
$$

con $x_0$ arbitrario en $U$, convergen al punto fijo único $x$ de $A$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


Sea $x_0 \in U$ entonces definimos recursivamente la siguiente sucesión en $U$:

$$
x_{n+1} := Ax_n, \quad \text{para } n=0,1,2,\ldots
$$

De donde se tiene que:

$$
\|x_{n+1} - x_n\| = \|Ax_n - Ax_{n-1}\| \leq q \|x_n - x_{n-1}\|,
$$

luego por inducción se deduce que:

$$
\|x_{n+1} - x_n\| \leq q^n \|x_1 - x_0\|, \quad \text{para } n=0,1,2,\ldots
$$

Por lo tanto para $m > n$ se tiene que:

\begin{align*}
\|x_n - x_m\| &\leq \|x_n - x_{n+1}\| + \|x_{n+1} - x_{n+2}\| + \cdots + \|x_{m-1} - x_m\| \\
&\leq (q^n + q^{n+1} + \cdots + q^{m-1}) \|x_1 - x_0\| \\
&\leq \frac{q^n}{1-q} \|x_1 - x_0\|.
\end{align*}

Como $q^n \to 0$ cuando $n \to \infty$, entonces $(x_n)$ es una sucesión de Cauchy y como $U$ es completo existe $x \in U$ tal que $x_n \to x$ cuando $n \to \infty$.  

$\blacksquare$
:::

::: {.callout-note title="Corolario: Cota del Error a Priori"}

Con las mismas hipótesis del teorema anterior se tiene el siguiente *estimado para el error a priori*:

$$
\|x_n - x\| \leq \frac{q^n}{1-q} \, \|x_1 - x_0\|.
$$

::: {.callout-caution collapse="true" title="Prueba"}

Es evidente de la desigualdad (1.1).  

$\blacksquare$
:::
 - Te dice: si hago $n$ pasos, el error máximo respecto a la solución verdadera $x$ estará acotado por esa fórmula.
 - Es útil porque permite decidir de antemano cuántas iteraciones necesito para alcanzar una precisión deseada.
 - Ejemplo: “si quiero error menor que $10^{-6}$, necesito al menos $n = 15$ iteraciones”.
:::

::: {.callout-note title="Corolario: Cota del Error a Posteriori"}

Con las mismas hipótesis del teorema anterior se tiene el siguiente *estimado para el error a posteriori*:

$$
\|x_n - x\| \leq \frac{q}{1-q} \, \|x_n - x_{n-1}\|.
$$

::: {.callout-caution collapse="true" title="Prueba"}

Se deduce del error a priori iniciando con $x_0 = x_{n-1}$.  

$\blacksquare$

:::
 - Usa la diferencia entre dos aproximaciones consecutivas ($x_n$ y $x_{n-1}$) que ya calculaste.
 - Sirve para verificar en la práctica si ya estás suficientemente cerca de la solución sin necesidad de conocerla.
 - Ejemplo: si $\|x_n - x_{n-1}\|$ es muy pequeño, puedes garantizar que $\|x_n - x\|$ también lo es.
:::

::: {.callout-note title="Teorema [Versión 1]"}

Sea $D \subset \mathbb{R}$ un cerrado y sea $g: D \to D$ una función continuamente diferenciable con la siguiente propiedad:

$$
q := \sup_{x \in D} |g'(x)| < 1.
$$

Entonces la ecuación $g(x) = x$ tiene solución única $x \in D$ y la sucesión de aproximaciones sucesivas:

$$
x_{n+1} := g(x_n), \quad \text{para } n = 0,1,2,\ldots
$$

con $x_0$ arbitrario en $D$ converge a esta solución. Además se tiene el siguiente *estimado para el error a priori*:

$$
|x_n - x| \leq \frac{q^n}{1-q}|x_1 - x_0|,
$$

y el siguiente *estimado para el error a posteriori*:

$$
|x_n - x| \leq \frac{q}{1-q}|x_n - x_{n-1}|.
$$

Además, si $D = [a,b]$ entonces se tiene también la siguiente cota del error:

$$
|x_n - x| \leq q^n \max\{x_0 - a, b - x_0\}.
$$

::: {.callout-caution collapse="true" title="Prueba"}


El espacio $\mathbb{R}$ equipado de la norma valor absoluto $|\cdot|$ es un espacio de Banach.  
Por el teorema del valor medio, para todo $x, y \in D$ con $x < y$ se tiene que:

$$
g(x) - g(y) = g'(\xi)(x-y)
$$

para algún punto $\xi \in ]x,y[$. Por lo tanto:

$$
|g(x) - g(y)| \leq \sup_{\xi \in D} |g'(\xi)| \cdot |x-y| = q|x-y|,
$$

lo cual también es válido para $x,y \in D$ con $x \geq y$.  
Por lo tanto $g$ es una contracción, luego aplicando el Teorema de Banach o el Teorema 1 se tiene la existencia y unicidad del punto fijo.

De los corolarios 1 y 2 se tienen obviamente las desigualdades (1.2) y (1.3).  
Para probar la cota del error (1.4) note que:


\begin{align*}
|x_n - x|
&= |g(x_{n-1}) - g(x)| = |g'(\xi_1)| \cdot |x_{n-1} - x| \leq q |x_{n-1} - x| \\[1ex]
&= q |g(x_{n-2}) - g(x)| = q |g'(\xi_2)| \cdot |x_{n-2} - x| \leq q^2 |x_{n-2} - x| \\[1ex]
&\leq \ldots \\[1ex]
&\leq q^n |x_0 - x| \\[1ex]
&\leq q^n \max\{x_0 - a, \; b - x_0\}.
\end{align*}


$\blacksquare$
:::
Este teorema garantiza que, bajo la condición de contracción ($|g’(x)|<1$), la iteración $x_{n+1}=g(x_n)$ converge al único punto fijo y nos da fórmulas concretas para medir el error (a priori, a posteriori, y también en el caso de intervalos cerrados).
:::

::: {.callout-note title="Teorema [Versión 2]"}

Sea $g \in C[a,b]$, con $g:[a,b] \to [a,b]$. Entonces:

1. $g$ tiene un punto fijo en $[a,b]$.

2. Además, si $g'(x)$ existe en $]a,b[$ y $|g'(x)| \leq q < 1$, para todo $x \in ]a,b[$, entonces $g$ tiene un punto fijo único en $[a,b]$.

::: {.callout-caution collapse="true" title="Prueba"}

**I Caso**: Si $g(a) = a$ o $g(b) = b$ se tiene la prueba.  

**II Caso**: Si $g(a) \neq a$ y $g(b) \neq b \Rightarrow g(a) > a$ y $g(b) < b$  

Tome $h(x) = g(x) - x$, note que:  

- $h$ es continua en $[a,b]$,  
- $h(a) = g(a) - a > 0$,  
- $h(b) = g(b) - b < 0$.  

Luego $h(a)$ y $h(b)$ tienen signos opuestos, usando el teorema de los valores intermedios se tiene que existe $x \in [a,b]$ tal que $h(x) = 0 \Rightarrow g(x) - x = 0 \Rightarrow g(x) = x$, por lo tanto $x$ es el punto fijo de $g$.  

2. Suponga que $g$ tiene dos puntos fijos en $[a,b]$, sean estos $x$ y $y$, con $x \neq y$, entonces por el Teorema del Valor Medio existe $\xi \in ]a,b[$ tal que:  

$$
|x-y| = |g(x) - g(y)| = g'(\xi)|x-y| \leq q|x-y| < |x-y|
$$

De donde $|x-y| < |x-y|$, lo cual es una contradicción, luego se tiene que $x=y$.  

$\blacksquare$
:::
:::


::: {.callout-tip collapse="true" title="Ejemplo"}

Pruebe que 

$$
f(x) = \frac{x^2 - 2x}{6}, \quad x \in [-1,1]
$$

tiene un punto fijo en $[-1,1]$.

::: {.callout-caution collapse="true" title="Solución"}

Se debe probar que $f(x) \in [-1,1]$ para todo $x \in [-1,1]$.  
Como 
$$
f'(x) = \tfrac{1}{6}(2x-2) = \frac{x-1}{3} = 0 \;\;\Leftrightarrow\;\; x=1
$$
entonces los máximos o mínimos posibles están en $x=-1$ o $x=1$.  

Como $f(-1) = \tfrac{1}{2}$ es máximo y $f(1) = -\tfrac{1}{6}$ es mínimo entonces para todo $x \in [-1,1]$ se tiene que $f(x)\in[-1,1]$, de donde $f$ tiene un punto fijo en $[-1,1]$.

Solo se ha probado que existe por lo menos un punto fijo, ahora tenemos que probar que es único.  
Se debe probar que existe $q<1$ tal que $|f'(x)| \leq q < 1$, para todo $x \in ]-1,1[$.  

Note que:

$$
|f'(x)| = \left|\frac{x-1}{3}\right| \leq \left|\frac{-1-1}{3}\right| = \tfrac{2}{3} < 1, 
\quad \text{para todo } x \in ]-1,1[.
$$

Luego $f(x)$ tiene un punto fijo único en $[-1,1]$.  
$\blacksquare$
:::

:::




::: {.callout-note title="Teorema"}


Sea $x$ un punto fijo de una función continuamente diferenciable $g$ tal que $|g'(x)|<1$.  
Entonces el método de las aproximaciones sucesivas 
$$
x_{n+1} := g(x_n), \quad \text{para } n=0,1,2,\ldots
$$
es localmente convergente, es decir, existe un vecindario $\mathcal{B}$ del punto fijo $x$ de $g$ tal que el método de aproximaciones sucesivas converge a $x$, para $x_0 \in \mathcal{B}$.

::: {.callout-caution collapse="true" title="Prueba"}


Como $g'$ es continua y $|g'(x)|<1$ entonces existe una constante $0<q<1$ y $\delta>0$ tal que $|g'(y)|<q$ para todo $y\in \mathcal{B}:=[x-\delta,x+\delta]$.  

Entonces se tiene que:

$$
|g(y)-x| = |g(y)-g(x)| \leq q|y-x| < |y-x| \leq \delta
$$

para todo $y\in \mathcal{B}$.  

Por lo que se deduce que $g$ mapea $\mathcal{B}$ en sí mismo, o sea que $g:\mathcal{B}\to \mathcal{B}$ es una contracción, por lo que el resultado se tiene del Teorema 1.  

$\blacksquare$
:::
El Teorema se ilustra en la Figura.
![El método de aproximaciones sucesivas.](Imagenes/3 graph.png)
:::


**Algoritmo**: Método de las aproximaciones sucesivas

**Entrada:** $x_0$ (aproximación inicial), $Tol$, $N$, $g(x)$  
**Salida:** $x$ (punto fijo aproximado) o mensaje de error  

**Pasos:**

1. $i \leftarrow 1$  
2. Mientras $i \leq N$, siga los pasos 3–6:  
   - $x \leftarrow g(x_0)$  
   - Si $|x - x_0| < Tol$:  
     - Salida: $x$  
     - **Parar**  
   - $i \leftarrow i+1$  
   - $x_0 \leftarrow x$  
3. Mensaje de error: *“Número máximo de iteraciones excedido”*.  
   **Parar**
   
Una implementación en R es la siguiente:
   
```{r}
punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_tem <- p0
  while (i <= n) {
    p <- g(p0_tem)
    if (abs(p - p0_tem) < tol) {
      return(p)
    }
    i <- i + 1
    p0_tem <- p
  }
  return(Inf)
}
```

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^{-x}$, es fácil probar que $f(x)$ mapea  $A = [0.5, 0.69]$ en sí mismo.

::: {.callout-caution collapse="true" title="Prueba"}


Como $f(x)$ es estrictamente decreciente y continua en $\mathbb{R}$, en particular lo es en el intervalo $A$, por lo que su imagen sobre $A$ es:
$$
f([0.5, 0.69]) = [f(0.69), f(0.5)]
$$
Calculamos:
$$
f(0.5) = e^{-0.5} \approx 0.6065, \quad f(0.69) = e^{-0.69} \approx 0.5016
$$
Por lo tanto:
$$
f([0.5, 0.69]) = [0.5016, 0.6065]
$$
Observamos que:
$$
[0.5016, 0.6065] \subseteq [0.5, 0.69]
$$
La función $f(x) = e^{-x}$ mapea el intervalo $A = [0.5, 0.69]$ en sí mismo, es decir:
$$
f(A) \subseteq A
$$
:::

Como $f$ es continuamente diferenciable, tome:

$$
q = \max\limits_{x \in A} \, \big| f'(x) \big| 
   = \max\limits_{x \in A} \, \big| -e^{-x} \big| 
   \approx 0.606531 < 1
$$

Si se ejecuta el programa iterativo en **R** como sigue:

```{r}
#| echo: false

punto.fijo <- function(p0, tol, n, g) {
  i <- 1
  p0_tem <- p0
  while (i <= n) {
    p <- g(p0_tem)
    cat(sprintf("En la iteración %d el valor de P es %.15f\n", i, p))
    if (abs(p - p0_tem) < tol) {
      return(p)
    }
    i <- i + 1
    p0_tem <- p
  }
  cat("El método no converge\n")
  return(NULL)
}
```


```{r}
g <- function(x) exp(-x)
sol3 <- punto.fijo(p0 = 0.55, tol = 1e-6, n = 30, g = g)
```

es decir, tomando $x_0 = 0.55$ como aproximación inicial, con $\varepsilon = Tol = 10^{-6}$ para el algoritmo anterior obtenemos que el “punto fijo” de $F$ es  

$$
x = x_{19} = 0.567143650676,
$$  

pues en $x_{19}$ se terminó la ejecución de la función,  
como se aprecia en la salida del programa:

```{r}
#| echo: false
cat("Solución punto.fijo:", sol3, "\n")
```

Por otro lado el error absoluto al calcular $x_{12} = 0.567124201933893$ es igual a:

$$
|x - x_{12}| \approx 1.91 \cdot 10^{-5},
$$

mientras que usando el error a priori se obtiene:

$$
|x - x_{12}| \leq \frac{q^{12}}{1-q} |x_1 - x_0| = 1.70 \cdot 10^{-4}
$$

y usando el error a posteriori se obtiene que:

$$
|x - x_{12}| \leq \frac{q}{1-q} |x_{12} - x_{11}| = 8.13 \cdot 10^{-5}
$$

que es una mejor estimación del verdadero error.  
Usando el error a priori se deduce que para obtener una precisión de $\varepsilon = 10^{-6}$ se requieren al menos:

$$
n \geq \frac{\log \left(\dfrac{\varepsilon(1-q)}{|x_1 - x_0|}\right)}{\log(q)} 
\approx 22.3 \leq 23 \text{ iteraciones},
$$

pero se observa que el programa requirió de 19 iteraciones. 

Ver la versión recursiva en el *HTML*.

```{r}
#| code-fold: true

punto.fijo.recursivo <- function(p0, tol, n, g) {
  p1 <- g(p0)
  if (abs(p0 - p1) < tol || n < 1) {
    if (n > 1) {
      return(p1)
    } else {
      return(Inf)
    }
  } else {
    return(punto.fijo.recursivo(p1, tol, n - 1, g))
  }
}
sol3 <- punto.fijo.recursivo(p0 = 0.55, tol = 1e-6, n = 30, g = g)
cat("Solución punto.fijo.recursivo:", sol3, "\n")
```

:::


::: {.callout-tip collapse="true" title="Ejemplo"}


Resuelva la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$.

**Solución:**  
Se debe plantear un problema de encontrar los puntos fijos de una función $g(x)$ que sea equivalente a resolver la ecuación $x^3 - x - 1 = 0$.  
Resolver $x^3 - x - 1 = 0$ es equivalente a resolver la ecuación $x^3 - 1 = x$, entonces se puede tratar de encontrar los puntos de $g(x) := x^3 - 1$.  

Pero $g(x)$ no cumple las hipótesis del Teorema de Punto Fijo de Banach, pues $g'(x) = 3x^2 > 0 \implies g(x)$ es creciente en $[1,2]$, luego $g(1) = 0$ y $g(2) = 7$ son el mínimo y el máximo de $g(x)$ en el intervalo $[1,2]$ respectivamente, por lo que $g(x) \notin [1,2] \quad \forall x \in [1,2]$.

Otro intento se puede hacer usando el hecho de que:

$$
x^3 - x - 1 = 0 \iff x = \pm \sqrt{1 + \frac{1}{x}},
$$

luego tome 

$$
g(x) := \sqrt{1 + \frac{1}{x}}, 
\quad g'(x) = -\frac{1}{2x^2\sqrt{1 + \frac{1}{x}}} < 0 \quad \text{en } [1,2].
$$

Esto implica que $g(x)$ es decreciente en $[1,2]$, luego:

$$
g(1) = \sqrt{2} \approx 1.41, 
\quad g(2) = \sqrt{\tfrac{3}{2}} \approx 1.22.
$$

El máximo y el mínimo respectivamente de $g(x)$ en $[1,2]$, por lo que $g(x) \in [1,2]$  
$\forall x \in [1,2]$, luego la función $g(x)$ tiene al menos un punto fijo en $[1,2]$.

Se probó que $g : [1,2] \to [1,2]$, falta probar que $g$ es una contracción en el intervalo $[1,2]$. Veamos

\begin{align*}
g'(x) = -\frac{1}{2x^2 \sqrt{1 + \frac{1}{x}}} 
\Rightarrow |g'(x)| = \frac{1}{2x^2 \sqrt{1 + \frac{1}{x}}} \leq \frac{1}{2} := q < 1
\end{align*}

de donde se puede tomar $q := \frac{1}{2}$.  
Se pueden ver las ejecuciones de las funciones en **R** de punto fijo con $x_0 = 2$ y $\epsilon = 10^{-5}$ en el archivo `punto_fijo.html`.

Luego el punto fijo de $g(x)$ y solución de la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$ es:  
$$x = 1.32471747253653$$  
Que coincide con la solución encontrada por nuestro programa. Gráficamente se ilustra en la Figura

```{r}
#| echo: false
# Definir los valores de x
x <- seq(0.01, 3, by = 0.01)

# Definir la función G(x)
G <- function(x) sqrt(1 + 1/x)

# Crear el gráfico
plot(x, G(x), type = "l", col = "blue", lwd = 2,
     ylim = c(0, 3.2), xlab = "x", ylab = "y", main = "Gráfico de G(x) y x")

# Agregar la recta y = x
lines(x, x, col = "red", lty = 2, lwd = 2)

# Agregar leyenda
legend("topright", legend = c("G(x) = sqrt(1 + 1/x)", "y = x"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```


$\blacksquare$

:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Dada la ecuación del ejemplo anterior $x^3 - x - 1 = 0$ en el intervalo $[1,2]$,  ¿cuántas iteraciones se requieren para obtener un error absoluto menor que $10^{-5}$?

**Solución:** Recuerde que $q = \frac{1}{2}$, de donde se tiene que:

\begin{align*}
|x_n - x| 
&\leq q^n \max\{x_0 - a, b - x_0\} \\
&= \left(\frac{1}{2}\right)^n \max\{1, 0\} \quad \text{(con $x_0 = 2$)} \\
&\leq \left(\frac{1}{2}\right)^n
\end{align*}

Luego:

\begin{align*}
|x_n - x| \leq 10^{-5} 
\Leftrightarrow \left(\frac{1}{2}\right)^n \leq 10^{-5} 
\Leftrightarrow n \geq 16.6
\end{align*}

Tome $n = 17$. $\blacksquare$

::: {.callout-important collapse="true" title="Observación"}


**Observación 1** En la práctica el programa requirió solamente 9 iteraciones.
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Para la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1,2]$, con $x_0 = 2$, estime usando el error a priori (1.2) ¿Cuántas iteraciones se requieren para obtener un error absoluto menor que $10^{-5}$?

**Solución:** Se tiene que

$$
q = \frac{1}{2}, \quad x_0 = 2, \quad g(x) = \sqrt{1 + \frac{1}{x}}
$$

De donde se obtiene que $x_1 = \sqrt{1 + \frac{1}{2}} = \sqrt{\frac{3}{2}} \approx 1.2$, entonces:

\begin{align*}
|x_n - x| 
&\leq \frac{\left(\frac{1}{2}\right)^n}{1 - \frac{1}{2}} |2 - 1.2| \\
&= \left(\frac{1}{2}\right)^{n-1} \cdot 0.8
\end{align*}

entonces:

\begin{align*}
|x_n - x| 
&\leq 10^{-5} 
\Leftrightarrow \left(\frac{1}{2}\right)^{n-1} \cdot 0.8 \leq 10^{-5} \\
&\Leftrightarrow (n-1)(-\log(2)) \leq -5 - \log(0.8)
\end{align*}

esto implica que $n \geq 17.28$, por lo que se puede tomar $n = 18$. $\blacksquare$

::: {.callout-important collapse="true" title="Observación"}


**Observación 2** En la práctica el programa requirió solamente 9 iteraciones.
:::
:::



## Método de bisección, método regula falsi y método de la secante.

### Método de la Bisección

Las hipótesis de este método son:

- $f$ debe ser continua en el intervalo $[a, b]$.
- $f(a)$ y $f(b)$ deben tener signos opuestos.

Luego, por el Teorema de los Valores Intermedios, existe $x \in [a, b]$ tal que $f(x) = 0$, gráficamente se ilustra en la Figura .

![El Método de la Bisección](Imagenes/4 grpah.png){width=400px fig-align="center"}

La idea es encontrar una sucesión $(x_n)$ tal que $x_n \to x$ cuando $n \to \infty$ tal que $f(x) = 0$. Para encontrar la sucesión $(x_n)$, la idea es la siguiente:

- Tome $a_1 = a$, $b_1 = b$, $x_1 = \dfrac{a_1 + b_1}{2}$.
- Si $f(x_1) = 0$, entonces ya se tiene el cero de la ecuación $x = x_1$.
- Si no:
  - Si $f(x_1)$ y $f(a)$ tienen el mismo signo, se toma:
  
    $$
    a_2 = x_1, \quad b_2 = b_1, \quad x_2 = \dfrac{a_2 + b_2}{2}.
    $$
  - Si no se toma:

  $$
  a_2 = a_1, \quad b_2 = x_1, \quad x_2 = \dfrac{a_2 + b_2}{2},
  $$

  y así sucesivamente hasta que $f(x_i) \approx 0$ o hasta superar el número máximo de iteraciones. El pseudocódigo se puede escribir como sigue:


#### Algoritmo Método de la Bisección

**Entrada:** $a, b, Tol$ (tolerancia), $N$, $f$  
**Salida:** Aproximación de $x$ (cero de la ecuación) o mensaje de error

1. $i \leftarrow 1$  
2. Mientras $i \leq N$, siga los pasos 3–6  
3. $x \leftarrow \dfrac{a + b}{2}$  
4. Si $f(x) = 0$ o $|b - a| < Tol$  
  -  Salida ($x$)  
  - **Parar**  
5. $i \leftarrow i + 1$  
6. Si $f(a) \cdot f(x) > 0$  
   - $a \leftarrow x$  
   -  Si no  
   -  $b \leftarrow x$  
7. Salida (“Número máximo de iteraciones excedido”)  
    **Parar**


Este algoritmo se puede programar iterativamente en **R**, ver el archivo `biseccion.html`.

```{r}
# Versión detallada: imprime cada iteración
biseccion <- function(a, b, tol, n, G) {
  i <- 1
  a1 <- a
  b1 <- b
  if (G(a) * G(b) > 0) {
    cat("No cumple las hipótesis\n")
  } else {
    while (i <= n) {
      X <- (a1 + b1) / 2
      cat("En la iteración", i, "el valor de X es", X, "\n")
      if (G(a) * G(X) > 0) {
        a1 <- X
      } else {
        b1 <- X
      }
      if ((b1 - a1) < tol) {
        return(X)
      }
      i <- i + 1
    }
    cat("El método no converge\n")
    return(NA)
  }
}
```


::: {.callout-tip collapse="true" title="Ejemplo"}


Resuelva la ecuación $x^3 + 4x^2 - 10 = 0$ en el intervalo $[1,2]$ con una tolerancia de $\varepsilon = 10^{-6}$.

**Solución:** 
```{r}
# Definición de la función
G <- function(x) x^3 + 4*x^2 - 10
# Gráfico de la función
curve(G, from = 1, to = 2, col = "blue", lwd = 2,
      main = "Gráfico de G(x) = x^3 + 4x^2 - 10",
      xlab = "x", ylab = "G(x)")
abline(h = 0, col = "red", lty = 2)
# Llamada a la función biseccion
resultado <- biseccion(1, 2, 1e-6, 50, G)
cat("Resultado final:", resultado, "\n")
```

$\blacksquare$
:::

#### Estudio del error en el método de la bisección

::: {.callout-note title="Teorema"}


Sea $f \in C[a,b]$, con $f(a)f(b) < 0$. Entonces el algoritmo de la bisección produce una sucesión $(x_n)$ que aproxima a $x$, el cero de la ecuación $f(x) = 0$, con un error absoluto tal que:

$$
|x_n - x| < \frac{b - a}{2^n} \quad \text{para } n \geq 1.
$$

::: {.callout-caution collapse="true" title="Prueba"}


\begin{align*}
|b_1 - a_1| &= |b - a| \\
|b_2 - a_2| &= \frac{1}{2} |b - a| \\
|b_3 - a_3| &= \frac{1}{2^2} |b - a| \\
&\vdots \\
|b_n - a_n| &= \frac{1}{2^{n-1}} |b - a|
\end{align*}

Como $x \in \, ]a_n, b_n[$ y $x_n = \dfrac{a_n + b_n}{2}$, se tiene que:

\begin{align*}
|x_n - x| &\leq \frac{|b_n - a_n|}{2} = \frac{1}{2} \cdot \frac{1}{2^{n-1}} |b - a| = \frac{1}{2^n}(b - a)
\end{align*}

$\blacksquare$
:::
:::
::: {.callout-important collapse="true" title="Observación"}


Sea $|x_n - x| < \dfrac{b - a}{2^n} \Rightarrow \dfrac{|x_n - x|}{\frac{1}{2^n}} < b - a = k$,  
esto implica que la sucesión $(x_n)$ es $\mathcal{O}\left( \dfrac{1}{2^n} \right)$,  
es decir, $x_n \to x$ con rapidez $\dfrac{1}{2^n}$, lo cual es bastante rápido.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Para $f(x) = x^3 + 4x^2 - 10$, con $a = 1$ y $b = 2$; usando (1.5), hallar el $n \in \mathbb{N}$ necesario para tener un error absoluto menor a $\varepsilon = 10^{-6}$.

**Solución:**

\begin{align*}
|x_n - x| &< \frac{b - a}{2^n} < 10^{-6} \\
\Leftrightarrow \quad 2^{-n} &< 10^{-6} \\
\Leftrightarrow \quad -n \log 2 &< -6 \\
\Leftrightarrow \quad n &> \frac{6}{\log 2} \\
\Leftrightarrow \quad n &> 19.9
\end{align*}

Entonces se puede escoger $n = 20$. $\blacksquare$
:::

::: {.callout-important collapse="true" title="Observación"}


$n = 20$ fue exactamente lo que requirió el programa.
:::

### Método de Newton–Raphson

El método de Newton–Raphson es uno de los más poderosos para resolver la ecuación $f(x) = 0$.  
Si $f$ es una función de una variable y $x_0$ es una aproximación a cero de la función $f$, entonces en un vecindario de $x_0$, por la fórmula de Taylor se tiene que:

$$
f(x) \approx f(x_0) + f'(x_0)(x - x_0).
$$

Si se define $g(x) := f(x_0) + f'(x_0)(x - x_0)$, entonces el cero de la función afín $g(x)$ se puede considerar como una nueva aproximación al cero de $f(x)$, el cual denotamos por $x_1$.  
Luego se tiene que:

$$
g(x_1) = f(x_0) + f'(x_0)(x_1 - x_0) = 0,
$$

despejando se tiene:

$$
x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}.
\tag{1.6}
$$

Geométricamente, la función afín $g(x)$ representa la recta tangente a $f(x)$ en el punto $x_0$. Esto se ilustra en la siguiente Figura.

![El Método de  Newton](Imagenes/MetodoNR.png){width=400px fig-align="center"}

De la Figura también se puede deducir el método de Newton–Raphson.  
Nótese que:

$$
f'(x_i) = \frac{f(x_i) - 0}{x_i - x_{i+1}}
$$

lo cual implica que:

$$
x_i - x_{i+1} = \frac{f(x_i)}{f'(x_i)}
$$

de donde:

$$
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
$$

que es la sucesión del método de Newton–Raphson.

::: {.callout-important collapse="true" title="Observación"}


El método de Newton–Raphson consiste en diseñar un algoritmo que calcule la sucesión:

$$
x_n =
\begin{cases}
x_0 & \text{si } n = 0 \\
x_{n-1} - \dfrac{f(x_{n-1})}{f'(x_{n-1})} & \text{si } n > 0
\end{cases}
$$
:::

::: {.callout-note title="Algoritmo 3 – Método de Newton–Raphson"}

**Entrada:** $x_0$, $N$, $Tol$, $f$  
**Salida:** La solución $x$ aproximada de la ecuación $f(x) = 0$ o mensaje de error.

**Pasos:**

1 $i \leftarrow 1$  
2 Mientras $i \leq N$, hacer pasos 3–6:  
  3 $x \leftarrow x_0 - \dfrac{f(x_0)}{f'(x_0)}$  
  4 Si $|x - x_0| < Tol$  
  Salida $(x)$  
  Parar  
   5 $i \leftarrow i + 1$  
   6 $x_0 \leftarrow x$  
7 Salida (“Número máximo de iteraciones excedido”)  
 Parar
:::

El método de Newton–Raphson se puede implementar iterativamente en **R**.  
Ver el archivo `Newton_Raphson.html`.

::: {.callout-tip collapse="true" title="Ejemplo"}


**Ejemplo 8.** Ejecutando el programa iterativo para resolver la ecuación  
$e^{-x} - x = 0$ con $x_0 = 1$ como aproximación inicial y una tolerancia de $\varepsilon = 10^{-6}$,  
se obtiene la siguiente solución en el archivo `Newton_Raphson.html`.
:::

::: {.callout-note title="Teorema"}


Sea $f \in C^2[a,b]$. Si $x \in [a,b]$ con $f(x) = 0$ y $f'(x) \ne 0$, entonces existe $\varepsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que está bien definida y converge a $x$ cuando $n \to \infty$, para todo $x_0 \in [x - \varepsilon, x + \varepsilon]$.

::: {.callout-caution collapse="true" title="Prueba"}


Sea

$$
g(x) := x - \frac{f(x)}{f'(x)}.
$$

Nótese que si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, entonces $g(\tilde{x}) \in [a,b]$ y $g(\tilde{x}) = \tilde{x}$.

Se define:

$$
p_n := \begin{cases}
p_0 & \text{si } n = 0 \\
g(p_{n-1}) & \text{si } n \geq 1
\end{cases}
$$

Es claro que la $(p_n)$ es la sucesión de Newton–Raphson, entonces basta probar que $g$ cumple las hipótesis del Teorema de punto fijo de Banach, a saber:

$$
\exists \, \varepsilon > 0 \text{ y } q \in ]0,1[ \text{ tal que } \forall x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \Rightarrow |g'(x)| \leq q < 1,
$$

y que $g$ mapea el intervalo $[\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$ en sí mismo.

Como $f'(\tilde{x}) \ne 0$ y $f'$ es continua, se tiene que existe un $\varepsilon_1 > 0$ tal que $f'(x) \ne 0$ para todo $x \in [\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1] \subset [a,b]$,  
de esta manera $g$ está definida y es continua en $[\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1]$.  
También lo está:

$$
g'(x) = \frac{f(x) f''(x)}{[f'(x)]^2} \quad \text{para todo } x \in [\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1].
$$

Como $f \in C^2[a,b]$, entonces $g \in C^1[\tilde{x} - \varepsilon_1, \tilde{x} + \varepsilon_1]$.  
Como por hipótesis $f(\tilde{x}) = 0$, entonces:

$$
g'(\tilde{x}) = \frac{f(\tilde{x}) f''(\tilde{x})}{[f'(\tilde{x})]^2} = 0.
$$

Luego, como $g'$ es continua, esto implica que para cualquier constante $q < 1$ existe un $\varepsilon$, con $0 < \varepsilon < \varepsilon_1$, tal que:

$$
|g'(x)| \leq q < 1 \quad \text{para todo } x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon].
$$

Solo falta probar que $g : [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \to [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$,  
pero si $x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$, por el Teorema del Valor Medio para algún $\xi$ entre $x$ y $\tilde{x}$ se tiene que:

$$
|g(x) - g(\tilde{x})| = |g'(\xi)||x - \tilde{x}|,
$$

entonces:

$$
|g(x) - \tilde{x}| = |g(x) - g(\tilde{x})| = |g'(\xi)| \cdot |x - \tilde{x}| \leq q \cdot |x - \tilde{x}| < |x - \tilde{x}|.
\tag{1.7}
$$

Como $x \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$, entonces $|x - \tilde{x}| < \varepsilon$,  
y luego por (1.7) se tiene que $|g(x) - \tilde{x}| < \varepsilon$,  
lo cual implica que $g(x) \in [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$,  
con lo que se prueba que $g : [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon] \to [\tilde{x} - \varepsilon, \tilde{x} + \varepsilon]$.

$\blacksquare$
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


**Ejemplo 9.** Hallar un método para calcular $\sqrt{A}$, con $A \geq 0$.

**Solución:** Probaremos que la sucesión $(x_n)$ definida por  
$x_{n+1} = \dfrac{1}{2} \left( x_n + \dfrac{A}{x_n} \right)$  
converge a $\sqrt{A}$.  

Nótese que calcular $\sqrt{A}$ es equivalente a resolver la ecuación:

$$
x^2 - A = 0,
$$

usando el método de Newton–Raphson con $f(x) = x^2 - A$ y $f'(x) = 2x$, se tiene que:

\begin{align*}
x_{n+1} &= x_n - \frac{f(x_n)}{f'(x_n)} \\
        &= x_n - \frac{x_n^2 - A}{2x_n} \\
        &= \frac{1}{2} \left( x_n + \frac{A}{x_n} \right).
\end{align*}

De donde es claro que la sucesión $x_n \to \sqrt{A}$ cuando $n \to \infty$. $\blacksquare$
:::

### Método de la Secante

El problema con el método de Newton–Raphson es que requiere la derivada de $f(x)$,  
la cual en muchos casos no se tiene.  

La idea del método de la secante es usar una aproximación para esta derivada,  
como se ilustra en la Figura 1.5.

![El Método de la Secante](Imagenes/MetodoSecante.png){width=400px fig-align="center"}

La deducción del método es la siguiente:

$$
f'(x_{n-1}) = \lim_{x \to x_{n-1}} \frac{f(x) - f(x_{n-1})}{x - x_{n-1}} \approx \frac{f(x_{n-2}) - f(x_{n-1})}{x_{n-2} - x_{n-1}}.
\tag{1.8}
$$

Si en el método de Newton–Raphson $x_n = x_{n-1} - \dfrac{f(x_{n-1})}{f'(x_{n-1})}$ sustituimos $f'(x_{n-1})$ por la aproximación dada en (1.8), se tiene que:

$$
x_n \approx x_{n-1} - \frac{f(x_{n-1})}{\dfrac{f(x_{n-2}) - f(x_{n-1})}{x_{n-2} - x_{n-1}}},
$$

simplificando se obtiene el Método de la Secante:

$$
x_n \approx x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}.
$$

Entonces el método de la secante consiste en calcular la sucesión $(x_n)$ definida por:

$$
x_n =
\begin{cases}
x_0 & \text{si } n = 0 \\
x_1 & \text{si } n = 1 \\
x_{n-1} - \dfrac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})} & \text{si } n \geq 2
\end{cases}
$$

::: {.callout-note title="Algoritmo 4 – Método de la Secante"}

**Entrada:** $N$, $Tol$, $x_0$, $x_1$  
**Salida:** Aproximación de $x$, con $f(x) = 0$, o mensaje de error.

 Paso 1. $i \leftarrow 2$  

  Paso 2. Mientras $i \leq N$, hacer pasos 3–6:

  Paso 3. $x \leftarrow x_1 - \dfrac{(x_0 - x_1)f(x_1)}{f(x_0) - f(x_1)}$

  Paso 4. Si $|x - x_0| < Tol$  

    Salida $(x)$  

   Parar  

  Paso 5. $x_0 \leftarrow x_1$ 

     $x_1 \leftarrow x$  
  Paso 6. $i \leftarrow i + 1$  
  Paso 7. Salida (“Número máximo de iteraciones excedido”)  
 Parar
:::

Una implementación iterativa y otra recursiva en **R** se pueden ver en el archivo `secante.html`.

::: {.callout-tip collapse="true" title="Ejemplo"}


**Ejemplo 10.** Ejecutando el programa iterativo para resolver la ecuación  
$e^{-x} - x = 0$ con $x_0 = 1$ y $x_1 = \dfrac{1}{2}$ como aproximaciones iniciales  
y con una tolerancia de $\varepsilon = 10^{-6}$, se obtienen las siguientes soluciones  
en el archivo `secante.html`.
:::

::: {.callout-note title="Teorema"}


Sea $f$ una función de clase $C^2[a,b]$ con $a < b$.  
Si existe un punto $x$ tal que $f(x) = 0$ y $f'(x) \ne 0$,  
entonces existe un número $\varepsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \varepsilon, x + \varepsilon]$,  
la sucesión generada por el método de la secante

$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1}) f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

está bien definida dentro del intervalo $[x - \varepsilon, x + \varepsilon]$  
y converge a $x$ (cero de la ecuación $f(x) = 0$).


::: {.callout-caution collapse="true" title="Prueba"}

1. Dado que $f'(x) \neq 0$, por el **Teorema de la Inversa Local** existe $\varepsilon > 0$ tal que $f$ es estrictamente monótona en $[x-\varepsilon, x+\varepsilon]$.  
   Esto garantiza que el denominador $f(x_{n-2}) - f(x_{n-1}) \neq 0$ para $x_{n-2}, x_{n-1}$ cercanos a $x$,  
   de modo que la sucesión $\{x_n\}$ está bien definida.

2. La expansión de Taylor alrededor de $x$ da:
   $$
   f(y) = f'(x)(y-x) + \tfrac{1}{2} f''(\xi_y)(y-x)^2,
   \quad \xi_y \in (x,y).
   $$

3. Usando esta expresión en la fórmula de la secante se obtiene que el error satisface aproximadamente:
   $$
   e_n = x_n - x \approx C \, e_{n-1} e_{n-2},
   $$
   con $C = -\dfrac{f''(\xi)}{2f'(x)}$ para algún $\xi \in (x-\varepsilon, x+\varepsilon)$.

4. Esto muestra que si $e_{n-1}, e_{n-2}$ son pequeños, $e_n$ tiende a cero y, por tanto, $x_n \to x$.  
   Además, la convergencia es **superlineal** con orden $\varphi = \tfrac{1+\sqrt{5}}{2}$ (número áureo).

$\blacksquare$

:::
:::

### Orden de convergencia

::: {.callout-note title="Definición"}

Sea $(x_n)$ una sucesión que converge a $x$, se denota $e_n := x_n - x$ para $n \geq 0$.  
Si existen constantes $\alpha$ y $\lambda$ positivas, tal que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - x|}{|x_n - x|^\alpha} = 
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|^\alpha} = \lambda,
$$

entonces se dice que la sucesión $(x_n)$ converge a $x$ con **orden $\alpha$**  
y con **constante asintótica** $\lambda$.
:::

::: {.callout-important collapse="true" title="Observación"}


- Entre mayor sea el orden de convergencia, o sea entre mayor sea $\alpha$, mayor será la “velocidad” de convergencia.
- Si $\alpha = 1$, se dice que el método tiene **orden lineal**.
- Si $\alpha = 2$, se dice que el método tiene **orden cuadrático**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Suponga que tenemos dos esquemas (sucesiones) $x_n$ y $\tilde{x}_n$ tal que:

- $\displaystyle \lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} = 0.75$ (método lineal),
- $\displaystyle \lim_{n \to \infty} \frac{|\tilde{e}_{n+1}|}{|\tilde{e}_n|^2} = 0.75$ (método cuadrático),
- Se supone además que $e_0 = 0.5$ y $\tilde{e}_0 = 0.5$.

**¿Cuántas iteraciones requieren $x_n$ y $\tilde{x}_n$ para converger con un error absoluto menor a $10^{-8}$?**

**Solución:**

**1. Analicemos primero la sucesión (esquema) $x_n$:**

\begin{align*}
|x_{n+1} - x| < 10^{-8} &\Leftrightarrow |e_{n+1}| < 10^{-8} \\
\text{Pero } \frac{|e_{n+1}|}{|e_n|} &\approx 0.75 \Rightarrow |e_{n+1}| \approx 0.75 |e_n| \approx 0.75^2 |e_{n-1}| \approx \cdots \\
&\approx 0.75^n |e_0| = 0.75^n \cdot 0.5 \\
\Rightarrow |e_{n+1}| < 10^{-8} &\Leftrightarrow 0.75^n \cdot 0.5 < 10^{-8} \\
\Leftrightarrow \log(0.75^n \cdot 0.5) &< \log(10^{-8}) \\
n \log(0.75) + \log(0.5) &< -8 \\
n &> \frac{-8 - \log(0.5)}{\log(0.75)} \approx 61.62
\end{align*}

Se puede tomar $n = 62$, por lo tanto $x_n$ requiere aproximadamente **62 iteraciones** para converger a $x$.

---

**2. Analicemos ahora la sucesión $\tilde{x}_n$:**

\begin{align*}
|\tilde{x}_{n+1} - x| < 10^{-8} &\Leftrightarrow |\tilde{e}_{n+1}| < 10^{-8} \\
\text{Pero } \frac{|\tilde{e}_{n+1}|}{|\tilde{e}_n|^2} &\approx 0.75 \Rightarrow |\tilde{e}_{n+1}| \approx 0.75 |\tilde{e}_n|^2 \\
&\approx 0.75 \left[ 0.75 |\tilde{e}_{n-1}|^2 \right]^2 = 0.75^3 |\tilde{e}_{n-1}|^4 \\
&\approx 0.75^3 \left[ 0.75 |\tilde{e}_{n-2}|^2 \right]^4 = 0.75^7 |\tilde{e}_{n-2}|^8 \\
&\approx \cdots \approx 0.75^{2^n - 1} |\tilde{e}_0|^{2^n}
\end{align*}
de donde
\begin{align*}
|\tilde{e}_{n+1}| &< 10^{-8} \\
0.75^{2^{n+1}-1} |\tilde{e}_0|^{2^{n+1}} &< 10^{-8} \\
0.75^{2^{n+1}-1} \cdot 0.5^{2^{n+1}} &< 10^{-8} \\
0.75^{-1} \cdot 0.375^{2^{n+1}} &< 10^{-8} \\
2^{n+1} \log(0.375) &< -8 + \log(0.75) \\
2^{n+1} &> \frac{-8 + \log(0.5)}{\log(0.375)} \\
(n + 1) \log(2) &> \log(19.07) \\
n &> \frac{\log(19.07)}{\log(2)} - 1 \approx 3.24
\end{align*}

Luego $n = 4$, por lo que $\tilde{x}_n$ requiere solamente de **4 iteraciones** para converger a $x$, mientras que el esquema lineal requirió aproximadamente **62 iteraciones** para converger a $x$.

$\blacksquare$

:::

::: {.callout-note title="Teorema"}


Sea $x_n = \begin{cases}
x_0 & \text{si } n = 0 \\
g(x_{n-1}) & \text{si } n \geq 1
\end{cases}$ el esquema (la sucesión) de punto fijo visto en la sección 1.1.  

Si además se supone que:

- $g : [a,b] \to [a,b]$
- $g \in C^2[a,b]$
- Existe $q$ tal que $0 \leq q < 1$ y $|g'(x)| \leq q < 1$ para todo $x \in ]a,b[$
- $g'(x) \ne 0$ para el punto fijo de $g$

Entonces $(x_n)$ converge al menos **linealmente a $x$**, cuando $n \to \infty$.
:::

::: {.callout-caution collapse="true" title="Prueba"}


\begin{align*}
|e_{n+1}| &= |x_{n+1} - x| = |g(x_n) - g(x)| = |g'(\xi_n)(x_n - x)| = |g'(\xi_n)||e_n|
\end{align*}

con $\xi_n$ entre $x_n$ y $x$.  
Como $x_n \to x$ cuando $n \to \infty$, y $\xi_n$ está entre $x_n$ y $x$, entonces la sucesión $(\xi_n)$ también converge a $x$ cuando $n \to \infty$.  

Luego:

$$
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} 
= \lim_{n \to \infty} |g'(\xi_n)| 
= |g'\left( \lim_{n \to \infty} \xi_n \right)| = |g'(x)|
$$

Por lo tanto:

$$
\lim_{n \to \infty} \frac{|e_{n+1}|}{|e_n|} = g'(x) := \lambda < 1,
$$

de donde la convergencia es lineal.  
$\blacksquare$
:::

::: {.callout-note title="Teorema"}


Sea $\tilde{x}$ un punto fijo de $g$, además $g'(\tilde{x}) = 0$, $g''$ es continua en un intervalo abierto $I$ que contiene a $x$ y $g''(\tilde{x}) \ne 0$.  
Entonces existe $\epsilon > 0$ tal que para $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ la sucesión  
$x_n = \begin{cases}
x_0 & \text{si } n = 0 \\
g(x_{n-1}) & \text{si } n \geq 1
\end{cases}$  
converge al menos **cuadráticamente** a $\tilde{x}$.

::: {.callout-caution collapse="true" title="Prueba"}


Escoja $\epsilon$ tal que el intervalo $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ esté contenido en $I$,  
$|g'(x)| \leq q < 1$ y $g''$ sea continua.  

Como $|g'(x)| \leq q < 1$, se tiene que los términos de la sucesión $(x_n)$ están contenidos en $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.  

Expandiendo $g(x)$ en su polinomio de Taylor para $x \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$, se tiene que:

\begin{align*}
g(x) &= g(\tilde{x}) + g'(\tilde{x})(x - \tilde{x}) + \frac{g''(\xi)}{2}(x - \tilde{x})^2
\end{align*}

con $\xi$ entre $x$ y $\tilde{x}$. Por hipótesis $g(\tilde{x}) = \tilde{x}$ y $g'(\tilde{x}) = 0$, esto implica que:

\begin{align*}
g(x) = \tilde{x} + \frac{g''(\xi)}{2}(x - \tilde{x})^2
\end{align*}

En particular si se toma $x = x_n$, entonces:

\begin{align*}
x_{n+1} = g(x_n) = \tilde{x} + \frac{g''(\xi_n)}{2}(x_n - \tilde{x})^2
\end{align*}

con $\xi_n$ entre $x_n$ y $\tilde{x}$. Luego:

\begin{align}
x_{n+1} - \tilde{x} = \frac{g''(\xi_n)}{2}(x_n - \tilde{x})^2 \tag{1.9}
\end{align}

Como $|g'(x)| \leq q < 1$ y $g$ mapea $[\tilde{x} - \epsilon, \tilde{x} + \epsilon]$ en sí mismo, es claro que $(x_n)$ converge a $\tilde{x}$ punto fijo de $g$,  
entonces como $\xi_n$ está entre $x_n$ y $\tilde{x}$, la sucesión $(\xi_n)$ converge también a $\tilde{x}$.  
Luego usando (1.9) se deduce que:

\begin{align*}
\lim_{n \to \infty} \frac{|x_{n+1} - \tilde{x}|}{|x_n - \tilde{x}|^2}
= \left| \frac{g''(\xi_n)}{2} \right| = \frac{|g''(\tilde{x})|}{2} = \lambda \ne 0
\end{align*}

Entonces la sucesión $(x_n)$ converge cuadráticamente a $\tilde{x}$.  
$\blacksquare$
:::
:::


::: {.callout-note title="Corolario"}

Sea $f \in C^3[a,b]$. Si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$,  
entonces existe $\epsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que **converge al menos cuadráticamente a $\tilde{x}$** cuando $n \to \infty$,  
para todo $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.
:::

::: {.callout-note title="Corolario"}

Sea $f \in C^3[a,b]$.  
Si $\tilde{x} \in [a,b]$ con $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$,  
entonces existe $\epsilon > 0$ tal que el método de Newton–Raphson:

$$
x_{n+1} := x_n - \frac{f(x_n)}{f'(x_n)}
$$

genera una sucesión que converge al menos cuadráticamente a $\tilde{x}$  
cuando $n \to \infty$, para todo $x_0 \in [\tilde{x} - \epsilon, \tilde{x} + \epsilon]$.


::: {.callout-caution collapse="true" title="Prueba"}


La convergencia de $(x_n)$ se demostró en el teorema anterior.  
Usando el teorema anterior se concluye el corolario, pues si se toma $g(x) := x - \frac{f(x)}{f'(x)}$  
entonces:

\begin{align*}
g(\tilde{x}) = \tilde{x} - \frac{f(\tilde{x})}{f'(\tilde{x})} = \tilde{x} \quad \text{pues } f(\tilde{x}) = 0 \text{ y } f'(\tilde{x}) \ne 0.
\end{align*}

Además,

\begin{align*}
g'(x) &= 1 - \frac{f'(x)f'(x) - f(x)f''(x)}{(f'(x))^2} = 1 - 1 - \frac{f(x)f''(x)}{[f'(x)]^2} = -\frac{f(x)f''(x)}{[f'(x)]^2}
\end{align*}

de donde $g'(\tilde{x}) = 0$, pues $f(\tilde{x}) = 0$.

Como:

\begin{align*}
g''(x) = \frac{[f'(x)]^2 f''(x) + f(x)f'(x)f^{(3)}(x) - 2f(x)[f''(x)]^2}{[f'(x)]^3}
\end{align*}

entonces:

\begin{align*}
g''(\tilde{x}) = \frac{f''(\tilde{x})}{f'(\tilde{x})} \ne 0.
\end{align*}

Es claro que $g''$ es continua en un intervalo que contiene a $\tilde{x}$ dado que $f \in C^3[a,b]$.  
Entonces $g(x)$ cumple todas las hipótesis del teorema anterior por lo que $(x_n)$ converge cuadráticamente a $\tilde{x}$.  
$\blacksquare$
:::
:::

::: {.callout-note title="Teorema"}


Sea $f$ una función de clase $C^2[a,b]$.  
Si existe un punto $\tilde{x}$ tal que $f(\tilde{x}) = 0$, $f'(\tilde{x}) \ne 0$ y $f''(\tilde{x}) \ne 0$  
entonces existe un número $\epsilon > 0$ tal que si $x_0$ y $x_1$ están dentro del intervalo $[x - \epsilon, x + \epsilon]$  
la sucesión generada por el método de la secante:

$$
x_n = x_{n-1} - \frac{(x_{n-2} - x_{n-1})f(x_{n-1})}{f(x_{n-2}) - f(x_{n-1})}
$$

converge a $\tilde{x}$ con orden de al menos:

$$
\frac{1 + \sqrt{5}}{2} \approx 1.618.
$$


::: {.callout-caution collapse="true" title="Prueba"}


(Ejercicio)  
Sugerencias: Pruebe que

\begin{align}
|e_{n+1}| \approx C \cdot |e_n| \cdot |e_{n-1}| \tag{1.10}
\end{align}

con $C := \left| \frac{f''(\tilde{x})}{2f'(\tilde{x})} \right|$.  
Por la definición 1 se busca una solución aproximada a la ecuación:

\begin{align}
|e_n| = \lambda |e_{n-1}|^\alpha \tag{1.11}
\end{align}

con $\lambda > 0$ y $\alpha \geq 1$.  
Pero considerando (1.10) como una ecuación y sustituyendo en (1.11), se tiene que:

\begin{align}
\lambda |e_n|^\alpha = \lambda \lambda^\alpha |e_{n-1}|^{\alpha^2} = C \lambda |e_{n-1}|^{\alpha + 1} \tag{1.12}
\end{align}

La ecuación (1.12) es válida solamente para $n$ suficientemente grandes si:

\begin{align*}
\lambda^\alpha = C \quad \text{y} \quad \alpha^2 = \alpha + 1
\end{align*}

de donde:

\begin{align*}
\alpha = \frac{1 + \sqrt{5}}{2}, \quad \text{y} \quad \lambda = C^{1/\alpha}.
\end{align*}

$\blacksquare$
:::
:::
## Aceleración de la convergencia y método de Steﬀensen.

### Método $\triangle^2$ de Aitken

El método $\triangle^2$ de *Aitken* tiene las siguientes hipótesis:

1. $(x_n)$ converge linealmente a $x$ con constante asintótica $\lambda$ tal que $0 < \lambda < 1$.

2. Los signos de $x_n - x$, $x_{n+1} - x$ y $x_{n+2} - x$ son iguales.

3. Se asume que para $n$ suficientemente grande:

$$
\frac{x_{n+1} - x}{x_n - x} \approx \frac{x_{n+2} - x}{x_{n+1} - x}.
$$

La idea es encontrar una sucesión $(\tilde{x}_n)$ la cual converge “más rápidamente” a $x$ que la sucesión $(x_n)$. Note que de la expresión anterior:

\begin{align*}
&\Rightarrow (x_{n+1} - x)(x_n - x) \approx (x_n - x)(x_{n+2} - x) \\
&\Rightarrow x_{n+1}^2 - 2x_nx_{n+1} + x^2 \approx x_nx_{n+2} - xx_{n+2} - xx_n + x^2 \\
&\Rightarrow -2x_nx_{n+1} + xx_{n+1} + xx_n \approx -x_{n+1}^2 + x_nx_{n+2} \\
&\Rightarrow (-2x_nx_{n+1} + x_nx_{n+2} + x_nx_{n+1})x \approx -x_{n+1}^2 + x_nx_{n+2} \\
&\Rightarrow x \approx \frac{-x_{n+1}^2 + x_nx_{n+2}}{(-2x_nx_{n+1} + x_nx_{n+2} + x_n)} \\
&\Rightarrow x \approx \frac{x_nx_{n+2} - x_{n+1}^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx \frac{x_n^2 + x_nx_{n+2} - 2x_nx_{n+1} - x_{n+1}^2 + 2x_nx_{n+1} - x_{n+1}^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx \frac{x_n(x_n + x_{n+2} - 2x_{n+1}) - (x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n} \\
&\Rightarrow x \approx x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n}
\end{align*}

El método de $\triangle^2$ de Aitken se basa en el hecho de que la sucesión $(\tilde{x}_n)$ definida por:

$$
\tilde{x}_{n+3} := x_n - \frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n},
$$

bajo ciertas condiciones, converge “más rápidamente” a $x$ que la sucesión $(x_n)$.


::: {.callout-note title="Notación: Diferencia Progresiva"}


- $\triangle(x_n) := x_{n+1} - x_n$ para $n \ge 0$.
- $\triangle^k(x_n) := \triangle^{k-1}(\triangle x_n)$ para $k \ge 2$.
:::
::: {.callout-important collapse="true" title="Observación"}

Nótese que con esta notación se tiene:

\begin{align*}
\triangle^2(x_n) &= \triangle(\triangle x_n) \\
              &= \triangle x_{n+1} - \triangle x_n \\
              &= x_{n+2} - x_{n+1} - (x_{n+1} - x_n) \\
              &= x_{n+2} - 2x_{n+1} + x_n.
\end{align*}

Por lo que el método $\triangle^2$ de Aitken se puede escribir como:

\begin{equation}
\tilde{x}_{n+3} := x_n - \frac{(\triangle x_n)^2}{\triangle^2 x_n}\tag{1.14}
\end{equation}
:::

::: {.callout-note title="Definición"}

Sean $(x_n)$ y $(\tilde{x}_n)$ dos sucesiones que convergen a $x$, se dice que la sucesión $(\tilde{x}_n)$ **converge más rápido a** $x$ que la sucesión $(x_n)$ si:

$$
\lim_{n \to \infty} \frac{\tilde{x}_n - x}{x_n - x} = 0.
$$
:::

::: {.callout-note title="Teorema"}


Sea $(x_n)$ una sucesión que converge a $x$ con orden lineal con constante asintótica $\lambda < 1$, además se asume que $e_n = x_n - x \ne 0$ para todo $n$. Entonces la sucesión $(\tilde{x}_n)$, definida como en (1.14), converge a $x$ más rápido que $(x_n)$.

::: {.callout-caution collapse="true" title="Prueba"}


Ejercicio. $\blacksquare$
:::
:::

### El método de Steffensen

Por el Teorema 8 la sucesión de aproximaciones sucesivas $(x_n)$ converge linealmente a $x$, cuando $n \to \infty$ con constante asintótica $\lambda < 1$, entonces tiene sentido aplicar el método $\triangle^2$ de Aitken a esta sucesión. Es así como una combinación entre el método de aproximaciones sucesivas y el método $\triangle^2$ de Aitken produce un método conocido como el **Método de Steffensen**, el cual consiste en calcular la sucesión:

$$
x_{n+1} := x_n - \frac{[g(x_n) - x_n]^2}{g(g(x_n)) - 2g(x_n) + x_n}.
$$

Algorítmicamente, el Método de Steffensen para acelerar el método de punto fijo se puede expresar de la siguiente manera. Sea $x_0$ la aproximación inicial en el método de punto fijo, entonces se toma:

\begin{align*}
x_0^{(0)} &= x_0 \\
x_1^{(0)} &= g\left(x_0^{(0)}\right) \\
x_2^{(0)} &= g\left(x_1^{(0)}\right) \\
x_0^{(1)} &= \Delta^2\left(x_0^{(0)}\right) = x_0^{(0)} - \frac{\left(x_1^{(0)} - x_0^{(0)}\right)^2}{x_2^{(0)} - 2x_1^{(0)} + x_0^{(0)}} \\
x_1^{(1)} &= g\left(x_0^{(1)}\right) \\
x_2^{(1)} &= g\left(x_1^{(1)}\right) \\
x_0^{(2)} &= \Delta^2\left(x_0^{(1)}\right) = x_0^{(1)} - \frac{\left(x_1^{(1)} - x_0^{(1)}\right)^2}{x_2^{(1)} - 2x_1^{(1)} + x_0^{(1)}} \\
\vdots &
\end{align*}


::: {.callout-note title="Algoritmo [Método de Steffensen]"}

**Entrada**: $N$, $Tol$, $x_0$, $g$  
**Salida**: Aproximación de $x$ o mensaje de error

\begin{align*}
&\text{Paso 1. } i \gets 2 \\
&\text{Paso 2. Mientras } i \leq N, \text{ siga los pasos 3–6} \\
&\quad \text{Paso 3. } x_1 = g(x_0), \quad x_2 = g(x_1) \\
&\qquad x = x_0 - \frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} \\
&\quad \text{Paso 4. Si } |x - x_0| < Tol \\
&\qquad \text{Salida } (x) \\
&\qquad \text{Parar} \\
&\quad \text{Paso 5. } i \gets i + 1 \\
&\quad \text{Paso 6. } x_0 \gets x \\
&\text{Paso 7. Salida (``Número máximo de iteraciones excedido'')} \\
&\text{Parar}
\end{align*}

Este algoritmo se puede programar iterativamente y recursivamente en **R**, como se muestra en el archivo `steffensen.html`.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Resolver la ecuación $x^3 - x - 1 = 0$ en el intervalo $[1, 2]$ con $\epsilon = 10^{-5}$.

**Solución**: Como se mostró en el ejemplo 3, utilizando $g(x) = \sqrt{1 + \frac{1}{x}}$, el método de punto fijo requirió de 9 iteraciones, con $x_0 = 2$, para resolver esta ecuación, con una tolerancia de $\epsilon = 10^{-5}$, mientras que el Método de Steffensen requiere solamente de 3 iteraciones, como se muestra en el archivo `steffensen.html`. $\blacksquare$
:::
::: {.callout-note title="Teorema"}


Si el método de punto fijo $x_{n+1} = g(x_n)$ converge linealmente, entonces el orden de convergencia del método de Steffensen es al menos dos.

::: {.callout-caution collapse="true" title="Prueba"}


(Ejercicio) Suponga que $g(x)$ es un número suficientemente de veces derivable, luego pruebe que:

$$
\lim_{n \to \infty} \frac{|x_{n+1} - \widetilde{x}|}{|x_n - \widetilde{x}|^2} 
= \frac{1}{2} \left| \frac{g'(\widetilde{x})g''(\widetilde{x})}{g'(\widetilde{x}) - 1} \right| := \lambda \neq 0.
$$

$\blacksquare$
:::
:::

# Interpolación

## Interpolación y aproximaciones polinómicas

### Polinomios de Bernstein

::: {.callout-note title="Teorema: [Weierstrass]"}

Sea $f$ continua en $[a, b]$; entonces dado $\varepsilon > 0$, existe $n \in \mathbb{N}$ y $P_n(x) \in P_n$ tal que

$$
\left| f(x) - P_n(x) \right| < \varepsilon, \quad \forall x \in [a, b].
$$


![Polinomio de Bernstein](Imagenes/Polinomio de Bernstein.png){width=400px fig-align="center"}

::: {.callout-caution collapse="true" title="Prueba"}


Sin pérdida de generalidad, suponga que $a = 0$ y $b = 1$. Sea:


$$
B_n(x) = \sum_{k=0}^n \binom{n}{k} x^k (1 - x)^{n-k} f\left( \frac{k}{n} \right),
$$

se puede probar que $B_n(x) \to f(x)$ uniformemente en $[0,1]$ (ejercicio). $\blacksquare$
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^x$ en $[0, 1]$, entonces:

\begin{align*}
B_2(x) 
&= \sum_{k=0}^2 \binom{2}{k} x^k (1 - x)^{2-k} e^{\frac{k}{2}} \\
&= \binom{2}{0} x^0 (1 - x)^2 e^0 
   + \binom{2}{1} x^1 (1 - x)^1 e^{\frac{1}{2}} 
   + \binom{2}{2} x^2 (1 - x)^0 e^1 \\
&= (1 - x)^2 + 2x(1 - x) e^{\frac{1}{2}} + x^2 e.
\end{align*}

:::

```{r}
#| code-fold: true

# Polinomio de Bernstein en [0,1]
bernstein <- function(n, F, x) {
  resultado <- numeric(length(x))
  for (i in seq_along(x)) {
    xi <- x[i]
    suma <- 0
    for (k in 0:n) {
      coef <- choose(n, k) * (xi^k) * ((1 - xi)^(n - k))
      suma <- suma + coef * F(k / n)
    }
    resultado[i] <- suma
  }
  return(resultado)
}

# Ejemplo: F(x) = exp(x) en [0,1]
F <- function(x) exp(x)
xs <- seq(0, 1, length.out = 400)

# Aproximación con n = 10
vals <- bernstein(10, F, xs)

# Graficar
plot(xs, F(xs), type = "l", col = "red", lwd = 2,
     main = "Aproximación de Bernstein de exp(x) en [0,1]",
     ylab = "f(x)", xlab = "x")
lines(xs, vals, col = "blue", lwd = 2)
legend("topleft", legend = c("exp(x)", "Bernstein n=10"),
       col = c("red", "blue"), lwd = 2)
```

**Observación.** El polinomio de Bernstein tiene solo valor teórico y no práctico.

### Existencia y unicidad del polinomio de interpolación

::: {.callout-note title="Problema"}

Sean $(x_i, y_i)$, para $i = 0, 1, 2, \ldots, n$, una secuencia de $n + 1$ puntos (nodos). Se busca un polinomio de grado $n$:

$$
P_n(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n,
$$

tal que satisfaga las condiciones de interpolación:

$$
P_n(x_i) = y_i \qquad i = 0, 1, 2, \ldots, n.
$$
:::

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 7

# Paquetes necesarios
library(ggplot2)

# Datos de los nodos
x <- c(-2, -1, 0, 1, 2)
y <- c(-1.2, 0.5, 1, 2.8, 3.5)

# Ajuste del polinomio interpolante de grado 4
modelo <- lm(y ~ poly(x, 4, raw = TRUE))

# Datos para graficar el polinomio
x_vals <- seq(min(x) - 0.5, max(x) + 0.5, length.out = 500)
y_vals <- predict(modelo, newdata = data.frame(x = x_vals))

# Crear un data frame con los nodos para etiquetas
df_nodos <- data.frame(x = x, y = y,
                       etiqueta = paste0("(", x, ", ", y, ")"))

# Gráfico
ggplot() +
  geom_line(aes(x = x_vals, y = y_vals), color = "darkred", linewidth = 1.3) +
  geom_point(data = df_nodos, aes(x = x, y = y), size = 3, shape = 21, fill = "white") +
  geom_text(data = df_nodos, aes(x = x, y = y, label = etiqueta),
            vjust = -1.2, size = 3.5) +
  labs(
    title="Interpolación polinómica con nodos: (-2,-1.2), (-1,0.5), (0,1), (1,2.8), (2,3.5)",
    subtitle="Polinomio P[n](x) tal que  P_n(xᵢ) = yᵢ",
    x = "x",
    y = "y"
  ) +
  theme_minimal(base_size = 14)
```

::: {.callout-note title="Teorema"}


Sean $(x_i, y_i)$ una secuencia de $n + 1$ puntos, con $i = 0, 1, 2, \ldots, n$, tal que $x_i \ne x_j$, $\forall i \ne j$, entonces existe un único polinomio (de interpolación) que satisface la condición de interpolación:

$$
P_n(x_i) = y_i, \qquad i = 0, 1, \ldots, n,
$$

el cual tiene a lo más grado $n$.

::: {.callout-caution collapse="true" title="Prueba"}


**Existencia:**

Sea

$$
L_i(x) := \prod_{\substack{j = 0 \\ j \ne i}}^n \frac{x - x_j}{x_i - x_j}
= \frac{(x - x_0)\cdots(x - x_{i-1})(x - x_{i+1})\cdots(x - x_n)}
{(x_i - x_0)\cdots(x_i - x_{i-1})(x_i - x_{i+1})\cdots(x_i - x_n)}.
$$

Note que:

$$
L_i(x_k) =
\begin{cases}
1 & \text{si } i = k \\
0 & \text{si } i \ne k
\end{cases}
= \delta_{ik}.
$$

Entonces el polinomio

$$
P_n(x) := \sum_{i = 0}^n y_i L_i(x),
$$

tiene la propiedad de interpolación, pues:

$$
P_n(x_k) = \sum_{i = 0}^n y_i L_i(x_k) = \sum_{i = 0}^n y_i \delta_{ik} = y_k, \qquad \text{para } k = 0, 1, \ldots, n.
$$

Además, el grado de $P_n(x)$ es menor o igual a $n$, pues es combinación lineal de polinomios de grado $n$.

**Unicidad:**

Sean $P_n(x)$ y $Q_n(x)$ dos polinomios de grado $n$ que satisfacen las condiciones de interpolación,

$$
P_n(x_k) = Q_n(x_k) = y_k, \qquad k = 0, 1, \ldots, n.
$$

Sea $D(x) := P_n(x) - Q_n(x)$, note que $D(x)$ es un polinomio de grado a lo más $n$ y tiene $n + 1$ raíces $x_0, x_1, \ldots, x_n$, por lo que de acuerdo al Teorema Fundamental del Álgebra se tiene que:

$$
D(x) \equiv 0 \Rightarrow P_n(x) - Q_n(x) = 0 \Rightarrow P_n(x) = Q_n(x). \quad \blacksquare
$$
:::
:::

### Interpolación de Lagrange


::: {.callout-note title="Definición: Polinomio de interpolación de Lagrange"}

El polinomio:

$$
P_n(x) = \sum_{i=0}^n y_i L_i(x),
$$

con $y_i = f(x_i)$ y $L_i(x)$ definido por:

$$
L_i(x) = \frac{(x - x_0)(x - x_1)\cdots(x - x_{i-1})(x - x_{i+1})\cdots(x - x_n)}
{(x_i - x_0)(x_i - x_1)\cdots(x_i - x_{i-1})(x_i - x_{i+1})\cdots(x_i - x_n)},
$$

se llama el **polinomio de interpolación de Lagrange**.
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^x$, $0 \leq x \leq 2$. Calcule $P_2(x)$ con $x_0 = 0$, $x_1 = 1$, y $x_2 = 2$.

**Solución:**

\begin{align*}
P_2(x) &= \frac{(x - 1)(x - 2)}{(0 - 1)(0 - 2)} e^0 + \frac{(x - 0)(x - 2)}{(1 - 0)(1 - 2)} e^1 + \frac{(x - 1)(x - 0)}{(2 - 0)(2 - 1)} e^2 \\
&= \frac{(x - 1)(x - 2)}{2} - x(x - 2)e + \frac{(x - 1)x}{2}e^2.
\end{align*}

$\blacksquare$
:::

![Polinomio de interpolación de Lagrange](Imagenes/Polinomio de interpolación de Lagrange.png){width=400px fig-align="center"}

#### Estudio del error

Recordemos el Teorema de Rolle Generalizado:

::: {.callout-note title="Teorema"}


Sea $f \in C[a, b]$ y $f \in C^n[a, b]$, si $f$ se anula en $n + 1$ puntos distintos $x_0, x_1, \ldots, x_n$ en $[a, b]$, entonces $\exists\, c \in ]a, b[$ tal que $f^{(n)}(c) = 0$.

::: {.callout-caution collapse="true" title="Prueba"}


Se omite. $\blacksquare$
:::
:::

::: {.callout-note title="Teorema: Error en el método de Lagrange"}

Sean $x_0, x_1, \ldots, x_n \in [a, b]$ y sea $f \in C^{n+1}[a, b]$. Entonces $\forall\, x \in [a, b]$, $\exists\, \xi_x \in ]a, b[$ tal que:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x - x_0)(x - x_1) \cdots (x - x_n).
$$

Es decir, el error absoluto es:

$$
\left|f(x) - P_n(x)\right| = \left| \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x - x_0)(x - x_1) \cdots (x - x_n) \right|.
$$

::: {.callout-caution collapse="true" title="Prueba"}


**I caso**  
Si $x = x_k$, entonces $f(x_k) = P_n(x_k)$ y el error es cero, por lo tanto cualquier $\xi_x$ funciona.

**II caso**  
Si $x \ne x_k$, se define:

$$
g(t) = f(t) - P_n(t) - \left[ f(x) - P_n(x) \right] \frac{(t - x_0)(t - x_1)\cdots(t - x_n)}{(x - x_0)(x - x_1)\cdots(x - x_n)}.
$$

Vamos a probar que $g(t)$ cumple las hipótesis del Teorema Generalizado de Rolle para $n + 1$.

- $g$ es $n + 1$ veces derivable pues $f \in C^{n+1}[a, b]$ y $P \in C^\infty[a, b]$.
- $g$ se anula en $n + 2$ puntos, a saber: $t = x_0, x_1, \ldots, x_n$ y $t = x$.


Como $g$ cumple las hipótesis del Teorema Generalizado del Rolle, entonces:

$$
\exists \, \xi_x \in ]a, b[ \text{ tal que } g^{(n+1)}(\xi_x) = 0.
$$

Vamos a calcular $g^{(n+1)}(t)$:

\begin{align*}
\frac{d^{n+1}}{dt^{n+1}} g(t) 
&= f^{(n+1)}(t) - 0 - [f(x) - P_n(x)] \cdot \frac{d^{n+1}}{dt^{n+1}} \prod_{i=0}^n \frac{t - x_i}{x - x_i} \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{1}{\prod_{i=0}^n (x - x_i)} \cdot \frac{d^{n+1}}{dt^{n+1}} \prod_{i=0}^n (t - x_i) \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{1}{\prod_{i=0}^n (x - x_i)} \cdot \frac{d^{n+1}}{dt^{n+1}} \left( t^{n+1} + \text{términos de grado } \leq n \right) \\
&= f^{(n+1)}(t) - [f(x) - P_n(x)] \cdot \frac{(n+1)!}{\prod_{i=0}^n (x - x_i)}.
\end{align*}

De donde se concluye que:

$$
g^{(n+1)}(\xi_x) = f^{(n+1)}(\xi_x) - [f(x) - P_n(x)] \cdot \frac{(n+1)!}{\prod_{i=0}^n (x - x_i)} = 0,
$$

por lo tanto:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!} \prod_{i=0}^n (x - x_i).
$$

$\blacksquare$
:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = e^x$, con $x \in [0, 2]$ y sean $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, tenemos que:

$$
P_2(x) = \frac{(x - 1)(x - 2)}{2} - x(x - 2)e + \frac{(x - 1)x}{2}e^2,
$$

así el error absoluto de aproximar $f(0.25)$ por $P_2(0.25)$ es:

$$
\text{Error Absoluto} = |f(0.25) - P_2(0.25)| = 0.1312511.
$$

Mientras que el error teórico es:

$$
\text{Error Teórico} = \left| \frac{e^{\xi_x}}{3!}(x - 0)(x - 1)(x - 2) \right| 
\leq \left| \frac{e^2}{3!}x(x - 1)(x - 2) \right|,
$$

luego con $x = 0.25$:

$$
\text{Error Teórico} = \left| \frac{e^2}{3!}(0.25)(0.25 - 1)(0.25 - 2) \right| = 0.404089.
$$

$\blacksquare$
:::

::: {.callout-note title="Método 1: Calcula el Polinomio de Lagrange retornando una función para ser evaluada en un $x$."}
```{r}
#| code-fold: true

PLagrange <- function(nodos, f, tol = 1e-12) {
  n  <- length(nodos)
  yi <- vapply(nodos, f, numeric(1))
  #  w_i = 1 / ∏_{j≠i} (x_i - x_j)
  pesos <- numeric(n)
  for (i in seq_len(n)) {
    pesos[i] <- 1 / prod(nodos[i] - nodos[-i])
  }
  # Función que evalúa el polinomio de interpolación
  polinomio <- function(x) {
    x <- as.numeric(x)
    salida <- numeric(length(x))
    for (k in seq_along(x)) {
      diferencias <- abs(x[k] - nodos)
      j <- which.min(diferencias)
      if (diferencias[j] < tol) {
        # Caso: x coincide con un nodo
        salida[k] <- yi[j]
      } else {
        denominador <- sum(pesos / (x[k] - nodos))
        numerador   <- sum((pesos * yi) / (x[k] - nodos))
        salida[k]   <- numerador / denominador
      }
    }
    salida
  }
  return(polinomio)
}
```
:::
::: {.callout-note title="Método 2: Calcula los coeficientes del Polinomio de Lagrange"}

Salida: $c(a_1, a_2,\ldots, a_{n-1})$ donde $$P_n(x) = \sum_{k=0}^{n-1}a_kx^k$$
```{r}
#| code-fold: true
PLagrangeCoeficientes <- function(nodos, f) {
  n     <- length(nodos)
  valores <- vapply(nodos, f, numeric(1))
  # Multiplicación de polinomios: coeficientes en orden ascendente
  multiplicar_polinomios <- function(a, b) {
    resultado <- numeric(length(a) + length(b) - 1)
    for (i in seq_along(a)) {
      for (j in seq_along(b)) {
        resultado[i + j - 1] <- resultado[i + j - 1] + a[i] * b[j]
      }
    }
    resultado
  }
  # ∏ (x - r_j), devuelve vector de coeficientes
  polinomio_desde_raices <- function(raices) {
    polinomio <- 1
    for (r in raices) {
      polinomio <- multiplicar_polinomios(polinomio, c(-r, 1))
    }
    polinomio
  }
  coeficientes <- numeric(n)  # grado ≤ n-1 → longitud n
  for (i in seq_len(n)) {
    raices_i     <- nodos[-i]
    numerador    <- polinomio_desde_raices(raices_i)   # longitud n
    denominador  <- prod(nodos[i] - raices_i)
    coeficientes <- coeficientes + valores[i] * (numerador / denominador)
  }
  return(coeficientes)
}
```

:::

::: {.callout-tip collapse="true" title="Ejemplo"}
 
1) Polinomio de Lagrange
```{r}
#| code-fold: true
## Nodos y función
nodos <- c(0, 1, 2)
f     <- function(x) exp(x)

## 1) Polinomio de Lagrange
P <- PLagrange(nodos, f)

## Evalua en algunos puntos
x_vals <- seq(0, 2, length.out = 5)
data.frame(x = x_vals,
           exp_x = exp(x_vals),
           P_x   = P(x_vals))
```
2) Obtener coeficientes del polinomio de Lagrange
```{r}
#| echo: false
## 2) Obtener coeficientes del polinomio de Lagrange
coefs <- PLagrangeCoeficientes(nodos, f)
coefs
```
3) Graficar
```{r}
#| echo: false

# --- Preparar datos para graficar ---
xi <- seq(-1, 3, length.out = 400)
df <- data.frame(
  x   = xi,
  expx = exp(xi),
  Lx   = P(xi)
)

# Nodos para marcar los puntos de interpolación
df_nodes <- data.frame(x = nodos, y = f(nodos))

# --- Gráfico ---
ggplot(df, aes(x = x)) +
  geom_line(aes(y = expx, color = "exp(x)"), linewidth = 1) +
  geom_line(aes(y = Lx,   color = "L(x)"),  linewidth = 1, linetype = "dashed") +
  geom_point(data = df_nodes, aes(x = x, y = y), shape = 21, size = 3, fill = "white") +
  scale_color_manual(values = c("exp(x)" = "blue", "L(x)" = "red")) +
  labs(title="Interpolación de Lagrange",
       subtitle="Nodos {0,1,2}, f(x) = exp(x)",
       y = "Valor", color = "Función") +
  theme_minimal(base_size = 14)
```
:::
::: {.callout-tip collapse="true" title="Ejemplo"}

1) Polinomio de Lagrange
```{r}
# --- Nodos y función ---
nodos <- c(0, 3, 6, 9, 12, 15)
f     <- function(x) exp(x)

# Polinomio de Lagrange
P <- PLagrange(nodos, f)

# --- Datos para graficar ---
xi <- seq(0, 15, length.out = 600)   # intervalo que cubre todos los nodos
df <- data.frame(
  x    = xi,
  expx = exp(xi),
  Lx   = P(xi)
)
df_nodes <- data.frame(x = nodos, y = f(nodos))
```
2) Obtener coeficientes del polinomio de Lagrange
```{r}
#| code-fold: true
## 2) Obtener coeficientes del polinomio de Lagrange
coefs <- PLagrangeCoeficientes(nodos, f)
coefs
```
3) Grafico
```{r}
# --- Gráfico ---

ggplot(df, aes(x = x)) +
  geom_line(aes(y = expx, color = "exp(x)"), linewidth = 1) +
  geom_line(aes(y = Lx,   color = "L(x)"),  linewidth = 1, linetype = "dashed") +
  geom_point(data = df_nodes, aes(x = x, y = y), shape = 21, size = 3, fill = "white") +
  scale_color_manual(values = c("exp(x)" = "blue", "L(x)" = "red")) +
  labs(title="Interpolación de Lagrange",
       subtitle="Nodos {0, 3, 6, 9, 12, 15},  f(x) = exp(x)",
       y = "Valor", color = "Función") +
  theme_minimal(base_size = 14)
```
:::

#### Interpolación iterada

::: {.callout-note title="Definición: Polinomio de Lagrange en subconjuntos de nodos"}

Sea $f$ una función definida en $x_0, x_1, \ldots, x_n$ y sean $0 \le m_i \le n$, para $i = 1, 2, \ldots, k$, entonces el polinomio de Lagrange de grado $\le (k - 1)$ que coincide con $f$ en $x_{m_1}, x_{m_2}, \ldots, x_{m_k}$ se denota por:
$$
P_{m_1, m_2, \ldots, m_k}(x).
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Sea $f(x) = x^3$, $x_0 = 1$, $x_1 = 2$, $x_2 = 3$, $x_3 = 4$, $x_4 = 6$, calcule $P_{0,3,4}(x)$.

**Solución:** $P_{0,3,4}(x)$ es el polinomio que coincide con $f$ en $x_0 =  = 1$, $x_3 = 4$, $x_4 = 6$, de donde:
\begin{align*}
P_{0,3,4}(x) &= \frac{(x - 4)(x - 6)}{(1 - 4)(1 - 6)} 1^3 + \frac{(x - 1)(x - 6)}{(4 - 1)(4 - 6)} 4^3 + \frac{(x - 1)(x - 4)}{(6 - 1)(6 - 4)} 6^3 \\
&= 11x^2 - 34x + 24.
\end{align*}
$\blacksquare$
:::
::: {.callout-tip collapse="true" title="Ejemplo"}


Calcule $P_{1,2,4}(x)$:

**Solución:**

\begin{align*}
P_{1,2,4}(x) &= \frac{(x - 3)(x - 6)}{(2 - 4)(2 - 6)} 2^3 
+ \frac{(x - 2)(x - 6)}{(3 - 2)(3 - 6)} 3^3 
+ \frac{(x - 2)(x - 3)}{(6 - 2)(6 - 3)} 6^3 \\
&= 10x^2 - 27x + 18.
\end{align*}
$\blacksquare$

:::

Los siguientes párrafos se dedican a encontrar un método para calcular los polinomios de Lagrange en forma recursiva.

::: {.callout-note title="Teorema"}


Sea $f$ una función definida en $x_0, x_1, \ldots, x_k$ y sea $x_i \ne x_j$ con $i,j \in \{0, 1, 2, \ldots, k\}$. Entonces el polinomio de Lagrange que coincide con $f$ en $x_0, x_1, \ldots, x_k$ se puede escribir como:

\begin{align*}
P(x) = \frac{(x - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x) - (x - x_i) P_{0,1,\ldots,(i-1),(i+1),\ldots,k}(x)}{x_i - x_j}.
\end{align*}

::: {.callout-caution collapse="true" title="Prueba"}


Hay que probar que $P(x_s) = f(x_s)$, $\ \forall \ s = 0, 1, 2, \ldots, k$.

**Primer caso:**  
Sea $x_r \ne x_i$ y $x_r \ne x_j$ un nodo:

\begin{align*}
P(x_r) &= \frac{(x_r - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x_r) - (x_r - x_i) P_{0,1,\ldots,(i-1),(i+1),\ldots,k}(x_r)}{x_i - x_j} \\
&= \frac{(x_r - x_j) f(x_r) - (x_r - x_i) f(x_r)}{x_i - x_j} \\
&= \frac{(-x_j + x_i) f(x_r)}{x_i - x_j} \\
&= f(x_r).
\end{align*}

**Segundo caso:**  
Sea $x_r = x_i$:

\begin{align*}
P(x_r) &= \frac{(x_i - x_j) P_{0,1,\ldots,(j-1),(j+1),\ldots,k}(x_i) - 0}{x_i - x_j} \\
&= \frac{(x_i - x_j)}{(x_i - x_j)} f(x_i) \\
&= f(x_i).
\end{align*}

Es análogo si $x_r = x_j$, por lo tanto $P(x)$ es el polinomio de Lagrange que coincide con $f$ en $x_0, x_1, \ldots, x_k$ pues este es único. $\blacksquare$
:::
:::

#### Método de Neville

Se desea aproximar $f(x^*)$ dada la siguiente tabla de valores para $f$:

\begin{align*}
\begin{array}{c|c}
x & f(x) \\
\hline
x_0 & f(x_0) \\
x_1 & f(x_1) \\
\vdots & \vdots \\
x_n & f(x_n)
\end{array}
\end{align*}

Se genera la tabla de $f(x^*)$:

$$
\begin{array}{ccccccc}
x_0 & P_0 \\
x_1 & P_1 & P_{0,1} \\
x_2 & P_2 & P_{1,2} & P_{0,1,2} \\
x_3 & P_3 & P_{2,3} & P_{1,2,3} & P_{0,1,2,3} \\
x_4 & P_4 & P_{3,4} & P_{2,3,4} & P_{1,2,3,4} & P_{0,1,2,3,4} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & P_n & P_{n-1,n} & P_{n-2,n-1,n} & P_{n-3,n-2,n-1,n} & \cdots & P_{0,1,\ldots,n}
\end{array}
$$

Con $P_i(x) = f(x_i)$ una función constante, polinomio de Lagrange de grado 0.  
Esta tabla puede ser calculada usando el Teorema anterior, veamos algunos ejemplos:


\begin{align*}
P_{0,1}(x) &= \frac{(x - x_0)P_1 - (x - x_1)P_0}{x_1 - x_0} \\
P_{1,2}(x) &= \frac{(x - x_1)P_2 - (x - x_2)P_1}{x_2 - x_1} \\
&\vdots \\
P_{n-1,n}(x) &= \frac{(x - x_{n-1})P_n - (x - x_n)P_{n-1}}{x_n - x_{n-1}} \\
P_{0,1,2}(x) &= \frac{(x - x_0)P_{1,2} - (x - x_2)P_{0,1}}{x_2 - x_0} \\
P_{1,2,3}(x) &= \frac{(x - x_1)P_{2,3} - (x - x_3)P_{1,2}}{x_3 - x_1} \\
&\vdots \\
P_{n-2,n-1,n}(x) &= \frac{(x - x_{n-2})P_{n-1,n} - (x - x_n)P_{n-2,n-1}}{x_n - x_{n-2}} \\
P_{0,1,2,3}(x) &= \frac{(x - x_0)P_{1,2,3} - (x - x_3)P_{0,1,2}}{x_3 - x_0} \\
P_{1,2,3,4}(x) &= \frac{(x - x_1)P_{2,3,4} - (x - x_4)P_{1,2,3}}{x_4 - x_1} \\
&\vdots
\end{align*}

::: {.callout-tip collapse="true" title="Ejemplo"}


Aproxime $f(2.5)$ dada la siguiente tabla:

$$
\begin{array}{c|c}
x & f(x) \\
\hline
x_0 = 2.0 & 0.5103757 \\
x_1 = 2.2 & 0.5207843 \\
x_2 = 2.4 & 0.5104147 \\
x_3 = 2.6 & 0.4813306 \\
x_4 = 2.8 & 0.4359160
\end{array}
$$

**Solución:**

Construimos la tabla de Neville:

\begin{align*}
x_0 &: P_0 \\
x_1 &: P_1 \quad P_{0,1} \\
x_2 &: P_2 \quad P_{1,2} \quad P_{0,1,2} \quad f(2.5) \\
x_3 &: P_3 \quad P_{2,3} \quad P_{1,2,3} \quad P_{0,1,2,3} \\
x_4 &: P_4 \quad P_{3,4} \quad P_{2,3,4} \quad P_{1,2,3,4} \quad P_{0,1,2,3,4}
\end{align*}

La tabla de Neville es:

$$
\begin{array}{cccccc}
x_0 & 0.5103757 \\
x_1 & 0.5207843 & \boxed{0.5363972} & \hookleftarrow P_{0,1}\\
x_2 & 0.5104147 & 0.5052299 & 0.4974380 \\
x_3 & 0.4813306 & 0.4958726 & 0.4982119 & 0.4980829 \\
x_4 & 0.4359160 & 0.5040379 & 0.4979139 & 0.4980629 & 0.49807047 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
$$

De donde $f(2.5) \approx 0.49807047$. Un ejemplo del cálculo en la matriz anterior es:

\begin{align*}
P_{0,1}(x) 
&= \frac{(x - x_0)P_1 - (x - x_1)P_0}{x_1 - x_0} \\
&= \frac{(2.5 - 2.0) \cdot 0.5207843 - (2.5 - 2.2) \cdot 0.5103757}{2.2 - 2.0} \\
&= 0.5363972.
\end{align*}

$\blacksquare$

:::


::: {.callout-note title="Notación"}

Se denota por $Q_{ij}$ el polinomio interpolante de Lagrange de grado $j$ que pasa por los $j + 1$ nodos siguientes:

$$
x_{i-j},\ x_{i-j+1},\ \ldots,\ x_{i-1},\ x_i
$$

es decir,

$$
Q_{ij} = P_{i-j,i-j+1,\ldots,i-1,i}(x).
$$

Ahora, usando el método de Neville (teorema anterior):

\begin{align*}
Q_{ij} 
&= \frac{(x - x_i)P_{i-j,i-j+1,\ldots,i-1}(x) - (x - x_{i-j})P_{i-j+1,\ldots,i}(x)}{x_i - x_{i-j}} \\
&= \frac{(x - x_{i-j})P_{i-j+1,\ldots,i}(x) - (x - x_i)P_{i-j,\ldots,i-1}(x)}{x_i - x_{i-j}} \\
&= \frac{(x - x_{i-j})Q_{i,j-1} - (x - x_i)Q_{i-1,j-1}}{x_i - x_{i-j}}.
\end{align*}

Pues:

$$
P_{i-j+1,i-j+2,\ldots,i-1,i} = Q_{i,j-1} \quad \text{dado que } (i - (j - 1)) = i - j + 1,
$$

$$
P_{i-j,i-j+1,\ldots,i-1} = Q_{i-1,j-1} \quad \text{dado que } (i - 1 - (j - 1)) = i - j.
$$

Note que:

$$
Q_{i0} = P_i = f(x_i), \quad \forall i = 0,1,\ldots,n.
$$

Con esta nueva notación, la tabla de Neville se puede escribir como:

$$
\begin{array}{cccccccc}
x_0 & Q_{00} \\
x_1 & Q_{10} & Q_{11} \\
x_2 & Q_{20} & Q_{21} & Q_{22} \\
x_3 & Q_{30} & Q_{31} & Q_{32} & Q_{33} \\
x_4 & Q_{40} & Q_{41} & Q_{42} & Q_{43} & Q_{44} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & Q_{n0} & Q_{n1} & Q_{n2} & Q_{n3} & \cdots & Q_{nn}
\end{array}
$$

$$
Q_{22} = P_{0,1,2}, \quad Q_{nn} = P_{0,1,\ldots,n}
$$
:::

::: {.callout-note title="Algoritmo: Para calcular la tabla de Neville y aproximar $f(x^*) \approx P_n(x^*)$"}

**Entrada:**  
Los nodos $x_0, x_1, \ldots, x_n$.  
Sus imágenes $f(x_0), f(x_1), \ldots, f(x_n)$ como primera columna de la matriz $Q$, es decir $Q_{00}, Q_{10}, Q_{n0}$.

**Salida:**  
La tabla o matriz $Q$, donde $f(x^*) \approx Q_{nn}$.

**Paso 1:** Para $i = 1$ hasta $n$  
\quad Para $j = 1, 2, \ldots, i$
$$
Q_{ij} = \frac{(x - x_{i-j})Q_{i,j-1} - (x - x_i)Q_{i-1,j-1}}{x_i - x_{i-j}}.
$$

**Paso 2:** Salida $Q_{nn}$, parar.  
FIN
:::

#### Diferencias divididas de Newton

La ventaja de este método es que permite calcular el polinomio de Lagrange en cualquier punto $x$.

::: {.callout-note title="Notación: Recursiva"}

\begin{align*}
f[x_i] &:= f(x_i) \\
f[x_i, x_{i+1}] &:= \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i} \\
f[x_i, x_{i+1}, x_{i+2}] &:= \frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i} \\
&\vdots \\
f[x_i, x_{i+1}, \ldots, x_{i+k}] &:= \frac{f[x_{i+1}, x_{i+2}, \ldots, x_{i+k}] - f[x_i, x_{i+1}, \ldots, x_{i+k-1}]}{x_{i+k} - x_i}
\end{align*}
:::

::: {.callout-note title="Teorema"}


Si $P_n(x)$ es el polinomio de Lagrange que coincide con $f(x)$ en $x_0, x_1, \ldots, x_n$, entonces:

\begin{align*}
P_n(x) &= f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) \\
&\quad + \cdots + f[x_0, x_1, \ldots, x_n](x - x_0)(x - x_1) \cdots (x - x_{n-1}) \\
&= f[x_0] + \sum_{k=1}^n f[x_0, \ldots, x_k](x - x_0) \cdots (x - x_{k-1}).
\end{align*}

::: {.callout-caution collapse="true" title="Prueba"}


Si $P_n(x)$ se escribe de la forma

\begin{align*}
P_n(x) &= a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \cdots + \\
&\quad a_n(x - x_0)(x - x_1)\cdots(x - x_{n-1}),
\end{align*}

entonces:

$$
P_n(x_0) = a_0, \quad \text{como } P_n(x_0) = f(x_0) \Rightarrow a_0 = f(x_0) = f[x_0].
$$

Además:

$$
P_n(x_1) = a_0 + a_1(x_1 - x_0), \quad \text{como } P_n(x_1) = f(x_1) \text{ y } a_0 = f[x_0]
\Rightarrow f[x_0] + a_1(x_1 - x_0) = f(x_1)
$$

$$
\Rightarrow a_1 = \frac{f[x_1] - f[x_0]}{x_1 - x_0} = f[x_0, x_1].
$$

Luego, por inducción se puede probar fácilmente que $a_k = f[x_0, x_1, \ldots, x_k]$ (ejercicio). $\blacksquare$
:::
:::

La **Tabla de diferencias divididas de Newton** es la siguiente:

$$
\begin{array}{cccccc}
x_0 & f[x_0] \\
x_1 & f[x_1] & f[x_0, x_1] \\
x_2 & f[x_2] & f[x_1, x_2] & f[x_0, x_1, x_2] \\
\vdots & \vdots & \vdots & \vdots & \ddots \\
x_n & f[x_n] & f[x_{n-1}, x_n] & f[x_{n-2}, x_{n-1}, x_n] & \cdots & f[x_0, x_1, \ldots, x_n]
\end{array}
$$

En la diagonal de la matriz anterior están los coeficientes del polinomio de Lagrange $P_n(x)$, según la forma presentada en el teorema anterior. El siguiente algoritmo calcula el polinomio de Lagrange usando la Tabla de diferencias divididas de Newton.

::: {.callout-note title="Algoritmo"}

**Objetivo**: Calcular el polinomio de Lagrange usando la Tabla de diferencias divididas de Newton.

**Entrada**: Los nodos $x_0, x_1, \ldots, x_n$ y los valores $f(x_0), f(x_1), \ldots, f(x_n)$ como primera columna de la matriz $F$.

**Salida**: $F_{00}, F_{11}, \ldots, F_{nn}$, los coeficientes de $P_n(x)$, donde:  
$$
P_n(x) = \sum_{i=0}^n F_{ii} \prod_{k=0}^{i-1} (x - x_k)
$$


**Paso 1**: Para $i = 1, \ldots, n$

\quad\quad Para $j = 1, 2, \ldots, i$

$$
F_{ij} = \frac{F_{i,j-1} - F_{i-1,j-1}}{x_i - x_{i-j}}
$$

**Paso 2**: Salida $(F_{00}, F_{11}, \ldots, F_{nn})$  
Parar.
:::


### Interpolación de Hermite


Los *Polinomios Osculantes* generalizan a los Polinomios de Taylor y a los Polinomios de Lagrange como veremos más adelante. 
La idea del polinomio de Hermite (que es un caso particular de los polinomios osculantes) es que si se tienen $x_0, x_1, \ldots, x_n$, $n+1$ nodos, entonces $f$ y $f'$ coincidan con $P(x)$ y $P'(x)$ en los nodos respectivamente.

::: {.callout-note title="Definición: Polinomio osculante"}

- Sean $x_0, x_1, \ldots, x_n$, $n + 1$ nodos distintos en $[a,b]$.
- Sea $m_i$ un entero no negativo, con $m_i$ asociado a $x_i$; para $i = 0,1,\ldots,n$.
- Sea $m = \max\limits_{0 \leq i \leq n} m_i$.
- Sea $f \in C^m[a,b]$.

Entonces el **Polinomio Osculante** que aproxima a $f$ es el polinomio de grado menor tal que:

$$
\frac{d^k P(x_i)}{dx^k} = \frac{d^k f(x_i)}{dx^k},
$$

para $i = 0,1,\ldots,n$ y $k = 1,\ldots,m_i$. Es decir, en el $i$-ésimo nodo el polinomio y la función $f$ coinciden hasta la derivada $m_i$.
:::

::: {.callout-important collapse="true" title="Observación"}


1. Si $n = 0$, el polinomio osculante es el polinomio de Taylor de grado $m_0$ para $f$ en $x_0$.

Para ver esto, sea $P(x)$ polinomio de Taylor de grado $m_0$ para $f$ en $x_0$, entonces:

$$
P(x) = f(x_0) + f'(x_0)(x - x_0) + f''(x_0) \frac{(x - x_0)^2}{2!} + \cdots + f^{(m_0)}(x_0) \frac{(x - x_0)^{m_0}}{m_0!}
$$

De donde se deduce que: $P(x_0) = f(x_0)$.

Además:

$$
P'(x) = f'(x_0) + 2f''(x_0) \frac{(x - x_0)}{2!} + \cdots + m_0 f^{(m_0)}(x_0) \frac{(x - x_0)^{m_0 - 1}}{m_0!},
$$

lo cual implica que: $P'(x_0) = f'(x_0)$ y así sucesivamente se puede probar que $P^{(k)}(x_0) = f^{(k)}(x_0)$ para todo $k \leq m_0$.

2. Si $m_i = 0$, e $i = 0,1,\ldots,n$, entonces el polinomio osculante es el polinomio de Lagrange que interpola a $f$ en $x_0, x_1, \ldots, x_n$.  
Pues se está pidiendo que solamente coincida con $f$ en $x_0, x_1, \ldots, x_n$ (no en sus derivadas) y se probó que el polinomio de menor grado que hace esto es el de Lagrange, además se probó que es único.
:::

Cuando $P(x)$ coincide con $f$ y $f'$ en $x_0, x_1, \ldots, x_n$, se dice que $P(x)$ tiene "la misma apariencia" que $f$ en los nodos y se denomina el **Polinomio de Hermite**.

El siguiente Teorema da un método para calcular el Polinomio de Hermite.

::: {.callout-note title="Teorema: Polinomio de Hermite (osculante con derivadas)"}

- Sea $f \in C[a,b]$.
- Sean $x_0, x_1, \ldots, x_n$, $(n+1)$ nodos distintos en $[a,b]$.

Entonces el polinomio de grado menor que coincide con $f$ y $f'$ en $x_0, x_1, \ldots, x_n$:

- Tiene grado $2n+1$.
- Está dado por

$$
H_{2n+1}(x) = \sum_{j=0}^{n} f(x_j) H_{nj}(x) + \sum_{j=0}^{n} f'(x_j) \widetilde{H}_{nj}(x),
$$

donde

$$
H_{nj}(x) = \big[1 - 2(x - x_j)L'_{nj}(x_j)\big]L_{nj}^2(x),
$$

y

$$
\widetilde{H}_{nj}(x) = (x - x_j) L_{nj}^2(x).
$$

- Además, el error absoluto es:

$$
\left| f(x) - H_{2n+1}(x) \right| = \left| \frac{(x - x_0)^2 \cdots (x - x_n)^2}{(2n+2)!} f^{(2n+2)}(\xi) \right|, \quad \text{con } \xi \in ]a, b[.
$$

::: {.callout-caution collapse="true" title="Prueba"}


- Se debe demostrar que $H_{2n+1}(x_i) = f(x_i)$ para todo $i = 0, 1, \ldots, n$. Para ver esto, recordemos que:

$$
L_{nj}(x_i) =
\begin{cases}
0 & \text{si } i \ne j \\
1 & \text{si } i = j
\end{cases}
$$

de donde, cuando $i \ne j$:

$$
H_{nj}(x_i) = 0 \quad \text{y} \quad \widetilde{H}_{nj}(x_i) = 0.
$$

Mientras que:

$$
H_{ni}(x_i) = [1 - 2(x_i - x_i)L'_{ni}(x_i)] \cdot 1 = 1,
$$

$$
\widetilde{H}_{ni}(x_i) = (x_i - x_i) \cdot 1^2 
$$

Luego:

$$
H_{2n+1}(x_i) = \sum_{\substack{j = 0 \\ j \ne i}}^n f(x_j) \cdot 0 + f(x_i) \cdot 1 + \sum_{j=0}^n f'(x_j) \cdot 0 = f(x_i).
$$

Por lo tanto:

- $H_{2n+1}(x_i) = f(x_i)$ para $i = 0, 1, 2, \ldots, n$.

- Se debe demostrar que $H'_{2n+1}(x_i) = f'(x_i)$ para todo $i = 0, 1, \ldots, n$.

Nótese que $L_{nj}(x)$ es un factor de $H'_{nj}(x)$, lo cual implica que $H'_{nj}(x_i) = 0$ cuando $i \ne j$.

Además, si $i = j$:

\begin{align*}
H'_{ni}(x_i) &= -2L'_{ni}(x_i)L^2_{ni}(x_i) + [1 - 2(x_i - x_i)L'_{ni}(x_i)] \cdot 2 \cdot L_{ni}(x_i)L'_{ni}(x_i) \\
&= -2L'_{ni}(x_i) + 2L'_{ni}(x_i) \\
&= 0.
\end{align*}

Por lo tanto, $H'_{nj}(x_i) = 0$ para todo $i = 0, 1, 2, \ldots, n$ y para todo $j = 0, 1, 2, \ldots, n$.

Además:

$$
\widetilde{H}_{nj}(x_i) = L^2_{nj}(x_i) + (x_i - x_j)L'_{nj}(x_j) \cdot 2 \cdot L_{nj}(x_i)L'_{nj}(x_i),
$$

de donde:

$$
\widetilde{H}'_{nj}(x_i) =
\begin{cases}
0 & \text{si } i \ne j \\
1 & \text{si } i = j
\end{cases}
$$

por lo tanto:

$$
\widetilde{H}'_{2n+1}(x_i) = \sum_{j=0}^n f(x_j) \cdot 0 + \sum_{\substack{j = 0 \\ j \ne i}}^n f'(x_j) \cdot 0 + f'(x_i) \cdot 1 = f'(x_i).
$$

es decir:

$$
H'_{2n+1}(x_i) = f'(x_i) \quad \text{para } i = 0, 1, 2, \ldots, n.
$$

- La unicidad queda de ejercicio al lector.

$\blacksquare$

:::
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Calcule $H_5(x)$ que aproxima a $f(x) = e^x$ en $x_0 = 0$, $x_1 = 1$, $x_2 = 2$.

**Solución:**

| $k$ | $x_k$ | $f(x_k)$     | $f'(x_k)$     |
|----:|:-----:|:------------:|:-------------:|
|  0  |   0   | 1            | 1             |
|  1  |   1   | 2.7182818    | 2.7182818     |
|  2  |   2   | 7.3890561    | 7.3890561     |

$$
H_5 = \sum_{j=0}^{2} f(x_j) H_{nj}(x) + \sum_{j=0}^{2} f'(x_j) \widetilde{H}_{nj}(x).
$$

Calculemos cada uno de los términos:

$$
H_{20} = [1 - 2(x - x_0) L'_{20}(x_0)] L_{20}^2(x),
$$

con

\begin{align*}
L_{20}(x) &= \frac{(x - 1)(x - 2)}{(0 - 1)(0 - 2)} \\
          &= \frac{x^2 - x - 2x + 2}{2} \\
          &= \frac{x^2 - 3x + 2}{2},
\end{align*}

esto implica que:

$$
L'_{20}(x) = \frac{2x - 3}{2},
$$

de donde:

$$
L'_{n0}(0) = -\frac{3}{2},
$$

luego:

$$
H_{20} = (1 - 3x) \left( \frac{x^2 - 3x + 2}{2} \right)^2.
$$


\begin{align*}
L_{21}(x) &= \frac{x(x - 2)}{(1 - 0)(1 - 2)} \\
         &= \frac{x(x - 2)}{-1} \\
         &= -x(x - 2) \\
         &= -x^2 + 2x.
\end{align*}

Esto implica que:

$$
L'_{21}(x) = -2x + 2,
$$

de donde:

$$
L'_{21}(x_1) = 0,
$$

por lo que:

\begin{align*}
H_{21} &= (1 - 2(x - 1) \cdot 0)(-x(x - 2))^2 \\
      &= x^2 (x - 2)^2.
\end{align*}

\begin{align*}
L_{22}(x) &= \frac{x(x - 1)}{(2 - 0)(2 - 1)} \\
         &= \frac{x(x - 1)}{2} \\
         &= \frac{x^2 - x}{2},
\end{align*}

de donde:

$$
L'_{22}(x) = \frac{2x - 1}{2},
$$

luego:

$$
L'_{22}(x_2) = \frac{3}{2},
$$

por lo que:


\begin{align*}
H_{22}(x) &= \left(1 - 2(x - 2) \cdot \frac{3}{2} \right) \left( \frac{x^2 - x}{2} \right)^2 \\
         &= (1 - 3(x - 2)) \left( \frac{x^2 - x}{2} \right)^2.
\end{align*}

Por otra parte,

\begin{align*}
\widehat{H}_{20}(x) &= (x - x_0) L_{20}^2(x) \\
                    &= x \left( \frac{x^2 - 3x + 2}{2} \right)^2.
\end{align*}

Análogamente:

$$
\widehat{H}_{21}(x) = (x - 1)x^2(x - 2)^2,
$$

$$
\widehat{H}_{22}(x) = (x - 2) \left( \frac{x^2 - x}{2} \right)^2.
$$

Finalmente:

\begin{align*}
H_5(x) =\; & (1 - 3x) \left( \frac{x^2 - 3x + 2}{2} \right)^2 + 2.7182818\,x^2(x - 2)^2 \\
          & +\; 7.3890561 (1 - 3(x - 2)) \left( \frac{x^2 - x}{2} \right)^2 \\
          & +\; x \left( \frac{x^2 - 3x + 2}{2} \right)^2 + 2.7182818 (x - 1)x^2(x - 2)^2 \\
          & +\; 7.3890561 (x - 2) \left( \frac{x^2 - x}{2} \right)^2.
\end{align*}

$\blacksquare$

2. Aproximando $f(0.25) \cong H(0.25)$ se tiene que:

$$
H_5(0.25) = 1.28364
$$

con el siguiente error absoluto:

Aproximando $e^{0.25} - H_5(0.25)$:

\begin{align*}
\left| e^{0.25} - H_5(0.25) \right| &= \left| 1.28402 - 1.28364 \right| \\
                                   &= 3.8 \times 10^{-5} \\
                                   &\cong 0.3 \times 10^{-4},
\end{align*}

mientras que utilizando el polinomio de Lagrange el error fue:

$$
\left| P(0.25) - e^{0.25} \right| \cong 0.1312511,
$$

que es mucho mayor.

3. Calculando una cota para el error teórico se tiene que:

$$
|e^x - H_5(x)| = \left| \frac{x^2(x - 1)^2(x - 2)^2}{(2 \cdot 2 + 2)!} e^{\xi} \right|, \quad \text{con } \xi \in [0, 2],
$$

esto implica que:

\begin{align*}
|e^{0.25} - H_5(0.25)| &\leq \frac{(0.25)^2 (0.75)^2 (1.75)^2}{6!} e^2 \\
                       &= \frac{0.7955}{720} \\
                       &= 1.1 \times 10^{-3} \\
                       &= 0.1 \times 10^{-2}.
\end{align*}

Como se ha visto, el cálculo del polinomio de Hermite es sumamente tedioso, por esto en la siguiente sección se propone un algoritmo que facilite dicho cálculo.
:::

#### Algoritmo para el polinomio de Hermite

Sabemos que:

\begin{align*}
P_n(x) = f[x_0] + \sum_{k=1}^{n} f[x_0, x_1, \dots, x_k](x - x_0) \cdots (x - x_{k-1}),
\end{align*}

además utilizaremos el siguiente lema.

::: {.callout-note title="Lema: Generalización del Teorema del valor medio"}

Si $f \in C^n[a, b]$ y $x_0, x_1, \dots, x_n$ son los $(n+1)$ nodos distintos en $[a,b]$, entonces:  
existe $\xi \in ]a, b[$ tal que:

$$
f[x_0, x_1, \dots, x_n] = \frac{f^{(n)}(\xi)}{n!}.
$$

::: {.callout-caution collapse="true" title="Prueba"}


Ejercicio al lector. $\blacksquare$
:::
:::

Suponga que se conocen

\begin{align*}
\begin{array}{ccc}
x_0 & f(x_0) & f'(x_0) \\
x_1 & f(x_1) & f'(x_1) \\
\vdots & \vdots & \vdots \\
x_n & f(x_n) & f'(x_n)
\end{array}
\end{align*}

Definimos la sucesión

$$
\{z_n\}_{n \in \mathbb{N}}, \; z_{2i} = z_{2i+1} = x_i, \quad \text{para } i = 0, 1, \dots, n.
$$

Luego se forma la tabla de Hermite como sigue:

\begin{align*}
\begin{array}{lllll}
z_0 = x_0 & f[z_0] = f(x_0) \\
z_1 = x_0 & f[z_1] = f(x_0) & f[z_0,z_1] \approx f'(z_0) \\
z_2 = x_1 & f[z_2] = f(x_1) & f[z_1,z_2] & f[z_0, z_1, z_2] \\
z_3 = x_1 & f[z_3] = f(x_1) & f[z_2,z_3] \approx f'(z_1) & f[z_1,z_2,z_3] & f[z_0,z_1,z_2,z_3] \\
z_4 = x_2 & f[z_4] = f(x_2) & f[z_3,z_4] & f[z_2,z_3,z_4] & f[z_1,z_2,z_3,z_4] \\
z_5 = x_2 & f[z_5] = f(x_2) & f[z_4,z_5] \approx f'(z_2) & f[z_3,z_4,z_5] & \cdots \\
\quad\;\vdots & \quad\;\vdots & \quad\;\vdots & \quad\;\vdots & \quad\;\vdots \\
\end{array}
\end{align*}

A partir de la tercera columna de la matriz anterior el cálculo se hace exactamente igual que en el método de Neville.  
Además, nótese que $f[z_0, z_1]$ no se puede calcular usando la definición, pues daría $\frac{0}{0}$, pero resulta “razonable” tomar $f[z_0, z_1] \approx f'(x_0)$. ¿Por qué?

Luego:

\begin{align*}
H_{2n+1}(x) 
&= f[z_0] + f[z_0, z_1]\overbrace{(x - z_0)}^{(z-z_0)} + f[z_0, z_1, z_2]\overbrace{(x - z_0)^2}^{(z-z_0)(z-z_1)} \\
&\quad + f[z_0, z_1, z_2, z_3](x - z_0)^2(x - z_1) \\
&\quad + f[z_0, z_1, z_2, z_3, z_4](x - z_0)^2(x - z_1)^2 + \cdots
\end{align*}

::: {.callout-note title="Algoritmo: Para obtener los coeficientes del polinomio de Hermite"}

**Entrada:** $x_0, x_1, \dots, x_n;\; f(x_0), f(x_1), \dots, f(x_n)\;$ y $\;f'(x_0), f'(x_1), \dots, f'(x_n)$  
**Salida:** Los números $Q_{0,0}, Q_{1,1}, \dots, Q_{(2n+1),(2n+1)}$ coeficientes de:

\begin{align*}
H_{2n+1}(x) &= Q_{0,0} + \\
&\quad Q_{1,1}(x - x_0) + \\
&\quad Q_{2,2}(x - x_0)^2 + \\
&\quad Q_{3,3}(x - x_0)^2(x - x_1) + \\
&\quad Q_{4,4}(x - x_0)^2(x - x_1)^2 + \cdots + \\
&\quad Q_{(2n+1),(2n+1)}(x - x_0)^2 \cdots (x - x_{n-1})^2(x - x_n)
\end{align*}

**Paso 1**: Para $i = 1, \dots, n$ siga pasos 2–3

**Paso 2**: Tomar  
\begin{align*}
z_{2i} &= x_i \\
z_{2i+1} &= x_i \\
Q_{2i,0} &= f(x_i) \\
Q_{2i+1,0} &= f(x_i) \\
Q_{2i+1,1} &= f'(x_i)
\end{align*}

**Paso 3**: Si $i \ne 0$, tome  
$$
Q_{2i,1} = \frac{Q_{2i,0} - Q_{2i-1,0}}{z_{2i} - z_{2i-1}}
$$

**Paso 4**: Para $i = 2,3,\dots, 2n+1$  
\quad Para $j = 2,3,\dots,i$ tomar  
$$
Q_{i,j} = \frac{Q_{i,j-1} - Q_{i-1,j-1}}{z_i - z_{i-j}}
$$

**Paso 5**: Salida $\big(Q_{0,0}, Q_{1,1}, \dots, Q_{(2n+1),(2n+1)}\big)$.  
**Parar**
:::

::: {.callout-tip collapse="true" title="Ejemplo"}


Si se corre el algoritmo con $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, entonces:

\begin{align*}
Q_{0,0} &= 1 \\
Q_{1,1} &= 1 \\
Q_{2,2} &= 0.71828183 \\
Q_{3,3} &= 0.28171817 \\
Q_{4,4} &= 0.09726402 \\
Q_{5,5} &= 0.02375378
\end{align*}

Así se obtiene el siguiente polinomio:

\begin{align*}
H_5(x) &= 1 + x + 0.71828183x^2 + 0.28171817x^2(x - 1) \\
&\quad + 0.09726402x^2(x - 1)^2 + 0.02375378x^2(x - 1)^2(x - 2)
\end{align*}
:::

::: {.callout-important title="NOTA"}

Ver `hermite.nb`.
:::

### Interpolación por Splines Cúbicos

#### Presentación geométrica

![](Imagenes/Interpolación por Splines Cúbicos.png)

Es encontrar polinomios cúbicos tales que:

- Coincidan con $f(x)$ en los nodos y $P(x)$ sea continuo.
- $P(x)$ tenga primera derivada en los nodos internos.
- $P(x)$ tenga segunda derivada en los nodos internos.

Así, se deben encontrar $n$ polinomios con cuatro coeficientes cada uno. Por lo tanto, se tienen $4n$ incógnitas. Para encontrarlas, se deben establecer $4n$ ecuaciones.

¿Cómo se encuentran tales ecuaciones?

1. Condición de continuidad en los nodos internos

 Como $P(x)$ debe coincidir con $f$ en los nodos internos y $P(x)$ debe ser continuo, se tienen las siguientes ecuaciones:

$$
\left\{
\begin{aligned}
a_{i-1}x_{i-1}^3 + b_{i-1}x_{i-1}^2 + c_{i-1}x_{i-1} + d_{i-1} &= f(x_{i-1}) \\
a_i x_{i-1}^3 + b_i x_{i-1}^2 + c_i x_{i-1} + d_i &= f(x_{i-1})
\end{aligned}
\right\}
\quad \text{para } i = 2, \dots, n
$$

 De aquí se tienen $2(n - 1) = 2n - 2$ ecuaciones.

2. Condición en los extremos

 Como $P(x)$ debe coincidir con $f$ en los extremos, se tienen las siguientes ecuaciones:

$$
\left\{
\begin{aligned}
a_1 x_0^3 + b_1 x_0^2 + c_1 x_0 + d_1 &= f(x_0) \\
a_n x_n^3 + b_n x_n^2 + c_n x_n + d_n &= f(x_n)
\end{aligned}
\right.
$$

 De aquí se tienen 2 ecuaciones.

3. Como las primeras derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:

\begin{align*}
3a_{i-1}x_{i-1}^2 + 2b_{i-1}x_{i-1} + c_{i-1} &= 3a_i x_{i-1}^2 + 2b_i x_{i-1} + c_i,
\quad \text{con } i = 2, \ldots, n.
\end{align*}

 De aquí se tienen $(n - 1)$ ecuaciones.

4. Como las segundas derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:

\begin{align*}
6a_{i-1}x_{i-1} + 2b_{i-1} &= 6a_i x_{i-1} + 2b_i,
\quad \text{con } i = 2, \ldots, n.
\end{align*}

 De aquí se tienen $(n - 1)$ ecuaciones.

En total tenemos: $2n - 2 + 2 + (n - 1) + (n - 1) = 4n - 2$ por lo que faltan todavía 2 ecuaciones.

5. Asumiendo que las segundas derivadas en los nodos extremos deben ser 0, se obtienen 2 ecuaciones más:

\begin{align*}
\left\{
\begin{aligned}
6a_1 x_0 + 2b_1 &= 0 \\
6a_{n-1} x_n + 2b_{n-1} &= 0
\end{aligned}
\right.
\end{align*}

::: {.callout-tip collapse="true" title="Ejemplo"}

Para $f(x)$ dada por la siguiente tabla:

$$
\begin{array}{c|c}
x & f(x) \\
\hline
3.0 & 2.5 \\
4.5 & 1.0 \\
7.0 & 2.5 \\
9.0 & 0.5
\end{array}
$$

Calcule el polinomio de interpolación usando Splines Cúbicos.


**Solución:** Se requieren $3 \cdot 4 = 12$ ecuaciones, las cuales se obtienen como sigue:

1. Como $P(x)$ debe coincidir con $f$ en los nodos internos y $P(x)$ debe ser continuo, se tienen las siguientes ecuaciones:

\begin{align*}
91.125a_1 + 20.25b_1 + 4.5c_1 + d_1 &= 1 \\
91.125a_2 + 20.25b_2 + 4.5c_2 + d_2 &= 1 \\
343a_2 + 49b_2 + 7c_2 + d_2 &= 2.5 \\
343a_3 + 49b_3 + 7c_3 + d_3 &= 2.5
\end{align*}

2. Como $P(x)$ debe coincidir con $f$ en los extremos, se tienen las siguientes ecuaciones:

\begin{align*}
27a_1 + 9b_1 + 3c_1 + d_1 &= 2.5 \\
729a_3 + 81b_3 + 9c_3 + d_3 &= 0.5
\end{align*}

 Igualdad de derivadas en nodos internos

 Como las primeras derivadas de $P(x)$ en los nodos internos deben ser iguales, se tienen las siguientes ecuaciones:
	- con $x = 4.5$:

\begin{align*}
60.75a_1 + 9b_1 + 3c_1 - 60.75a_2 - 9b_2 - c_2 &= 0,
\end{align*}
	- con $x = 7$:

\begin{align*}
147a_2 + 14b_2 + c_2 - 147a_3 - 14b_3 - c_3 &= 0,
\end{align*}
	- con $x = 4.5$:

\begin{align*}
27a_1 + 2b_1 - 27a_2 - 2b_2 &= 0,
\end{align*}
	- con $x = 7$:

\begin{align*}
42a_2 + 2b_2 - 42a_3 - 2b_3 &= 0.
\end{align*}

Condición de derivadas segundas nulas en extremos

Asumiendo que las segundas derivadas en los nodos extremos deben ser $0$, se obtienen 2 ecuaciones más:
	- con $x = 3$:

\begin{align*}
18a_1 + 2b_1 &= 0,
\end{align*}
	- con $x = 9$:

\begin{align*}
54a_3 + 2b_3 &= 0.
\end{align*}

Sistema completo de ecuaciones

De donde, resolviendo el siguiente sistema de ecuaciones:

\begin{align*}
&91.125a_1 + 20.25b_1 + 4.5c_1 + d_1 &&= 1 \\
&91.125a_2 + 20.25b_2 + 4.5c_2 + d_2 &&= 1 \\
&343a_2 + 49b_2 + 7c_2 + d_2 &&= 2.5 \\
&343a_3 + 49b_3 + 7c_3 + d_3 &&= 2.5 \\
&27a_1 + 9b_1 + 3c_1 + d_1 &&= 2.5 \\
&729a_3 + 81b_3 + 9c_3 + d_3 &&= 0.5 \\
&60.75a_1 + 9b_1 + 3c_1 - 60.75a_2 - 9b_2 - c_2 &&= 0 \\
&147a_2 + 14b_2 + c_2 - 147a_3 - 14b_3 - c_3 &&= 0 \\
&27a_1 + 2b_1 - 27a_2 - 2b_2 &&= 0 \\
&42a_2 + 2b_2 - 42a_3 - 2b_3 &&= 0 \\
&18a_1 + 2b_1 &&= 0 \\
&54a_3 + 2b_3 &&= 0
\end{align*}

se obtiene la siguiente solución: 

\begin{align*}
a_1 &= 0.187 & a_2 &= -0.214 & a_3 &= 0.128 \\
b_1 &= -1.679 & b_2 &= 3.73 & b_3 &= -3.449 \\
c_1 &= 3.617 & c_2 &= -20.726 & c_3 &= 29.534 \\
d_1 &= 1.722 & d_2 &= 38.237 & d_3 &= -79.035
\end{align*}

por lo que el spline cúbico es:

$$
P(x) =
\begin{cases}
0.183x^3 - 1.679x^2 + 3.617x + 1.722 & \text{si } 3 \leq x \leq 4.5, \\
-0.214x^3 + 3.73x^2 - 20.726x + 38.237 & \text{si } 4.5 \leq x \leq 7, \\
0.128x^3 - 3.499x^2 + 29.53x - 79.035 & \text{si } 7 \leq x \leq 9.
\end{cases}
$$

$\blacksquare$

:::

#### Presentación algorítmica

![Condiciones de interpolación por Splines](Imagenes/Condiciones de interpolación por Splines.png){width=400px fig-align="center"}

:::{.callout-note title="Definición: Spline cúbico interpolante"}

Dada una función $f$ definida en $[a,b]$ y un conjunto de nodos

$$a = x_0 < x_1 < \cdots < x_n = b$$

un Spline Cúbico Interpolante $S$ para $f$ es una función que satisface las siguientes condiciones:

1. $S(x)$ es un polinomio cúbico, denotado $S_j(x)$, en el subintervalo $[x_j, x_{j+1}]$ para cada $j = 0,1,\ldots,n-1$.
2. $S_j(x_j) = f(x_j)$  y $S_j(x_{j+1}) = f(x_{j+1})$ para cada $j = 0,1,\ldots,n-1$.
3. $S_{j+1}(x_{j+1}) = S_j(x_{j+1})$ para cada $j = 0,1,\ldots,n-2$ (se deduce de (2)).
4. $S’_{j+1}(x{j+1}) = S’_j(x{j+1})$ para cada $j = 0,1,\ldots,n-2$.
5. $S’’_{j+1}(x{j+1}) = S’’_j(x{j+1})$ para cada $j = 0,1,\ldots,n-2$.
6. Se satisface uno de los siguientes conjuntos de condiciones de borde:
   a) $S’’(x_0) = S’’(x_n) = 0$ (borde natural o libre);
   b) $S’(x_0) = f’(x_0)$ y $S’(x_n) = f’(x_n)$ (borde clamped o sujeto).

:::

#### Spline cúbico natural

::: {.callout-tip collapse="true" title="Ejemplo"}

Construir un spline cúbico natural que pase por los puntos $(1,2)$, $(2,3)$ y $(3,5)$.

**Solución:**  
Este spline consiste en dos funciones cúbicas. La primera para el intervalo $[1,2]$, denotada por:

\begin{align*}
S_0(x) &= a_0 + b_0(x-1) + c_0(x-1)^2 + d_0(x-1)^3,
\end{align*}

y la otra para $[2,3]$, denotada por:

\begin{align*}
S_1(x) &= a_1 + b_1(x-2) + c_1(x-2)^2 + d_1(x-2)^3.
\end{align*}

Existen 8 constantes a determinar, lo que requiere 8 condiciones.  
Cuatro condiciones provienen del hecho de que los splines deben coincidir con los datos en los nodos. Así:

\begin{align*}
2 &= f(1) = a_0, \\
3 &= f(2) = a_0 + b_0 + c_0 + d_0, \\
3 &= f(2) = a_1, \\
5 &= f(3) = a_1 + b_1 + c_1 + d_1.
\end{align*}

Otras dos condiciones provienen del hecho de que $S_0'(2) = S_1'(2)$ y $S_0''(2) = S_1''(2)$. Estas son:

\begin{align*}
S_0'(2) = S_1'(2) &: \quad b_0 + 2c_0 + 3d_0 = b_1, \\
S_0''(2) = S_1''(2) &: \quad 2c_0 + 6d_0 = 2c_1.
\end{align*}

Las dos condiciones restantes provienen de las condiciones de frontera naturales:

\begin{align*}
S_0''(1) &= 0 : \quad 2c_0 = 0, \\
S_1''(3) &= 0 : \quad 2c_1 + 6d_1 = 0.
\end{align*}

Resolviendo este sistema de ecuaciones se obtiene el Spline:

$$
S(x) =
\begin{cases}
2 + \tfrac{3}{4}(x-1) + \tfrac{1}{4}(x-1)^3, & x \in [1,2], \\[6pt]
3 + \tfrac{3}{2}(x-2) + \tfrac{3}{4}(x-2)^2 - \tfrac{1}{4}(x-2)^3, & x \in [2,3].
\end{cases}
$$

:::

**Construcción de un Spline Cúbico**

Como se ve en el ejemplo anterior, un spline definido en un intervalo que se divide en $n$ subintervalos requerirá determinar $4n$ constantes. Para construir el spline cúbico interpolante de una función dada $f$, las condiciones de la definición se aplican a los polinomios cúbicos

\begin{align*}
S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3,
\end{align*}

para cada $j = 0,1, \ldots, n-1$.

Como $S_j(x_j) = a_j = f(x_j)$, la condición (3) se puede aplicar para obtener:

\begin{align*}
a_{j+1} &= S_{j+1}(x_{j+1}) = S_j(x_{j+1}) \\
&= a_j + b_j(x_{j+1} - x_j) + c_j(x_{j+1} - x_j)^2 + d_j(x_{j+1} - x_j)^3,
\end{align*}

para cada $j = 0,1, \ldots, n-2$.

Los términos $x_{j+1} - x_j$ se utilizan repetidamente en este desarrollo, por lo que conviene introducir la notación más simple

\begin{align*}
h_j = x_{i+1} - x_i,
\end{align*}

para cada $j = 0,1, \ldots, n-1$.  
Si además definimos $a_n = f(x_n)$, entonces la ecuación:

\begin{align*}
a_{j+1} = a_j + b_jh_j + c_jh_j^2 + d_jh_j^3
\end{align*}

es válida para cada $j = 0,1, \ldots, n-1$.  
De manera similar, definimos $b_n = S'(x_n)$ y observamos que

\begin{align*}
S_j'(x) = b_j + 2c_j(x - x_j) + 3d_j(x - x_j)^2
\end{align*}

implica $S_j'(x_j) = b_j$, para cada $j = 0,1, \ldots, n-1$. Aplicando la condición (4) se obtiene

\begin{align*}
b_{j+1} = b_j + 2c_jh_j + 3d_jh_j^2,
\end{align*}

para cada $j = 0,1, \ldots, n-1$.

Otra relación entre los coeficientes de $S_j$ se obtiene definiendo $c_n = S''(x_n)/2$ y aplicando la condición (5). Entonces, para cada $j = 0,1, \ldots, n-1$:

\begin{align*}
c_{j+1} = c_j + 3d_jh_j.
\end{align*}

Resolviendo para $d_j$ en la ecuación (1.4) y sustituyendo este valor en las ecuaciones (1.2) y (1.3), se obtiene, para cada $j = 0,1, \ldots, n-1$, las nuevas ecuaciones:

\begin{align*}
a_{j+1} &= a_j + b_jh_j + \tfrac{h_j^2}{3}(2c_j + c_{j+1}), \\
b_{j+1} &= b_j + h_j(c_j + c_{j+1}).
\end{align*}

La relación final que involucra los coeficientes se obtiene resolviendo la ecuación apropiada en la forma de la ecuación (1.5), primero para $b_j$:

\begin{align*}
b_j = \tfrac{1}{h_j}(a_{j+1} - a_j) - \tfrac{h_j}{3}(2c_j + c_{j+1}),
\end{align*}

y luego, reduciendo el índice, para $b_{j-1}$. Esto da:

\begin{align*}
b_{j-1} = \tfrac{1}{h_{j-1}}(a_j - a_{j-1}) - \tfrac{h_{j-1}}{3}(2c_{j-1} + c_j).
\end{align*}

Sustituyendo estos valores en la ecuación derivada de la ecuación (1.6), con el índice reducido en uno, se obtiene el sistema lineal de ecuaciones:

\begin{align*}
h_{j-1}c_{j-1} + 2(h_{j-1} + h_j)c_j + h_jc_{j+1} 
= \tfrac{3}{h_j}(a_{j+1} - a_j) - \tfrac{3}{h_{j-1}}(a_j - a_{j-1}),
\end{align*}

para cada $j = 1,2, \ldots, n-1$.

Este sistema involucra únicamente a los $\{c_j\}_{j=0}^n$ como incógnitas.  
Los valores de $\{h_j\}_{j=0}^{n-1}$ y $\{a_j\}_{j=0}^n$ son dados, respectivamente, por el espaciamiento de los nodos $\{x_j\}_{j=0}^n$ y los valores de $f$ en los nodos.  

Una vez determinados los valores de $\{c_j\}_{j=0}^n$, es sencillo calcular el resto de las constantes $\{b_j\}_{j=0}^{n-1}$ a partir de la ecuación (1.9) y $\{d_j\}_{j=0}^{n-1}$ a partir de la ecuación (1.4).  

Así, podemos construir los polinomios cúbicos $\{S_j(x)\}_{j=0}^{n-1}$.

::: {.callout-note title="Teorema [Splines Naturales] "}

Si $f$ está definida en $a = x_0 < x_1 < \cdots < x_n = b$, entonces $f$ tiene un único spline natural interpolante $S$ en los nodos $x_0, x_1, \ldots, x_n$; es decir, un spline interpolante que satisface las condiciones de frontera naturales

\begin{align*}
S''(a) &= 0, \quad S''(b) = 0.
\end{align*}

::: {.callout-caution collapse="true" title="Prueba"}

**Demostración.**  
Las condiciones de frontera en este caso implican que

\begin{align*}
c_n = \tfrac{1}{2} S''(x_n) = 0 
\quad \text{y que} \quad 
0 = S''(x_0) = 2c_0 + 6d_0(x_0 - x_0),
\end{align*}

por lo que $c_0 = 0$.

Las dos ecuaciones $c_0 = 0$ y $c_n = 0$, junto con las ecuaciones en (1.10), producen un sistema lineal descrito por la ecuación vectorial $Ax = b$, donde $A$ es la matriz $(n+1) \times (n+1)$:

$$
A =
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 & 0 \\
h_0 & 2(h_0 + h_1) & h_1 & \cdots & 0 & 0 \\
0 & h_1 & 2(h_1 + h_2) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & 2(h_{n-2} + h_{n-1}) & h_{n-1} \\
0 & 0 & 0 & \cdots & 0 & 1
\end{bmatrix}.
$$

Los vectores $b$ y $x$ son:

$$
b =
\begin{bmatrix}
0 \\
\tfrac{3}{h_1}(a_2 - a_1) - \tfrac{3}{h_0}(a_1 - a_0) \\
\vdots \\
\tfrac{3}{h_{n-1}}(a_n - a_{n-1}) - \tfrac{3}{h_{n-2}}(a_{n-1} - a_{n-2}) \\
0
\end{bmatrix},
\quad
x =
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_n
\end{bmatrix}.
$$

La matriz $A$ es estrictamente diagonalmente dominante, es decir, en cada fila la magnitud del elemento diagonal excede la suma de las magnitudes de todas las demás entradas de esa fila.  

Un sistema lineal con una matriz de esta forma tendrá, como se demostrará más adelante en el capítulo de solución numérica a sistemas lineales, una solución única para $c_0, c_1, \ldots, c_n$.

:::
:::

###### Spline cúbico sujeto (clamped)

::: {.callout-tip collapse="true" title="Ejemplo"}

Construir el *Spline Sujeto (clamped)* $s$ que pasa por los puntos $(1,2)$, $(2,3)$ y $(3,5)$, con $s'(1) = 2$ y $s'(3) = 1$.  

Sea:

\begin{align*}
s_0(x) &= a_0 + b_0(x-1) + c_0(x-1)^2 + d_0(x-1)^3, \quad x \in [1,2], \\
s_1(x) &= a_1 + b_1(x-2) + c_1(x-2)^2 + d_1(x-2)^3, \quad x \in [2,3].
\end{align*}

Las condiciones de interpolación y suavidad son:

###### Algoritmo: Spline Cúbico Natural

**Entrada:**  
$n; \; x_0, x_1, \ldots, x_n; \; a_0 = f(x_0), \; a_1 = f(x_1), \ldots, a_n = f(x_n)$  

**Salida:**  
$a_j, b_j, c_j, d_j \quad$ para $j = 0,1,\ldots,n-1$.  

**Nota:**  
En cada subintervalo $[x_j, x_{j+1}]$ se tiene que:

$$
S_j(x) = a_j + b_j(x-x_j) + c_j(x-x_j)^2 + d_j(x-x_j)^3.
$$

**Paso 1:** Para $i = 0,1,\ldots,n-1$ defina:

$$
h_i = x_{i+1} - x_i.
$$

**Paso 2:** Para $i = 1,2,\ldots,n-1$ defina:

$$
\alpha_i = \tfrac{3}{h_i}(a_{i+1} - a_i) - \tfrac{3}{h_{i-1}}(a_i - a_{i-1}).
$$

**Paso 3:** Defina:

$$
\ell_0 = 1, \quad \mu_0 = 0, \quad z_0 = 0.
$$

**Paso 4:** Para $i = 1,2,\ldots,n-1$ sea:

$$
\ell_i = 2(x_{i+1} - x_{i-1}) - h_{i-1}\mu_{i-1}, 
\quad \mu_i = \tfrac{h_i}{\ell_i}, 
\quad z_i = \tfrac{\alpha_i - h_{i-1}z_{i-1}}{\ell_i}.
$$

**Paso 5:** Defina:

$$
\ell_n = 1, \quad z_n = 0, \quad c_n = 0.
$$

**Paso 6 (Sustitución hacia atrás):**  
Para $j = n-1,n-2,\ldots,0$ haga:

$$
c_j = z_j - \mu_j c_{j+1},
$$

$$
b_j = \tfrac{a_{j+1} - a_j}{h_j} - \tfrac{h_j}{3}(c_{j+1} + 2c_j),
$$

$$
d_j = \tfrac{c_{j+1} - c_j}{3h_j}.
$$


**Paso 7:**  
Retorne $\{a_j, b_j, c_j, d_j\}_{j=0}^{n-1}$.

---

De las condiciones de interpolación se tiene:

$$
f(1) = 2 \;\;\Rightarrow\;\; a_0 = 2,
$$

$$
f(2) = 3 \;\;\Rightarrow\;\; a_0 + b_0 + c_0 + d_0 = 3, \quad a_1 = 3,
$$

$$
f(3) = 5 \;\;\Rightarrow\;\; a_1 + b_1 + c_1 + d_1 = 5,
$$

$$
s_0'(2) = s_1'(2) \;\;\Rightarrow\;\; b_0 + 2c_0 + 3d_0 = b_1,
$$

$$
s_0''(2) = s_1''(2) \;\;\Rightarrow\;\; 2c_0 + 6d_0 = 2c_1.
$$

Las condiciones de frontera (*clamped*) son:

$$
s_0'(1) = 2 \;\;\Rightarrow\;\; b_0 = 2, 
\qquad
s_1'(3) = 1 \;\;\Rightarrow\;\; b_1 + 2c_1 + 3d_1 = 1.
$$

Al resolver el sistema se obtiene el Spline por trozos:

$$
s(x) =
\begin{cases}
2 + 2(x-1) - \tfrac{5}{2}(x-1)^2 + \tfrac{3}{2}(x-1)^3, & x \in [1,2], \\[6pt]
3 + \tfrac{3}{2}(x-2) + 2(x-2)^2 - \tfrac{3}{2}(x-2)^3, & x \in [2,3].
\end{cases}
$$
:::

::: {.callout-note title="Teorema"}

**[Unicidad del spline clamped]**  
Sea $f$ una función definida en $a = x_0 < x_1 < \cdots < x_n = b$ y diferenciable en $a$ y $b$.  
Entonces $f$ tiene un único spline cúbico interpolante *clamped* $S$ en los nodos $x_0, x_1, \ldots, x_n$; es decir, un spline interpolante que satisface las condiciones de frontera:

$$
S'(a) = f'(a), 
\qquad 
S'(b) = f'(b).
$$


::: {.callout-caution collapse="true" title="Prueba"}

Como $f'(a) = S'(a) = S'(x_0) = b_0$, la ecuación (1.9) con $j=0$ implica que:

$$
f'(a) = \tfrac{1}{h_0}(a_1 - a_0) - \tfrac{h_0}{3}(2c_0 + c_1).
$$

En consecuencia:

$$
2h_0c_0 + h_0c_1 = \tfrac{3}{h_0}(a_1 - a_0) - 3f'(a).
$$

---

De forma similar:

$$
f'(b) = b_n = b_{n-1} + h_{n-1}(c_{n-1} + c_n).
$$

De la ecuación (1.9) con $j=n-1$ se obtiene:

$$
f'(b) = \tfrac{a_n - a_{n-1}}{h_{n-1}}
- \tfrac{h_{n-1}}{3}(2c_{n-1} + c_n) + h_{n-1}(c_{n-1} + c_n),
$$

es decir,

$$
f'(b) = \tfrac{a_n - a_{n-1}}{h_{n-1}} + \tfrac{h_{n-1}}{3}(c_{n-1} + 2c_n).
$$

Por tanto:

$$
h_{n-1}c_{n-1} + 2h_{n-1}c_n 
= 3f'(b) - \tfrac{3}{h_{n-1}}(a_n - a_{n-1}).
$$

Las ecuaciones (1.10), junto con:

$$
2h_0c_0 + h_0c_1 = \tfrac{3}{h_0}(a_1 - a_0) - 3f'(a),
$$

y

$$
h_{n-1}c_{n-1} + 2h_{n-1}c_n = 3f'(b) - \tfrac{3}{h_{n-1}}(a_n - a_{n-1}),
$$

determinan el sistema lineal $Ax = b$, donde:

$$
A =
\begin{bmatrix}
2h_0 & h_0 & 0 & \cdots & 0 \\
h_0 & 2(h_0+h_1) & h_1 & \cdots & 0 \\
0 & h_1 & 2(h_1+h_2) & \cdots & h_2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & h_{n-2} & 2(h_{n-2}+h_{n-1}) & h_{n-1} \\
0 & \cdots & 0 & h_{n-1} & 2h_{n-1}
\end{bmatrix},
$$

$$
b =
\begin{bmatrix}
\tfrac{3}{h_0}(a_1 - a_0) - 3f'(a) \\
\tfrac{3}{h_1}(a_2 - a_1) - \tfrac{3}{h_0}(a_1 - a_0) \\
\vdots \\
\tfrac{3}{h_{n-1}}(a_n - a_{n-1}) - \tfrac{3}{h_{n-2}}(a_{n-1} - a_{n-2}) \\
3f'(b) - \tfrac{3}{h_{n-1}}(a_n - a_{n-1})
\end{bmatrix},
\qquad
x =
\begin{bmatrix}
c_0 \\
c_1 \\
\vdots \\
c_n
\end{bmatrix}.
$$

La matriz $A$ es estrictamente diagonalmente dominante, por tanto, el sistema lineal tiene solución única para $c_0, c_1, \ldots, c_n$, como veremos más adelante en el capítulo de solución de sistemas lineales. $\blacksquare$

:::

##### Algoritmo: Spline Cúbico Sujeto (Clamped)

**Entrada:**  
$n; \; x_0, x_1, \ldots, x_n; \; a_0 = f(x_0), \; a_1 = f(x_1), \ldots, a_n = f(x_n);$  
$\text{FDX0} = f'(x_0), \; \text{FDXN} = f'(x_n)$.  

**Salida:**  
$a_j, b_j, c_j, d_j \quad$ para $j = 0,1,\ldots,n-1$.  

**Nota:**  
En cada subintervalo $[x_j, x_{j+1}]$ se tiene:

$$
S_j(x) = a_j + b_j(x-x_j) + c_j(x-x_j)^2 + d_j(x-x_j)^3.
$$

**Paso 1:** Para $i = 0,1,\ldots,n-1$ defina:

$$
h_i = x_{i+1} - x_i.
$$

**Paso 2 (Condiciones aseguradas):** Defina:

$$
\alpha_0 = 3 \frac{a_1 - a_0}{h_0} - 3 \,\text{FDX0}, 
\qquad
\alpha_n = 3 \,\text{FDXN} - 3 \frac{a_n - a_{n-1}}{h_{n-1}}.
$$

**Paso 3:** Para $i = 1,2,\ldots,n-1$ defina:

$$
\alpha_i = \frac{3}{h_i}(a_{i+1} - a_i) - \frac{3}{h_{i-1}}(a_i - a_{i-1}).
$$

**Paso 4:** Defina:

$$
\ell_0 = 2h_0, 
\qquad \mu_0 = \tfrac{1}{2}, 
\qquad z_0 = \tfrac{\alpha_0}{\ell_0}.
$$

**Paso 5:** Para $i = 1,2,\ldots,n-1$ sea:

$$
\ell_i = 2(x_{i+1} - x_{i-1}) - h_{i-1}\mu_{i-1}, 
\qquad \mu_i = \tfrac{h_i}{\ell_i}, 
\qquad z_i = \tfrac{\alpha_i - h_{i-1}z_{i-1}}{\ell_i}.
$$

**Paso 6:** Defina:

$$
\ell_n = h_{n-1}(2 - \mu_{n-1}), 
\qquad z_n = \tfrac{\alpha_n - h_{n-1}z_{n-1}}{\ell_n}, 
\qquad c_n = z_n.
$$

**Paso 7 (Sustitución hacia atrás):**  
Para $j = n-1, n-2, \ldots, 0$ haga:

$$
c_j = z_j - \mu_j c_{j+1},
$$

$$
b_j = \tfrac{a_{j+1} - a_j}{h_j} - \tfrac{h_j}{3}(c_{j+1} + 2c_j),
$$

$$
d_j = \tfrac{c_{j+1} - c_j}{3h_j}.
$$

**Paso 8:**  
Retorne $\{a_j, b_j, c_j, d_j\}_{j=0}^{n-1}$.

:::


# Integración Numérica

## Derivacón e Integración Numérica

### Cuadratura Numérica

#### Diferenciación numérica

En el capítulo anterior se obtuvo que:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)(x-x_1)\cdots(x-x_n),
$$

donde $P_n(x)$ es el polinomio de Lagrange.  
La idea es derivar $P_n(x)$ en lugar de $f(x)$.


##### Fórmula de dos puntos

Se tienen dos nodos $x_0, x_1$, entonces:

$$
f(x) = P_{01}(x) + \frac{f^{(2)}(\xi_x)}{2!}(x-x_0)(x-x_1),
$$

esto implica que:

$$
f(x) = \frac{(x-x_1)f(x_0) - (x-x_0)f(x_1)}{x_0 - x_1} 
+ \frac{f^{(2)}(\xi_x)}{2!}(x-x_0)(x-x_1).
$$

Si $f \in C^2[a,b]$ y tomamos los nodos $x_0$ y $x_1 = x_0 + h$,  
con $h$ suficientemente pequeño, se tiene que:

Continuando, se obtiene:

$$
f(x) = \frac{(x-x_0-h)f(x_0) - (x-x_0)f(x_0+h)}{x_0 - (x_0-h)}
+ \frac{f^{(2)}(\xi_x)}{2!}(x-x_0)(x-x_0-h),
$$

$$
= \frac{(x-x_0)f(x_0+h) - (x-x_0-h)f(x_0)}{h}
+ \frac{f^{(2)}(\xi_x)}{2!}(x-x_0)(x-x_0-h),
$$

de donde:

$$
f'(x) = \frac{f(x_0+h) - f(x_0)}{h} 
+ \frac{d}{dx}\left[ \frac{f^{(2)}(\xi_x)}{2!}(x-x_0)(x-x_0-h) \right].
$$

Esto implica que:

$$
f'(x) = \frac{f(x_0+h) - f(x_0)}{h} 
+ \frac{f^{(3)}(\xi_x)}{2!}\xi'_x (x-x_0)(x-x_0-h) 
+ \frac{f^{(2)}(\xi_x)}{2!}\left[(x-x_0-h) + (x-x_0)\right].
$$

Si $x = x_0$ se tiene que:

$$
f'(x_0) = \frac{f(x_0+h) - f(x_0)}{h} 
+ \frac{f^{(2)}(\xi_x)}{2!}\left[(x_0-x_0-h) + (x_0-x_0)\right],
$$

$$
= \frac{f(x_0+h) - f(x_0)}{h} - \frac{h}{2}f^{(2)}(\xi_x).
$$

Por lo tanto:

$$
f'(x_0) = \frac{f(x_0+h) - f(x_0)}{h}, 
\quad \text{con error absoluto igual a } \tfrac{h}{2}f^{(2)}(\xi_x).
$$

En general, si se tienen $(n+1)$ nodos $x_0, x_1, \ldots, x_n$ en $[a,b]$ y $f \in C^{n+1}[a,b]$, entonces:

$$
f(x) = \sum_{k=0}^n f(x_k)L_k(x) 
+ \frac{(x-x_0)(x-x_1)\cdots(x-x_n)}{(n+1)!}f^{(n+1)}(\xi_x),
\quad \xi_x \in ]a,b[,
$$

de donde se tiene que:

$$
f'(x) = \sum_{k=0}^n f(x_k)L_k'(x) 
+ \frac{d}{dx}\left[\frac{(x-x_0)(x-x_1)\cdots(x-x_n)}{(n+1)!}\right] f^{(n+1)}(\xi_x)
+ \frac{(x-x_0)(x-x_1)\cdots(x-x_n)}{(n+1)!} f^{(n+2)}(\xi_x)\xi'_x.
$$
Si $x = x_k$ se tiene que:

$$
f'(x_k) = \sum_{k=0}^n f(x_k) L_k'(x_k) 
+ \frac{f^{(n+1)}(\xi_x)}{(n+1)!}(x_k - x_0)(x_k - x_1)\cdots(x_k - x_{k-1})(x_k - x_{k+1})\cdots(x_k - x_n),
$$

los demás términos se anulan en $x_k$, de donde se tiene que:

$$
f'(x_k) = \sum_{k=0}^n f(x_k) L_k'(x_k) 
+ \frac{f^{(n+1)}(\xi_x)}{(n+1)!} \prod_{\substack{j=0 \\ j \neq k}}^n (x_k - x_j).
$$

##### Fórmula de $(n+1)$ puntos

**Fórmulas de 3 puntos:** $x_0, x_1, x_2$

En este caso:

$$
f'(x_k) = \sum_{k=0}^2 f(x_k) L_k'(x_k) 
+ \frac{f^{(2+1)}(\xi_x)}{(2+1)!} \prod_{\substack{j=0 \\ j \neq k}}^2 (x_k - x_j),
$$

así:

$$
L_0(x) = \frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}
= \frac{x^2 - (x_1+x_2)x + x_1x_2}{(x_0-x_1)(x_0-x_2)},
$$

$$
L_1(x) = \frac{x^2 - (x_0+x_2)x + x_0x_2}{(x_1-x_0)(x_1-x_2)},
$$

$$
L_2(x) = \frac{x^2 - (x_0+x_1)x + x_0x_1}{(x_2-x_0)(x_2-x_1)}.
$$

De donde:

$$
L_0'(x) = \frac{2x - (x_1+x_2)}{(x_0-x_1)(x_0-x_2)},
$$

$$
L_1'(x) = \frac{2x - (x_0+x_2)}{(x_1-x_0)(x_1-x_2)}.
$$
De $L_2'(x)$ se tiene que:

$$
L_2'(x) = \frac{2x - (x_0+x_1)}{(x_2-x_0)(x_2-x_1)}.
$$

Tomando $x = x_k$ se tiene que:

$$
f'(x_k) \approx f(x_0)\left[\frac{2x_k - (x_1+x_2)}{(x_0-x_1)(x_0-x_2)}\right]
+ f(x_1)\left[\frac{2x_k - (x_0+x_2)}{(x_1-x_0)(x_1-x_2)}\right]
+ f(x_2)\left[\frac{2x_k - (x_0+x_1)}{(x_2-x_0)(x_2-x_1)}\right].
$$

De modo que, si se toman los nodos igualmente espaciados $x_0, \; x_1 = x_0 + h, \; x_2 = x_0 + 2h$,  
con $h = \tfrac{x_2 - x_0}{2}$, se tienen las siguientes fórmulas:

**Fórmula A:** Tome $x_k = x_0$

$$
f'(x_0) \approx 
f(x_0)\left[\frac{2x_0 - x_0 - h - x_0 - 2h}{(x_0-x_0-h)(x_0-x_0-2h)}\right]
+ f(x_0+h)\left[\frac{2x_0 - x_0 - x_0 - 2h}{(x_0+h-x_0)(x_0+h-x_0-2h)}\right]
+ f(x_0+2h)\left[\frac{2x_0 - x_0 - x_0 - h}{(x_0+2h-x_0)(x_0+2h-x_0-h)}\right].
$$

Simplificando:

$$
f'(x_0) = f(x_0)\left[-\tfrac{3h}{2h^2}\right]
+ f(x_0+h)\left[-\tfrac{2h}{-h^2}\right]
+ f(x_0+2h)\left[-\tfrac{h}{-2h^2}\right].
$$

De donde:

$$
f'(x_0) \approx \tfrac{1}{h}\left[-\tfrac{3}{2}f(x_0) + 2f(x_0+h) - \tfrac{1}{2}f(x_0+2h)\right].
$$

**Fórmula B:** Tome $x_k = x_1 = x_0 + h$

Se puede probar fácilmente que (ejercicio):

$$
f'(x_0+h) \approx \tfrac{1}{h}\left[-\tfrac{1}{2}f(x_0) + \tfrac{1}{2}f(x_0+2h)\right].
$$

Luego, haciendo el cambio de variable $\tilde{x}_0 = x_0 + h$, se obtiene que:

$$
f'(\tilde{x}_0) = \tfrac{1}{h}\left[-\tfrac{1}{2}f(\tilde{x}_0-h) + \tfrac{1}{2}f(\tilde{x}_0+h)\right].
$$

#### Integración Numérica


La idea es integrar el polinomio de Lagrange $P(x)$ en lugar de la función original,  
es decir, hacer la siguiente aproximación:

$$
\int_a^b f(x)\,dx \;\approx\; \int_a^b P_n(x)\,dx,
$$

sabemos que:

$$
f(x) = P_n(x) + \frac{f^{(n+1)}(\xi_x)}{(n+1)!} \prod_{i=1}^n (x-x_i),
$$

lo cual implica que:

$$
f(x) = \sum_{i=0}^n f(x_i)L_i(x) 
+ \frac{f^{(n+1)}(\xi_x)}{(n+1)!} \prod_{i=1}^n (x-x_i),
$$

de donde se tiene que:

$$
\int_a^b f(x)\,dx 
= \sum_{i=0}^n f(x_i)\int_a^b L_i(x)\,dx
+ \int_a^b \frac{f^{(n+1)}(\xi_x)}{(n+1)!}\prod_{i=1}^n (x-x_i)\,dx,
$$

tomado $\xi_x \in ]a,b[$.


**La Regla del Trapecio**

Usamos dos nodos igualmente espaciados $x_0 = a, \; x_1 = x_0+h$, con $h = b-a$,  
es decir, $x_1 = b$, de donde:

$$
f(x) = \frac{(x-x_1)f(x_0) - (x-x_0)f(x_1)}{x_0-x_1}
+ \frac{f''(\xi_x)}{2}(x-x_0)(x-x_1).
$$

Entonces:

$$
\int_a^b f(x)\,dx 
= \int_a^b \frac{(x-x_1)f(x_0)}{x_0-x_1}\,dx
- \int_a^b \frac{(x-x_0)f(x_1)}{x_0-x_1}\,dx
+ \int_a^b \frac{f''(\xi_x)}{2}(x-x_0)(x-x_1)\,dx.
$$

Sean:

$$
I_1 = \int_a^b \frac{(x-x_1)f(x_0)}{x_0-x_1}\,dx
+ \int_a^b \frac{(x-x_0)f(x_1)}{x_1-x_0}\,dx,
$$

$$
I_2 = \int_a^b \frac{f''(\xi_x)}{2}(x-x_0)(x-x_1)\,dx.
$$

Calculemos $I_1$:

\begin{align*}
  I_1 &= \int_{x_0}^{x_1}\frac{(x-x_1)f(x_0)}{x_0-x_1}\,dx
+ \int_{x_0}^{x_1}\frac{(x-x_0)f(x_1)}{x_1-x_0}\,dx.\\
&= \frac{f(x_0)}{x_0-x_1}\left[\frac{(x-x_1)^2}{2}\right]_{x_0}^{x_1}
+ \frac{f(x_1)}{x_1-x_0}\left[\frac{(x-x_0)^2}{2}\right]_{x_0}^{x_1}\\
&= \frac{1}{2}\frac{f(x_0)}{(x_0-x_1)}\left[(x_1-x_1)^2 - (x_0-x_1)^2\right]
+ \frac{1}{2}\frac{f(x_1)}{(x_1-x_0)}\left[(x_1-x_0)^2 - (x_0-x_0)^2\right]\\
&= -\tfrac{1}{2}f(x_0)(x_0-x_1) + \tfrac{1}{2}f(x_1)(x_1-x_0).\\
&= \tfrac{1}{2}(x_1-x_0)[f(x_0)+f(x_1)].
\end{align*}

De donde se tiene que:

$$
I_1 = \tfrac{h}{2}[f(x_0) + f(x_1)] 
= \tfrac{(b-a)}{2}[f(a) + f(b)].
$$
![Regla de Trapecio](Imagenes/Regla de Trapecio.png){width=400px fig-align="center"}

(Usando el Teorema del Valor Medio para Integrales podemos calcular $I_2$:)

$$
I_2 = \int_a^b \frac{f''(\xi_x)}{2}(x-x_0)(x-x_1)\,dx
= -\frac{h^3}{12} f''(\xi_x).
$$

De donde se tiene que:

$$
\int_a^b f(x)\,dx 
= \tfrac{(b-a)}{2}[f(a)+f(b)] - \tfrac{h^3}{12} f''(\xi_x).
$$

Finalmente, sumando $I_1$ e $I_2$ se tiene la **fórmula del trapecio**:

$$
\int_a^b f(x)\,dx \;\approx\; \tfrac{(b-a)}{2}[f(a)+f(b)].
$$
**Aproximación de Integrales Definidas**
```{r}
#| code-fold: true

# Calcula la integral de F de a hasta b usando la regla del Trapecio.
# Regla del trapecio simple
Trapecio <- function(F, a, b) {
  return(((b - a) / 2) * (F(a) + F(b)))
}

# Calcula la integral de F de a hasta b usando la regla de Simpson.
Simpson <- function(F, a, b) {
  return(((b - a) / 6) * (F(a) + 4 * F((a + b) / 2) + F(b)))
}

# Calcula la integral de F de a hasta b usando la Regla de Simpson Compuesta.
SimpsonCompuesto <- function(F, a, b, m) {
  h  <- (b - a) / (2 * m)
  XI0 <- F(a) + F(b)
  XI1 <- 0  # suma de puntos impares
  XI2 <- 0  # suma de puntos pares
  for (j in 1:(2 * m - 1)) {
    X <- a + j * h
    if (j %% 2 == 0) {
      XI2 <- XI2 + F(X)
    } else {
      XI1 <- XI1 + F(X)
    }
  }
  return((h / 3) * (XI0 + 2 * XI2 + 4 * XI1))
}
```
**Ejemplos**
```{r}
#| code-fold: true

# Definición de la función f
f <- function(x) sqrt(1 + x^2)
# Evaluación Trapecio
print(Trapecio(f, 0, 1), digits = 15)
# Evaluación Simpson
print(Simpson(f, 0, 1), digits = 15)
# Evaluación Simpson Compuesta
print(SimpsonCompuesto(f, 0, 1, 50), digits = 15)
# Usando integrate() de R para comparar con el trapecio
resultado <- integrate(f, lower = 0, upper = 1)
print(resultado$value, digits = 15)
```

::: {.callout-tip collapse="true" title="Ejemplo"}

Calcule $\int_0^1 \sqrt{1+x^2}\,dx$ con la regla del trapecio.  

**Solución:** Con $h=1$:

$$
\int_0^1 \sqrt{1+x^2}\,dx 
\approx \tfrac{1}{2}[\sqrt{1} + \sqrt{2}] 
= 1.2071068.
$$

En forma exacta:

$$
\int_0^1 \sqrt{1+x^2}\,dx = 1.1477936.
$$

Se tiene un Error Absoluto $= 0.0593131$. $\blacksquare$

:::

::: {.callout-tip collapse="true" title="Ejemplo"}

Aproximar $\int_0^1 e^x\,dx$ con la regla del trapecio.  

**Solución:**

$$
\int_0^1 e^x\,dx 
\approx \tfrac{1}{2}[e^0 + e^1] 
= 1.8591409.
$$

El valor exacto es:

$$
\int_0^1 e^x\,dx = e^1 - e^0 = 1.7182818.
$$

Error Absoluto $= 0.140859$.  

Error Teórico:

$$
\left|\frac{h^3 f''(\xi_x)}{12}\right|
= \frac{1}{12}e^{\xi_x} \leq 0.2265234,
\qquad \xi_x \in ]0,1[. \;\; \blacksquare
$$

:::


**La Regla de Simpson**

Usando tres nodos igualmente espaciados $x_0,\; x_1 = x_0+h,\; x_2 = x_0+2h$,  
con $h = \tfrac{b-a}{2}$, se tiene que:

$$
\begin{aligned}
f(x) &= \frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}f(x_0) 
+ \frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}f(x_1) \\
&\quad + \frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}f(x_2) 
+ \frac{f''(\xi_x)}{3!}(x-x_0)(x-x_1)(x-x_2).
\end{aligned}
$$

De donde:

$$
\int_a^b f(x)\,dx 
= \int_{x_1}^{x_2} \frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} f(x_0)\,dx 
+ \cdots + \int_{x_0}^{x_1} \frac{f''(\xi_x)}{3!}(x-x_0)(x-x_1)(x-x_2)\,dx.
$$

Desarrollando esta expresión se tiene que (Ejercicio):

$$
\int_a^b f(x)\,dx 
= \frac{h}{3}[f(x_0) + 4f(x_1) + f(x_2)] 
- \frac{h^5}{90} f^{(4)}(\xi_x),
$$

lo cual implica que:

$$
\int_a^b f(x)\,dx 
= \frac{h}{3}[f(a) + 4f(a+h) + f(a+2h)] 
- \frac{h^5}{90} f^{(4)}(\xi_x),
$$

y por lo tanto:

$$
\int_a^b f(x)\,dx 
= \frac{b-a}{6}\left[f(a) + 4f\!\left(\tfrac{a+b}{2}\right) + f(b)\right] 
- \frac{h^5}{90} f^{(4)}(\xi_x).
$$
![Regla de Simpson](Imagenes/Regla de Simpson.png){width=400px fig-align="center"}
![Comparación entre la Regla del Trapecio y la Regla de Simpson](Imagenes/ReglaTrapecio vs RegraSimpson.png){width=400px fig-align="center"}

##### Aproximación de Integrales Definidas

```{r}
#| code-fold: true

# Calcula la integral de F de a hasta b usando la regla del Trapecio.
# Regla del trapecio simple
Trapecio <- function(F, a, b) {
  return(((b - a) / 2) * (F(a) + F(b)))
}

# Calcula la integral de F de a hasta b usando la regla de Simpson.
Simpson <- function(F, a, b) {
  return(((b - a) / 6) * (F(a) + 4 * F((a + b) / 2) + F(b)))
}

# Calcula la integral de F de a hasta b usando la Regla de Simpson Compuesta.
SimpsonCompuesto <- function(F, a, b, m) {
  h  <- (b - a) / (2 * m)
  XI0 <- F(a) + F(b)
  XI1 <- 0  # suma de puntos impares
  XI2 <- 0  # suma de puntos pares
  for (j in 1:(2 * m - 1)) {
    X <- a + j * h
    if (j %% 2 == 0) {
      XI2 <- XI2 + F(X)
    } else {
      XI1 <- XI1 + F(X)
    }
  }
  return((h / 3) * (XI0 + 2 * XI2 + 4 * XI1))
}
```


```{r}
# Definición de la función f
f <- function(x) sqrt(1 + x^2)
# Evaluación Trapecio
print(Trapecio(f, 0, 1), digits = 15)
# Evaluación Simpson
print(Simpson(f, 0, 1), digits = 15)
# Evaluación Simpson Compuesta
print(SimpsonCompuesto(f, 0, 1, 50), digits = 15)
# Usando integrate() de R para comparar con el trapecio
resultado <- integrate(f, lower = 0, upper = 1)
print(resultado$value, digits = 15)
```

::: {.callout-note title="Definición: Fórmula de cuadratura"}

Se llama **fórmula de cuadratura** a una expresión de la forma:

$$
\int_a^b f(x)\,dx = \sum_{i=0}^{n} a_i f(x_i) + E(f).
$$

:::

::: {.callout-note title="Definición: Grado de precisión o exactitud algebraica"}

El **grado de precisión** o **exactitud algebraica** de una fórmula de cuadratura es un entero $n$ tal que:

- $E(P_k) = 0, \quad \forall k \le n.$  
- $E(P_{n+1}) \ne 0.$

:::

::: {.callout-important collapse="true" title="Observación"}

Para tener exactitud algebraica en una fórmula de cuadratura igual a $n$,  
es suficiente probar que:

$$
E(x^k) = 0, \quad \forall k \le n, \quad \text{y que} \quad E(x^{n+1}) \ne 0.
$$

:::

::: {.callout-tip title="Ejemplo"}

- Regla de **Simpson** → Precisión = 3.  
- Regla del **Trapecio** → Precisión = 1.  

:::

::: {.callout-note title="Definición: Operador del error"}

$$
E(f) = \int_a^b f(x)\,dx - \sum_{i=0}^{n} a_i f(x_i),
$$

se llama **el operador del error**.

:::

::: {.callout-tip title="Ejemplo: Regla de Simpson"}

La **regla de Simpson** con $a = -1$, $b = 1$ es:

$$
\int_{-1}^{1} f(x)\,dx = \frac{1}{6}\,[f(-1) + 4f(0) + f(1)] + E(f),
$$

lo cual implica que:

$$
E(f) = \int_{-1}^{1} f(x)\,dx - \frac{1}{6}\,[f(-1) + 4f(0) + f(1)].
$$

Se puede probar que $E$ anula a $1, x, x^2, x^3$ y que $E(x^4) \ne 0$ (ejercicio),  
por lo tanto la **exactitud algebraica** de la regla de Simpson es **3**.

:::

::: {.callout-note title="Fórmula de Gauss–Chebyshev"}

Se desea encontrar una fórmula de cuadratura del tipo:

$$
\int_{-1}^{1} f(x)\,dx = f(-\alpha) + f(\alpha) + E(f),
$$

con **exactitud algebraica 2** y con $|\alpha| < 1$.

:::

**Solución:**

$$
E(1) = \int_{-1}^{1} 1\,dx - 1 - 1 = 2 - 2 = 0,
$$

$$
E(x) = \int_{-1}^{1} x\,dx + \alpha - \alpha = 0,
$$

$$
E(x^2) = \int_{-1}^{1} x^2\,dx - \alpha^2 - \alpha^2
= \left.\frac{x^3}{3}\right|_{-1}^{1} - 2\alpha^2 = 0,
$$

lo que implica:

$$
\frac{2}{3} - 2\alpha^2 = 0,
$$

de donde:

$$
\alpha = \pm \frac{\sqrt{3}}{3},
$$

por lo tanto:

$$
\int_{-1}^{1} f(x)\,dx = f\!\left(-\frac{\sqrt{3}}{3}\right) + f\!\left(\frac{\sqrt{3}}{3}\right) + E(f).
$$

Se puede probar que $E(x^3) = 0$ (ejercicio), pero que $E(x^4) \ne 0$,  
lo cual implica que la **exactitud algebraica** es igual a **3**. $\blacksquare$

::: {.callout-tip title="Ejemplo: Aplicación numérica de la fórmula de Gauss–Chebyshev"}

El valor aproximado es:

$$
\int_{-1}^{1} e^x\,dx \approx e^{\frac{\sqrt{3}}{3}} + e^{-\frac{\sqrt{3}}{3}}
= 1.7813122 + 0.5613839 = 2.3426961.
$$

Mientras que el valor exacto es:

$$
\int_{-1}^{1} e^x\,dx = e^x\big|_{-1}^{1} = e^1 - e^{-1} = 2.3504024.
$$

:::

::: {.callout-note title="Ejemplo: Deducción de la regla de Simpson"}

Deduza la regla de Simpson, usando el hecho de que la siguiente fórmula de cuadratura:

$$
\int_{-1}^{1} f(x)\,dx = a f(-1) + b f(0) + c f(1) + E(f),
$$

tiene **exactitud algebraica igual a 3**.

**Solución:**

- Consideremos primero $f(x) = 1$:

$$
E(1) = \int_{-1}^{1} 1\,dx - [a f(-1) + b f(0) + c f(1)] = 0,
$$

$$
\Rightarrow x\big|_{-1}^{1} - (a + b + c) = 0
\Rightarrow 2 - (a + b + c) = 0,
$$

de donde:

$$
a + b + c = 2.
$$

- Veamos $f(x) = x$:

$$
E(x) = \int_{-1}^{1} x\,dx - [a f(-1) + b f(0) + c f(1)] = 0,
$$

$$
\Rightarrow \frac{x^2}{2}\Big|_{-1}^{1} - (-a + c) = 0 
\Rightarrow a - c = 0.
$$

- Veamos $f(x) = x^2$:

$$
E(x^2) = \int_{-1}^{1} x^2\,dx - [a f(-1) + b f(0) + c f(1)] = 0,
$$

$$
\Rightarrow \frac{x^3}{3}\Big|_{-1}^{1} - a - c = 0 
\Rightarrow \frac{2}{3} - a - c = 0,
$$

$$
\Rightarrow a + c = \frac{2}{3}.
$$

Por lo tanto, se debe resolver el sistema:

$$
\begin{cases}
a + b + c = 2, \\
a - c = 0, \\
a + c = \dfrac{2}{3}.
\end{cases}
$$

De donde:

$$
a = \frac{1}{3}, \quad b = \frac{4}{3}, \quad c = \frac{1}{3}.
$$

Por lo tanto, la **regla de Simpson** es:

$$
\int_{-1}^{1} f(x)\,dx \approx 
\frac{1}{3}f(-1) + \frac{4}{3}f(0) + \frac{1}{3}f(1).
$$

$\blacksquare$

:::

**Integración Numérica Compuesta**

![Regla del Trapecio Compuesta](Imagenes/Regla del Trapecio Compuesta.png){width=400px fig-align="center"}

![Regla del Trapecio Compuesta](Imagenes/Regla del Trapecio Compuesta2.png){width=400px fig-align="center"}

La idea es calcular 

$$
\int_a^b f(x)\,dx
$$ 

usando $n = 2m$ subintervalos, luego aplicar la regla de Simpson $m$ veces a cada par de intervalos (por lo tanto, $n$ debe ser par).

La regla de Simpson es la siguiente:

$$
\int_a^b f(x)\,dx = \int_{x_0}^{x_2} f(x)\,dx 
= \frac{h}{3}\,[f(x_0) + 4f(x_1) + f(x_2)] 
- \frac{h^5}{90}\,f^{(4)}(\xi_x),
$$

con $\xi_x \in [x_0, x_2]$.  
Además, sean:

$$
h = \frac{b - a}{n} = \frac{b - a}{2m}, 
\quad a = x_0 < x_1 < \dots < x_m = x_n = b,
$$

con $x_j = x_0 + jh$, para $j = 0, 1, 2, \dots, 2m$.  
Existe $\xi_j \in [x_{2j-2}, x_{2j}]$ tal que:

$$
\int_{x_{2j-2}}^{x_{2j}} f(x)\,dx 
= \frac{h}{3}\,[f(x_{2j-2}) + 4f(x_{2j-1}) + f(x_{2j})]
- \frac{h^5}{90}\,f^{(4)}(\xi_j),
$$

entonces:

\begin{align*}
\int_a^b f(x)\,dx 
&= \sum_{j=1}^{m} \int_{x_{2j-2}}^{x_{2j}} f(x)\,dx \\[1em]
&= \frac{h}{3} \left[ 
\sum_{j=1}^{m} f(x_{2j-2}) 
+ 4\sum_{j=1}^{m} f(x_{2j-1})
+ \sum_{j=1}^{m} f(x_{2j})
\right]
- \frac{h^5}{90} \sum_{j=1}^{m} f^{(4)}(\xi_j).
\end{align*}


Reorganizando términos, se obtiene:


\begin{align*}
\int_a^b f(x)\,dx 
&= \frac{h}{3} \left[
f(x_0) + 2\sum_{j=1}^{m-1} f(x_{2j}) 
+ 4\sum_{j=1}^{m} f(x_{2j-1})
+ f(x_{2m})
\right]
- \frac{h^5}{90} \sum_{j=1}^{m} f^{(4)}(\xi_j).
\end{align*}

**Regla conocida como:**  
$$
\boxed{\text{La Fórmula SC1 (Simpson Compuesta)}}
$$

::: {.callout-note title="Teorema: Error en la Regla de Simpson Compuesta"}

Si $f \in C^4[a,b]$, entonces existe $\mu \in ]a,b[$ tal que la fórmula **SC1** se puede escribir como:

$$
\int_a^b f(x)\,dx
= \frac{h}{3}
\left[
f(a)
+ 2\sum_{j=1}^{m-1} f(x_{2j})
+ 4\sum_{j=1}^{m} f(x_{2j-1})
+ f(b)
\right]
- \frac{(b-a)^5}{2880\,m^4}\, f^{(4)}(\mu).
$$

![Regla del Simpson Compuesta](Imagenes/Regla de Simpson Compuesta.png){width=400px fig-align="center"}

::: {.callout-caution collapse="true" title="Prueba"}

Hay que probar que existe $\mu \in ]a,b[$ tal que:

$$
\frac{h^5}{90} \sum_{j=1}^{m} f^{(4)}(\xi_j)
= \frac{(b-a)^5}{2880\,m^4} f^{(4)}(\mu),
$$

con $\xi_j \in ]x_{2j-2}, x_{2j}] \subset ]a,b[$.

Se sabe que:

$$
\min_{x \in [a,b]} f^{(4)}(x)
\le f^{(4)}(\xi_j)
\le \max_{x \in [a,b]} f^{(4)}(x),
$$

por lo que:

$$
m \min_{x \in [a,b]} f^{(4)}(x)
\le \sum_{j=1}^{m} f^{(4)}(\xi_j)
\le m \max_{x \in [a,b]} f^{(4)}(x),
$$

y dividiendo entre $m$:

$$
\min_{x \in [a,b]} f^{(4)}(x)
\le \frac{1}{m} \sum_{j=1}^{m} f^{(4)}(\xi_j)
\le \max_{x \in [a,b]} f^{(4)}(x).
$$

Luego, por el **Teorema del Valor Intermedio**, existe $\mu \in ]a,b[$ tal que:

$$
f^{(4)}(\mu)
= \frac{1}{m} \sum_{j=1}^{m} f^{(4)}(\xi_j),
$$

lo cual implica que:

$$
m f^{(4)}(\mu)
= \sum_{j=1}^{m} f^{(4)}(\xi_j).
$$

Por tanto, tenemos que:


\begin{align*}
\frac{h^5}{90} \sum_{j=1}^{m} f^{(4)}(\xi_j)
&= \frac{h^5}{90} m f^{(4)}(\mu) \\[1em]
&= \frac{\left(\dfrac{b-a}{2m}\right)^5 m}{90} f^{(4)}(\mu) \\[1em]
&= \frac{(b-a)^5}{2^5 \cdot 90 \, m^4} f^{(4)}(\mu) \\[1em]
&= \frac{(b-a)^5}{2880\,m^4} f^{(4)}(\mu).
\end{align*}


$\blacksquare$

:::

:::

::: {.callout-note title="Algoritmo 1: Método de Simpson Compuesto"}

**Entrada:** $a, b, m, f$  

**Salida:** $XI$, valor aproximado de la integral $\displaystyle \int_a^b f(x)\,dx$.

   **Paso 1.** Tomar $h = \frac{b - a}{2m}.$

   **Paso 2.** Inicializar:

      $XI0 = f(a) + f(b)$
      
      $XI1 = 0 \quad \text{(impares)}$
      
      $XI2 = 0 \quad \text{(pares)}$

   **Paso 3.** Para $j = 1$ hasta $2m - 1$, seguir los pasos 4–5:

      **Paso 4.** Calcular  $x = a + jh.$

      **Paso 5.** Si $j$ es par:
$$
XI2 = XI2 + f(x),
$$
            si no:
$$
XI1 = XI1 + f(x).
$$

   **Paso 6.** Calcular el valor aproximado de la integral:

$$
XI = \frac{h}{3}\,[\,XI0 + 2*XI2 + 4\,XI1\,].
$$

   **Paso 7.** Salida $(XI)$.

   **Parar.**

:::

::: {.callout-note title="Ejemplo 8"}

Calcular $\displaystyle \int_0^3 e^x\,dx$ con **6 cifras significativas**.

**Solución**

Se tiene que el error de la regla de Simpson compuesta está dado por:

$$
\frac{(b - a)^5}{2880\,m^4} f^{(4)}(\mu) \le 10^{-6},
$$

por lo tanto:

$$
\frac{3^5}{2880\,m^4} f^{(4)}(\mu)
\le
\frac{3^5}{2880\,m^4} e^3
\le 10^{-6},
$$

lo que implica que:

$$
\frac{3^5 e^3}{2880} \le m^4,
$$

$$
1694717.2 \le m^4,
$$

$$
36 \le m.
$$

Por lo tanto, se requiere que:

$$
2m - 1 = 2(36) - 1 = 71.
$$

$\blacksquare$

:::


#### Integración Numérica Múltiple

Se quiere calcular integrales del siguiente tipo:

$$
\iint_R f(x,y)\,dA, 
\qquad 
R = \{(x,y)\,/\, a \le x \le b,\, c \le y \le d\},
$$

con $a, b, c, d$ constantes.

Entonces se debe calcular:

$$
\iint_R f(x,y)\,dA 
= \int_a^b \left[\int_c^d f(x,y)\,dy \right] dx.
$$

Si se aplica la **regla de Simpson Compuesta**, se tiene que para calcular  
$\displaystyle \int_c^d f(x,y)\,dy$, donde $x$ se toma como constante, se define:

$$
k = \frac{d - c}{2m}, 
\qquad 
y_j = c + jk, 
\qquad 
j = 0, 1, 2, \ldots, 2m.
$$

Entonces:

$$
\int_c^d f(x,y)\,dy 
= 
\frac{k}{3} 
\left[
f(x,y_0)
+ 2 \sum_{j=1}^{m-1} f(x,y_{2j})
+ 4 \sum_{j=1}^{m} f(x,y_{2j-1})
+ f(x,y_m)
\right]
-
\frac{(d-c)\,k^4}{180}
\frac{\partial^4 f(x,\mu)}{\partial y^4},
$$

donde $\mu \in ]c,d[$.

![Integración en Regiones NO Rectángulares](Imagenes/Integraci´on en Regiones NO Rect´angulares..png){width=400px fig-align="center"}

![Integración en Regiones NO Rectángulares](Imagenes/Integraci´on en Regiones NO Rect´angulares.png){width=400px fig-align="center"}

Con $\mu \in ]c, d[$, se tiene que:

\begin{align*}
\int_a^b \int_c^d f(x,y)\,dy\,dx
=
&\frac{k}{3}
\left[
\int_a^b f(x,y_0)\,dx
+ 2 \sum_{j=1}^{m-1} \int_a^b f(x,y_{2j})\,dx\right.\\
&\left.+ 4 \sum_{j=1}^{m} \int_a^b f(x,y_{2j-1})\,dx
+ \int_a^b f(x,y_m)\,dx
\right]\\
&-
\frac{(d-c)\,k^4}{180}
\int_a^b 
\frac{\partial^4 f(x,\mu)}{\partial y^4}\,dx.
\end{align*}

Pero la **Regla de Simpson Compuesta** establece que:

$$
\int_a^b f(x)\,dx 
= 
\frac{h}{3}
\left[
f(a)
+ 2 \sum_{j=1}^{m-1} f(x_{2j})
+ 4 \sum_{j=1}^{m} f(x_{2j-1})
+ f(b)
\right]
-
\frac{(b-a)^5}{2880\,m^4} f^{(4)}(\mu),
$$

donde 

$$
h = \frac{b - a}{2n}, 
\qquad
x_i = x_0 + ih, 
\qquad
i = 1, 2, \ldots, 2n.
$$

Luego, aplicando Simpson Compuesta se tiene que:

\begin{align*}
\int_a^b f(x, \alpha)\,dy 
&= \frac{h}{3} \left[ 
f(x_0, \alpha) 
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, \alpha) 
+ 4 \sum_{i=1}^{n} f(x_{2i-1}, \alpha) 
+ f(x_{2n}, \alpha) 
\right] \\
&\quad - \frac{(b-a)h^4}{180} \frac{\partial^4 f(\xi_\alpha, \alpha)}{\partial x^4},
\end{align*}

Sustituyendo se tiene que

\begin{align*}
\int_a^b \int_c^d f(x,y)\,dy\,dx =
&\frac{hk}{9}
\Bigg\{
f(x_0, y_0)
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, y_0)
+ 4 \sum_{i=1}^{n} f(x_{2i-1}, y_0)
+ f(x_{2n}, y_0) \\
&+ 2 \sum_{j=1}^{m-1}
\Big[
f(x_0, y_{2j})
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, y_{2j})
+ 4 \sum_{i=1}^{n} f(x_{2i-1}, y_{2j})
+ f(x_{2n}, y_{2j})
\Big] \\
&+ 4 \sum_{j=1}^{m}
\Big[
f(x_0, y_{2j-1})
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, y_{2j-1})
+ 4 \sum_{i=1}^{n} f(x_{2i-1}, y_{2j-1})
+ f(x_{2n}, y_{2j-1})
\Big]\\
&+ f(x_0, y_{2m})
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, y_{2m})
+ 4 \sum_{i=1}^{n} f(x_{2i-1}, y_{2m})
+ f(x_{2n}, y_{2m})
\Bigg\}
+ E
\end{align*}

donde el término de error $E$ está dado por:

\begin{align*}
E =
&-\frac{k(b-a)h^4}{540}
\Bigg[
\frac{\partial^4 f(\xi_0, y_0)}{\partial x^4}
+ 2 \sum_{i=1}^{n-1} \frac{\partial^4 f(\xi_{2i}, y_{2j})}{\partial x^4}\\
&+ 4 \sum_{j=1}^{m}
\Big(
\frac{\partial^4 f(\xi_{2j-1}, y_{2j-1})}{\partial x^4}
+ \frac{\partial^4 f(\xi_{2m}, y_{2m})}{\partial x^4}
\Big)
\Bigg]\\
&-\frac{(d-c)k^4}{180}
\int_a^b 
\frac{\partial^4 f(x, \mu)}{\partial y^4}
\frac{\partial^4 f(\xi_{2j}, y_{2j})}{\partial x^4}
\,dx.
\end{align*}

donde

\begin{align*}
\int_a^b \int_c^d f(x,y)\,dy\,dx &= \frac{hk}{9} \Bigg\{ 
f(x_0, y_0) 
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, y_0) 
+ 4 \sum_{i=1}^{n} f(x_{2i-1}, y_0) 
+ f(x_{2n}, y_0) \\
&\quad + 2 \sum_{j=1}^{m-1} f(x_0, y_{2j}) 
+ 4 \sum_{j=1}^{m-1} \sum_{i=1}^{n-1} f(x_{2i}, y_{2j}) 
+ 8 \sum_{j=1}^{m-1} \sum_{i=1}^{n} f(x_{2i-1}, y_{2j}) \\
&\quad + 2 \sum_{j=1}^{m-1} f(x_{2n}, y_{2j}) 
+ 4 \sum_{j=1}^{m-1} f(x_0, y_{2j-1}) 
+ 8 \sum_{j=1}^{m-1} \sum_{i=1}^{n} f(x_{2i-1}, y_{2j-1}) \\
&\quad + 4 \sum_{j=1}^{m-1} f(x_{2n}, y_{2j-1}) 
+ f(x_0, y_{2m}) 
+ 2 \sum_{i=1}^{n-1} f(x_{2i}, y_{2m}) \\
&\quad + 4 \sum_{i=1}^{n} f(x_{2i-1}, y_{2m}) 
+ f(x_{2n}, y_{2m}) 
\Bigg\} + E,
\end{align*}

Con:

$$
E = -\frac{(d-c)(b-a)}{180}
\left[
h^4 \frac{\partial^4 f(\bar{\eta}, \bar{\mu})}{\partial x^4}
+ 
k^4 \frac{\partial^4 f(\hat{\eta}, \hat{\mu})}{\partial y^4}
\right],
\qquad
(\bar{\eta}, \bar{\mu}) \in R, \; (\hat{\eta}, \hat{\mu}) \in R.
$$

::: {.callout-note title="Ejemplo 9"}

Calcule el error cometido al aproximar:

$$
\int_a^b \int_c^d e^x\,dy\,dx,
$$

con:

$$
h = \frac{1}{4}, 
\qquad 
k = \frac{1}{4}.
$$

**Solución:**

$$
|E| 
\le 
\frac{1}{180}
\left[
\left(\frac{1}{4}\right)^4 e^{\bar{\eta}}
+
\left(\frac{1}{4}\right)^4 \cdot 0
\right]
$$

$$
\le 
\frac{1}{180}
\left(\frac{1}{4}\right)^4 e^{1}
$$

$$
= 5.8 \times 10^{-5}.
$$

:::

**Segundo Caso: Integrales Tipo I**

![Integración en Regiones Tipo I](Imagenes/Integración de Regiones Tipo I.png){width=400px fig-align="center"}

Se desea calcular integrales del siguiente tipo:

$$
\iint_R f(x,y)\,dA 
= 
\int_a^b 
\int_{\varphi_1(x)}^{\varphi_2(x)} f(x,y)\,dy\,dx,
$$

con 

$$
h = \frac{b - a}{2n}, 
\qquad 
x_i = x_0 + ih, 
\qquad 
x_0 = a.
$$

Luego, para $i = 0, 1, 2, \ldots, 2n$, se debe calcular:

$$
\int_a^b 
\int_{\varphi_1(x)}^{\varphi_2(x)} 
f(x,y)\,dy\,dx,
\qquad
\text{usando Simpson Compuesto.}
$$

Recordemos el **Algoritmo de Simpson Compuesto**:

En R ver `3-Integrales_multiples.html`.


::: {.callout-note title="Ejemplo 10"}

Calcular

$$
\iint_R e^{-(x+y)}\,dA = 1.469840,
$$

con $n = m = 7$, donde $R$ es la región limitada por $y = x^2$ y $y = \sqrt{x}$.

:::

::: {.callout-note title="Algoritmo 2: Método de Simpson Compuesto"}

**Entrada:** $a, b, f, n$  

**Salida:** $XI$, valor aproximado de la integral $\displaystyle \int_a^b f(x)\,dx$.

   **Paso 1.** Tomar 
$$
h = \frac{b - a}{2n}.
$$

   **Paso 2.** Inicializar:

      $XI0 = f(a) + f(b)$
      
      $XI1 = 0 \quad \text{(impares)}$
      
      $XI2 = 0 \quad \text{(pares)}$

   **Paso 3.** Para $j = 1$ hasta $2n - 1$, seguir los pasos 4–5:

      **Paso 4.** Calcular  $x = a + jh.$

      **Paso 5.** Si $j$ es par:
$$
XI2 = XI2 + f(x),
$$
            si no:
$$
XI1 = XI1 + f(x).
$$

   **Paso 6.** Calcular el valor aproximado de la integral:

$$
XI = \frac{h}{3}\,[\,XI0 + 2*XI2 + 4\,XI1\,].
$$

   **Paso 7.** Salida $(XI)$.

   **Parar.**

:::

#### Extrapolación de Richardson


Suponga que se tiene una fórmula $N(h)$ que aproxima un valor desconocido $M$,  
y que:

$$
M = N(h) + K_1h^2 + O(h^4) \tag{1.1}
$$

Si se sustituye $h$ por $\dfrac{h}{2}$ en (1.1) se tiene que:

$$
M = N\!\left(\frac{h}{2}\right) + K_1\!\left(\frac{h}{2}\right)^2 + O\!\left(\left(\frac{h}{2}\right)^4\right),
$$

multiplicando todo por 4 se tiene que:

$$
4M = 4N\!\left(\frac{h}{2}\right) + 4K_1\!\left(\frac{h}{2}\right)^2 + 4O\!\left(\left(\frac{h}{2}\right)^4\right),
$$

Entonces:

$$
4M = 4N\!\left(\frac{h}{2}\right) + K_1h^2 + 4O\!\left(\left(\frac{h}{2}\right)^4\right), \tag{1.2}
$$

restando (1.2) − (1.1) se tiene que:

$$
3M = 4N\!\left(\frac{h}{2}\right) - N(h) + 4O\!\left(\left(\frac{h}{2}\right)^4\right) - O(h^4),
$$

lo cual implica que:

$$
M = \frac{4N\!\left(\frac{h}{2}\right) - N(h)}{3} + 4O(h^4).
$$

Si denotamos $N_1(h) = N(h)$ y $N_2(h) = M$, se obtiene que:

$$
N_2(h) = \frac{4N_1\!\left(\frac{h}{2}\right) - N_1(h)}{3} + 4O(h^4). \tag{1.3}
$$

A partir de la fórmula (1.3) se tiene que:

$$
\begin{array}{ccc}
N_1(h) &  & \\
N_1\!\left(\frac{h}{2}\right) & N_2(h) & \\
N_1\!\left(\frac{h}{4}\right) & N_2\!\left(\frac{h}{2}\right) & \\
N_1\!\left(\frac{h}{8}\right) & N_2\!\left(\frac{h}{4}\right) & \\
\vdots & \vdots & \\
\end{array}
$$

La tabla se puede ir ampliando si se tiene una fórmula del tipo:

$$
M = N(h) + K_1h^2 + K_2h^4 + O(h^6). \tag{1.4}
$$

Por ejemplo:

$$
f'(x_0) = \frac{1}{2h}\,[f(x_0 + h) - f(x_0 - h)] = N(h) - \frac{h^2}{6}f^{(3)}(x_0) - \frac{h^4}{120}f^{(5)}(x_0) + O(h^6),
$$

donde 
$$
K_1 = \frac{f^{(3)}(x_0)}{6}, 
\qquad 
K_2 = -\frac{f^{(5)}(x_0)}{120}.
$$

En (1.4) tome $h = \frac{h}{2}$, entonces:

$$
M = N\!\left(\frac{h}{2}\right) + K_1\!\left(\frac{h}{2}\right)^2 + K_2\!\left(\frac{h}{2}\right)^4 + O\!\left(\left(\frac{h}{2}\right)^6\right).
$$

multiplicando por $4^2$, se tiene que:

$$
4^2 M = 4^2 N\!\left(\frac{h}{2}\right) + 4K_1h^2 + K_2h^4 + O\!\left(\left(\frac{h}{2}\right)^6\right). \tag{1.5}
$$

Restando (1.5) − (1.4), se tiene que:

$$
(4^2 - 1)M = 4^2 N\!\left(\frac{h}{2}\right) - N(h) + 3K_1h^2 + O\!\left(\left(\frac{h}{2}\right)^6\right) + O(h^6),
$$

lo cual implica que:

$$
M \approx \frac{4^2 N\!\left(\frac{h}{2}\right) - N(h)}{15},
$$

si tomo $N = N_2$ y $N_3 = M$, entonces:

$$
N_3(h) \approx \frac{4^2 N_2\!\left(\frac{h}{2}\right) - N_2(h)}{15}.
$$

Esta fórmula genera la siguiente tabla:

$$
\begin{array}{cccc}
N_1(h) &  &  &  \\
N_1\!\left(\frac{h}{2}\right) & N_2(h) &  &  \\
N_1\!\left(\frac{h}{4}\right) & N_2\!\left(\frac{h}{2}\right) & N_3(h) &  \\
N_1\!\left(\frac{h}{8}\right) & N_2\!\left(\frac{h}{4}\right) & N_3\!\left(\frac{h}{2}\right) &  \\
\vdots & \vdots & \vdots &  \\
\end{array}
$$

En general se tiene que:

$$
M = N(h) + \sum_{j=1}^{m-1} K_j h^{2j} + O(h^{2m}),
$$

entonces los $N_j(h)$ se definen como:

$$
N_j(h) = \frac{4^{\,j-1} N_{j-1}\!\left(\dfrac{h}{2}\right) - N_{j-1}(h)}{4^{\,j-1} - 1},
$$

y se genera la tabla de extrapolación:

$$
\begin{array}{cccccc}
N_1(h) &  &  &  &  &  \\
N_1\!\left(\dfrac{h}{2}\right) & N_2(h) &  &  &  &  \\
N_1\!\left(\dfrac{h}{4}\right) & N_2\!\left(\dfrac{h}{2}\right) & N_3(h) &  &  &  \\
N_1\!\left(\dfrac{h}{8}\right) & N_2\!\left(\dfrac{h}{4}\right) & N_3\!\left(\dfrac{h}{2}\right) & N_4(h) &  &  \\
N_1\!\left(\dfrac{h}{16}\right) & N_2\!\left(\dfrac{h}{8}\right) & N_3\!\left(\dfrac{h}{4}\right) & N_4\!\left(\dfrac{h}{2}\right) & N_5(h) &  \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
$$

::: {.callout-note title="Algoritmo 3: Simpson Compuesto para Integrales Dobles Tipo I"}

**Objetivo:**  
Calcular la integral doble de tipo I mediante la regla de Simpson compuesta:

$$
I = \int_a^b \int_{\varphi_1(x)}^{\varphi_2(x)} f(x,y)\,dy\,dx.
$$

**Entrada:**  $n, m, a, b, \varphi_1, \varphi_2, f(x,y)$

**Salida:**  
$XI$, valor aproximado de la integral.

  **Paso 1.** Tomar  $h = \frac{b - a}{2n}.$

  **Paso 2.** Inicializar:  
    $XI0 = 0$ (inicial + final)  
    $XI1 = 0$ (impares)  
    $XI2 = 0$ (pares)

  **Paso 3.** Para $i = 1$ hasta $2n$, seguir los pasos 4–10:

    **Paso 4.** Calcular  $x = a + ih.$

    **Paso 5.** Calcular  $k = \frac{\varphi_2(x) - \varphi_1(x)}{2m}.$

    **Paso 6.** Inicializar:  
      $YI0 = f(x, \varphi_1(x)) + f(x, \varphi_2(x))$  
      $YI1 = 0$ (impares)  
      $YI2 = 0$ (pares)

    **Paso 7.** Para $j = 1, 2, \ldots, 2m - 1$, seguir pasos 8–9:

      **Paso 8.** Calcular  $y = \varphi_1(x) + jk.$

      **Paso 9.** Si $j$ es par:  
      $$YI2 = YI2 + f(x, y),$$  
      si no:  
      $$YI1 = YI1 + f(x, y).$$

    **Paso 10.** Calcular  $YI = \frac{k}{3}\,[\,YI0 + 2\,YI2 + 4\,YI1\,].$

    **Paso 11.** Si $i = 0$ o $i = 2n$, tomar  
    $$XI0 = XI0 + YI.$$

    **Paso 12.** Si $i$ es par:  
    $$XI2 = XI2 + YI,$$  
    si no:  
    $$XI1 = XI1 + YI.$$

  **Paso 13.** Calcular el valor final de la integral:  $XI = \frac{h}{3}\,[\,XI0 + 2\,XI2 + 4\,XI1\,].$

  **Paso 14.** Salida $(XI)$  

  **Parar.**

:::

::: {.callout-note title="Algoritmo 4: Métodos de Extrapolación"}

**Entrada:**  
La fórmula $N(h)$, el paso $h$, y $n$ (tamaño de la tabla).

**Salida:**  
La tabla de extrapolación $Q$.

  **Paso 1.** Para $i = 1$ hasta $n$:
$$
Q_{i1} \leftarrow N\!\left(\frac{h}{2^{i-1}}\right).
$$

  **Paso 2.** Para $i = 2$ hasta $n$:

    **Paso 3.** Para $j = 2$ hasta $i$:
$$
Q_{ij} \leftarrow 
\frac{4^{\,j-1}\, Q_{i,\,j-1} - Q_{i-1,\,j-1}}{4^{\,j-1} - 1}.
$$

**Paso 4.** Salida: $Q_{nn}$, aproximación de $M$.

**Parar.**

:::

**Integración de Romberg**

# Métodos numéricos para problemas de valor inicial

## Ecuaciones diferenciales

### El Teorema de Picard–Lindelöf

::: {.callout-note title="Definición: Solución de una ecuación diferencial de primer orden"}

Sean $G \subset \mathbb{R}^2$ y $f : G \to \mathbb{R}$ una función continuamente diferenciable.  
Una función $U : [a,b] \to \mathbb{R}$ se llama **solución** de una ecuación diferencial de primer orden:

$$
U' = f(x, U),
$$

si:

- $(x, U(x)) \in G$, y  
- $U'(x) = f(x, U(x)) \ \forall x \in [a,b]$.

:::

::: {.callout-note title="Definición: Problema de valor inicial"}

El **problema de valor inicial** para la ecuación diferencial ordinaria

$$
U' = f(x, U),
$$

consiste en encontrar una solución $U$ continuamente diferenciable que satisfaga la condición inicial:

$$
U(x_0) = U_0.
$$

:::

::: {.callout-note title="Teorema: Picard–Lindelöf"}

Sea $G \subset \mathbb{R}^{n+1}$ y sea $f : G \to \mathbb{R}$ una función continuamente diferenciable que satisface la **condición de Lipschitz**:

$$
\| f(x,U) - f(x,V) \| \le L \| U - V \| ,
$$

para todo $(x,U), (x,V) \in G$ y para alguna constante $L > 0$.  
Entonces, para cada par de valores iniciales $(x_0, U_0) \in G$, existe un intervalo $[x_0 - a, x_0 + a]$ con $a > 0$ tal que el problema de valor inicial dado por (1.2), (1.3) tiene solución única.

\begin{equation*} 
\left\{
\begin{aligned}
    U' &= f(x, U) \\
    U(x_0) &= U_0
\end{aligned}
\right.
\end{equation*}

tiene solución única en este intervalo.

::: {.callout-caution collapse="true" title="Prueba"}

Se transforma el problema

$$
\begin{cases}
U' = f(x,U) \\
U(x_0) = U_0
\end{cases}
$$

en un problema equivalente conocido como la **ecuación integral de Volterra**:

$$
U(x) = U_0 + \int_{x_0}^{x} f(\xi, U(\xi)) \, d\xi.
$$

Luego se escoge $D$ abierto tal que (como $G$ es abierto) $(x_0, U_0) \in G$ y $\overline{D} \subset G$.

Se denota por $M$ una cota de la función continua $f$ en $\overline{D}$, es decir, para  
$f : \overline{D} \to \mathbb{R}^n$, se cumple:

$$
\| f(x, U) \| \le M, \quad (x, U) \in \overline{D}.
$$

Como $D$ es abierto, se puede escoger $a > 0$ tal que el rectángulo:

$$
B := \{ (x, U) \in \mathbb{R}^{n+1} : |x - x_0| \le a, \ \| U - U_0 \| \le Ma \}
$$

esté contenido en $D$.

La prueba consiste en demostrar que el operador:

$$
(AU)(x) := U_0 + \int_{x_0}^{x} f(\xi, U(\xi))\, d\xi, \quad |x - x_0| \le a,
$$

definido en:

$$
U := \{ x \in C[x_0 - a, x_0 + a] : \| U - U_0 \| \le Ma \},
$$

cumple las hipótesis del **teorema del punto fijo de Banach**.  
Si $C[x_0 - a, x_0 + a]$ denota el espacio de las funciones continuas  
$U : [x_0 - a, x_0 + a] \to \mathbb{R}^{n+1}$,

entonces el punto fijo único de $A$ es la solución de la ecuación integral (1.6),  
y por tanto, la solución del problema de valor inicial (1.5).

$\blacksquare$

::: 

:::

::: {.callout-note title="Corolario"}

Tomando las mismas hipótesis del teorema anterior, la sucesión $U_n$ definida por:

$$
\begin{cases}
U_0(x) := U_0, \\
U_{n+1} := U_0 + \displaystyle \int_{x_0}^{x} f(\xi, U_n(\xi))\, d\xi
\end{cases}
$$

con $|x - x_0| < a$, converge uniformemente cuando $n \to \infty$ en $[x_0 - a, x_0 + a]$  
a la solución única $U$ del problema de valor inicial.  

Además, se tiene la siguiente cota para el error a posteriori:

$$
\| U - U_{n+1} \|_\infty \le \frac{La}{1 - La} \, \| U_n - U_{n-1} \|_\infty,
\qquad n = 1, 2, 3, \ldots
$$



::: {.callout-caution collapse="true" title="Prueba"}

Es evidente que la ecuación anterior define la **solución de aproximaciones sucesivas** del teorema previo.  
Además, en la prueba del teorema anterior se probó que:

$$
\| AU - AV \|_\infty \le La \, \| U - V \|_\infty,
$$

es decir, la constante de Lipschitz del operador $A$ es $La$, por lo que $A$ es una contracción.  
Usando el error a posteriori del **Teorema del Punto Fijo de Banach**, se obtiene la desigualdad deseada.

$\blacksquare$
:::

:::

::: {.callout-tip collapse="true" title="Ejemplo: Aproximaciones sucesivas para un PVI"}

Considere el problema de valor inicial
$$
\begin{cases}
U' = x^2 + U^2,\\[2pt]
U(0)=0 .
\end{cases}
$$

- En $G = ]-0.5,0.5[\times]-0.5,0.5[$ tenemos que $f(x,U):=x^2+U^2$, y es claro que $|f(x,U)|\le 0.5$ en $G$.
- Para cualquier $a<0.5$ y $M=0.5$, el rectángulo $B$ del teorema de Picard satisface $B\subset G$.
- Además, $|f(x,U)-f(x,V)|=|U^2-V^2|=|(U-V)(U+V)|\le |U-V|$; luego $f$ es Lipschitz con $L=1$, y por tanto $La<0.5$.
- La sucesión de aproximaciones sucesivas converge a la solución única del problema:
  $$
  \begin{cases}
  U_0(x)=0,\\[2pt]
  U_{n+1}(x)=\displaystyle\int_0^x\!\big[\xi^2+U_n^2(\xi)\big]\,d\xi .
  \end{cases}
  $$

**Iteraciones y cotas de error a posteriori**

1) Primera iteración:
$$
U_1(x)=\int_0^x \xi^2\,d\xi=\frac{x^3}{3}.
$$
Error a posteriori:
$$
\|U-U_1\|_\infty \le \frac{La}{1-La}\,\|U_1-U_0\|_\infty
< 1\cdot \frac{1}{24}=\frac{1}{24}\approx 4.1667\times 10^{-2}.
$$

2) Segunda iteración:
$$
U_2(x)=\int_0^x \Big(\xi^2+U_1^2(\xi)\Big)\,d\xi
      =\int_0^x\!\Big(\xi^2+\frac{\xi^6}{9}\Big)d\xi
      =\frac{x^3}{3}+\frac{x^7}{63}.
$$
Error a posteriori:
$$
\|U-U_2\|_\infty \le \|U_2-U_1\|_\infty
=\frac{1}{63\cdot 2^{7}}\approx 1.2401\times 10^{-4}.
$$

3) Tercera iteración:
$$
\begin{aligned}
U_3(x)
&=\int_0^x \Big(\xi^2+U_2^2(\xi)\Big)\,d\xi
 =\int_0^x \left[\xi^2+\left(\frac{\xi^3}{3}+\frac{\xi^7}{63}\right)^2\right] d\xi \\[4pt]
&=\int_0^x\!\left(\xi^2+\frac{\xi^6}{9}+\frac{2\xi^{10}}{189}+\frac{\xi^{14}}{3969}\right)d\xi
 =\frac{x^3}{3}+\frac{x^7}{63}+\frac{2x^{11}}{2079}+\frac{x^{15}}{59535}.
\end{aligned}
$$
Error a posteriori:
$$
\|U-U_3\|_\infty \le \|U_3-U_2\|_\infty
=\frac{1}{59535\cdot 2^{15}}\approx 4.7024\times 10^{-7}.
$$

:::

### Métodos numéricos

#### El método de Euler

::: {.callout-note title="Definición: Problema bien planteado (PVI)"}

Se dice que el problema de valor inicial:

$$
\begin{cases}
\dfrac{dy}{dt} = f(t,y),\\[3pt]
y(a) = \alpha,
\end{cases}
\qquad a \le t \le b,
$$

es **bien planteado** si:

1. Existe una solución única $y(t)$.

2. Existen $k, \varepsilon \in \mathbb{R}^+$ tales que existe una solución única al **problema perturbado**:

$$
\begin{cases}
\dfrac{dZ}{dt} = f(t,Z) + \delta(t),\\[3pt]
Z(a) = \alpha + \varepsilon_0,
\end{cases}
$$

donde $|Z(t) - y(t)| < k \varepsilon \quad \forall\, t \in [a,b]$, siempre que $|\varepsilon_0| < \varepsilon$ y $|\delta(t)| < \varepsilon$.

- La idea es construir una partición (red) y hallar una aproximación de la función $y$ en esos puntos.

- Suponga que $y(t)$ es la solución del problema bien planteado

$$
\begin{cases}
\dfrac{dy}{dt} = f(t, y), \\[3pt]
y(a) = \alpha,
\end{cases}
\qquad a \le t \le b,
$$

y que $y \in C^2[a,b]$.

- Defínase una red $\{t_0, t_1, \ldots, t_N\}$ en $[a,b]$ mediante:

$$
\begin{cases}
t_i = a + ih, \\[3pt]
h = \dfrac{b - a}{N},
\end{cases}
\qquad i = 0,1,2,\ldots,N.
$$

- Aplicando el teorema de Taylor a $y(t)$ en $x_0 = t_i$ y evaluando en $t_{i+1}$:

$$
y(t_{i+1}) = y(t_i) + (t_{i+1}-t_i)y'(t_i) + \dfrac{(t_{i+1}-t_i)^2}{2!}y''(\xi_i),
$$

con $\xi_i \in ]t_i,t_{i+1}[$ y $(t_{i+1}-t_i) = h$.

- Por tanto:

$$
y(t_{i+1}) = y(t_i) + h y'(t_i) + \dfrac{h^2}{2} y''(\xi_i).
$$

- Como $y$ es solución del PVI:

$$
\begin{cases}
\dfrac{dy}{dt} = f(t, y), \\[3pt]
y(a) = \alpha,
\end{cases}
$$

entonces:

$$
y(t_{i+1}) = y(t_i) + h f(t_i, y(t_i)) + \dfrac{h^2}{2} y''(\xi_i).
$$

- Si se define $w_i = y(t_i)$, el **Método de Euler** consiste en calcular:

$$
\begin{cases}
w_0 = \alpha, \\[3pt]
w_{i+1} = w_i + h f(t_i, w_i),
\end{cases}
\qquad i = 0,1,\ldots,N-1.
$$

:::


#### Ecuación de diferencias

::: {.callout-note title="Algoritmo 1: Método de Euler"}

**Entrada:** $a, b, N, \alpha, f(t,w)$  

**Salida:** Los pares $(t_i, w_i)$ que aproximan $y(t_i) = w_i$.

   **Paso 1.** Tomar:
$$
h = \frac{b - a}{N}, \qquad t = a, \qquad w = \alpha.
$$
Salida $(t, w)$.

  **Paso 2.** Para $i = 1$ hasta $N$, seguir los pasos 3–5:

    **Paso 3.** $w = w + h f(t, w).$  
    **Paso 4.** $t = t + h.$  
    **Paso 5.** Salida $(t, w).$

  **Paso 6.** Parar.

:::

::: {.callout-tip collapse="true" title="Ejemplo: Aplicación del método de Euler"}

Resolver:
$$
\begin{cases}
\dfrac{dy}{dt} = \dfrac{2}{t}y + t^2 e^t, \\[3pt]
y(1) = 0.
\end{cases}
$$

**Datos:**

- $t \in [1,2]$
- $N = 10 \Rightarrow h = 0.1$
- $\alpha = 0$
- $f(t,w) = \dfrac{2}{t}w + t^2 e^t$

**Solución exacta:**
$$
y(t) = t^2 (e^t - e).
$$

:::

::: {.callout-note title="Teorema: Cota del error en el Método de Euler"}

Sea $y(t)$ la solución del problema bien planteado:

$$
\begin{cases}
y'(t) = f(t,y), & t \in [a,b], \\[3pt]
y(a) = \alpha,
\end{cases}
$$

y sean $w_1, w_2, \ldots, w_N$ las aproximaciones generadas por el **Método de Euler**.  
Si además:

- $f$ satisface la **condición de Lipschitz** con constante $L$ en
$$
D = \{(t,y)/ a \le t \le y, -\infty < y < +\infty\},
$$

- y existe una constante $M$ tal que:
$$
|y''(t)| \le M \qquad \forall t \in [a,b],
$$

entonces se cumple que:

$$
|y(t_i) - w_i| \le \dfrac{hM}{2L}\,\big(e^{L(t_i - a)} - 1\big),
\qquad i = 0,1,2,\ldots,N.
$$

:::

::: {.callout-note title="Lema"}

Para todo $x \ge -1$ y para cualquier número positivo $m$ se tiene:
$$
0 \le (1+x)^m \le e^{mx}.
$$



::: {.callout-caution collapse="true" title="Prueba"}

Aplicando el teorema de Taylor a $f(x)=e^x$ en $x_0=0$ con $n=1$, existe $\xi$ entre $x$ y $0$ tal que
$$
e^x = 1 + x + \frac{1}{2}x^2 e^{\xi}.
$$
Por lo tanto
$$
0 \le 1+x \le 1 + x + \frac{1}{2}x^2 e^{\xi} = e^x.
$$
Como $x \ge -1$ implica $1+x \ge 0$, elevando a la potencia $m>0$ se obtiene
$$
0 \le (1+x)^m \le (e^x)^m = e^{mx}.
$$

$\blacksquare$

:::

:::

::: {.callout-note title="Lema"}

Sean $s$ y $t$ números reales positivos y sea la sucesión $(a_i)_{i=0}^k$ tal que  
$a_0 \ge -\dfrac{t}{s}$ y además cumple:
$$
a_{i+1} \le (1+s)a_i + t, \quad \text{para } i = 0, 1, 2, \ldots, k.
$$
Entonces:
$$
a_{i+1} \le e^{(i+1)s}\left(\frac{t}{s} + a_0\right) - \frac{t}{s}.
$$

::: {.callout-caution collapse="true" title="Prueba"}

De la desigualdad anterior se tiene:
$$
a_{i+1} \le (1+s)a_i + t.
$$
Aplicando recursivamente:
$$
\begin{aligned}
a_{i+1} &\le (1+s)[(1+s)a_{i-1} + t] + t \\
&\le (1+s)^2 a_{i-1} + [(1+s) + 1]t, \\
&\vdots \\
&\le (1+s)^{i+1}a_0 + [1 + (1+s) + (1+s)^2 + \cdots + (1+s)^i]t.
\end{aligned}
$$

Pero:
$$
1 + (1+s) + (1+s)^2 + \cdots + (1+s)^i
= \sum_{j=0}^{i} (1+s)^j
= \frac{(1+s)^{i+1} - 1}{(1+s) - 1}
= \frac{1}{s}\big[(1+s)^{i+1} - 1\big].
$$

Sustituyendo, obtenemos:
$$
\begin{aligned}
a_{i+1} &\le (1+s)^{i+1}a_0 + \frac{t}{s}\big[(1+s)^{i+1} - 1\big] \\
&= (1+s)^{i+1}\left(a_0 + \frac{t}{s}\right) - \frac{t}{s}.
\end{aligned}
$$

Finalmente, aplicando el **Lema 1**, se cumple:
$$
a_{i+1} \le e^{(i+1)s}\left(a_0 + \frac{t}{s}\right) - \frac{t}{s}.
$$

$\blacksquare$

:::

:::

::: {.callout-caution collapse="true" title="Prueba del Teorema del Error en el Método de Euler"}

- Para $i = 0$ es evidente, pues $y(t_0) = w_0 = \alpha$.

- Para $i = 1, 2, \ldots, N-1$, por el **Teorema de Taylor**, se tiene:

$$
y(t_{i+1}) = y(t_i) + h f(t_i, y(t_i)) + \frac{h^2}{2}y''(\xi_i),
$$

mientras que, por definición del **método de Euler**:

$$
w_{i+1} = w_i + h f(t_i, w_i).
$$

Sea $y_i = y(t_i)$, entonces:

$$
|y_{i+1} - w_{i+1}| \le |y_i - w_i| + h\,|f(t_i, y_i) - f(t_i, w_i)| + \frac{h^2}{2}|y''(\xi_i)|.
$$

Como $f$ es **Lipschitz** en la segunda variable y $|y''(t)| \le M$, se cumple:

$$
|y_{i+1} - w_{i+1}| \le |y_i - w_i|(1 + hL) + \frac{h^2 M}{2}.
$$

Aplicando el **Lema 2** con $a_i = |y_i - w_i|$, $s = hL$ y $t = \dfrac{h^2 M}{2}$, se deduce que:

$$
|y_{i+1} - w_{i+1}| \le e^{(i+1)hL}\left(|y_0 - w_0| + \frac{hM}{2L}\right) - \frac{hM}{2L}.
$$

Dado que $|y_0 - w_0| = 0$ y $t_i = a + i h$, se tiene:

$$
(1 + i)h = t_{i+1} - t_0 = t_{i+1} - a.
$$

Por lo tanto:

$$
|y_{i+1} - w_{i+1}| \le \frac{hM}{2L}\left(e^{(t_{i+1}-a)L} - 1\right),
\qquad \text{para } i = 1, 2, \ldots, N-1.
$$

$\blacksquare$

:::

::: {.callout-tip collapse="true" title="Ejemplo: Cotas del error para el Método de Euler"}

Encuentre las cotas del error para el PVI
$$
\begin{cases}
\dfrac{dy}{dt}=\dfrac{2}{t}y+t^2e^t,\\[3pt]
y(1)=0,
\end{cases}
\qquad t\in[1,2].
$$

**Solución**

- $\displaystyle \left|\frac{\partial f}{\partial y}\right|=\left|\frac{2}{t}\right|\le 2$ para $t\in[1,2]\;\Rightarrow$ tome $L=2$.

- Como $y(t)=t^2(e^t-e)$, entonces
  $$
  y''(t)=e^t(2+4t+t^2)-2e.
  $$

- $y''(t)$ es creciente en $[1,2]$, por lo que
  $$
  y''(t)\le e^2(2+4\cdot 2+2^2)-2e\le 100.
  $$
  Tome $M=100$.

- Por el teorema de cota del error en Euler:
  $$
  |y(t_i)-w_i|\le \frac{hM}{2L}\big(e^{L(t_i-a)}-1\big).
  $$
  Con $h=0.1$, $L=2$, $a=1$, $M=100$:
  $$
  |y_i-w_i|\le \frac{0.1\cdot100}{2\cdot2}\big(e^{2(t_i-1)}-1\big)
  =\frac{5}{2}\big(e^{2t_i-2}-1\big).
  $$

$\blacksquare$

:::

#### Método de Euler Implícito

En la ecuación integral de Volterra:

$$
U(x_1) \;=\; U_0 \;+\; \int_{x_0}^{x_1} f(\xi, U(\xi))\, d\xi,
$$

se puede utilizar la **Regla del Trapecio** para aproximar la integral, de modo que:

$$
\int_{x_0}^{x_1} f(\xi, U(\xi))\, d\xi \;\approx\; \frac{h}{2}\,\big[\,f(x_0, U(x_0)) \;+\; f(x_1, U(x_1))\,\big].
$$

Así:

$$
U_1 \;=\; U_0 \;+\; \frac{h}{2}\,\big[\,f(x_0, U_0) \;+\; f(x_1, U_1)\,\big],
$$

aplicando esto repetidamente se puede dar la siguiente definición:

::: {.callout-note title="Definición: Método de Euler Implícito"}

El método de **Euler implícito** para la solución del problema de valor inicial (1.5) construye una aproximación $U_j$ para $U(x_j)$ en la malla de puntos equidistantes $x_j := x_0 + jh$, $j = 0, 1, 2, \ldots$, con paso $h$, usando la ecuación:

$$
U_{j+1} := U_j + \frac{h}{2}\,[\,f(x_j, U_j) + f(x_{j+1}, U_{j+1})\,], \quad j = 0, 1, 2, \ldots
$$

:::

::: {.callout-note title="Definición: Método Predictor–Corrector de Euler (Método de Heun o Euler Mejorado)"}

El método **Predictor–Corrector de Euler** construye las aproximaciones $U_j$ para $U(x_j)$ en la malla $x_j := x_0 + jh$, $j = 0, 1, 2, \ldots$, usando la ecuación:

$$
U_{j+1} := U_j + \frac{h}{2}\,[\,f(x_j, U_j) + f(x_{j+1}, U_j + h f(x_j, U_j))\,], \quad j = 0, 1, 2, \ldots
$$

:::

::: {.callout-important collapse="true" title="Observación"}

Se puede usar el método de Euler para calcular (predecir) $U_{j+1}$ y luego aplicar Euler implícito para recalcularlo, obteniendo un valor corregido de $U_{j+1}$.

:::

::: {.callout-note title="Algoritmo: Método de Euler Predictor–Corrector"}

**Entrada:** $a, b, N, \alpha, f(t, w)$  
**Salida:** Los pares $(t_i, w_i)$ que aproximan $y(t_i) = w_i$, para $i = 0, 1, 2, \ldots, N$.

  **Paso 1.** Tomar:
$$
h = \frac{b - a}{N}, \quad t_0 = a, \quad w_0 = \alpha.
$$  
      Salida $(t_0, w_0)$.

  **Paso 2.** Para $i = 1$ hasta $N$, seguir pasos 3–6:

    **Paso 3.** $w_i = w_{i-1} + h f(t_{i-1}, w_{i-1})$  (Predictor).  
    **Paso 4.** $t_i = t_{i-1} + h$.  
    **Paso 5.** $w_i = w_{i-1} + \dfrac{h}{2}\,[\,f(t_{i-1}, w_{i-1}) + f(t_i, w_i)\,]$  (Corrector).  
    **Paso 6.** Salida $(t_i, w_i)$.

  **Paso 7.** Parar.

:::

#### Métodos de un Paso

::: {.callout-note title="Definición: Método de un paso"}

Un **método de un paso** para resolver el problema de valor inicial

$$
u' = f(x, u), \quad u(x_0) = u_0,
$$

construye aproximaciones $u_j$ para $u(x_j)$ en una malla de puntos equidistantes:

$$
x_j := x_0 + jh, \quad j = 1, 2, 3, \ldots,
$$

mediante una relación del tipo:

$$
u_{j+1} := u_j + h\,\varphi(x_j, u_j; h), \quad j = 0, 1, 2, \ldots,
$$

en la cual la función $\varphi : G \times ]0, +\infty[ \to \mathbb{R}$ está definida en términos de  
$f : G \to \mathbb{R}$ (la parte derecha de la ecuación diferencial).

:::

::: {.callout-tip collapse="true" title="Ejemplo"}

- En el **método de Euler**, se tiene $\varphi(x, u; h) = f(x, u)$.

- En el **método de Euler Mejorado**, se define:

$$
\varphi(x, u; h) = \frac{1}{2}\,[\,f(x, u) + f(x + h, u + h f(x, u))\,].
$$

:::

::: {.callout-important collapse="true" title="Observación"}

Como $\varphi$ describe la ecuación diferencial $u' = f(x, u)$, es razonable pensar que:

$$
\frac{1}{h}\,[\,u(x + h) - u(x)\,] - \varphi(x, u; h) \to 0
\quad \text{cuando } h \to 0.
$$

:::

::: {.callout-note title="Definición: Error de discretización local"}

Para cada $(x, u) \in G$, se denota por $\eta' = f(\xi, \eta)$, donde $\eta(x) = u$ son condiciones iniciales $(x, u)$.  
Entonces:

$$
\Delta(x, u; h) := \frac{1}{h}\,[\,\eta(x + h) - \eta(x)\,] - \varphi(x, u; h),
$$

y se llama **Error de discretización local**.

:::

::: {.callout-note title="Definición: Método consistente"}

Un método de un paso se dice **consistente** con el problema de valor inicial si:

$$
\lim_{h \to 0} \Delta(x, u; h) = 0,
$$

uniformemente con $(x, u) \in G$.

:::

::: {.callout-note title="Definición: Orden de consistencia"}

Se dice que el método es **consistente de orden $p$** si:

$$
|\Delta(x, u; h)| \leq k h^p,
$$

para todo $(x, u) \in G$, para todo $h > 0$ y para alguna constante $k$.

:::

::: {.callout-important collapse="true" title="Observación"}

En adelante se asume que $f$ y sus derivadas son uniformemente continuas y acotadas en $G$,  
pues de no serlo, entonces se reduce el dominio $G$.

:::

::: {.callout-note title="Teorema: Criterio de consistencia"}

Un método de un paso es **consistente si y solo si**:

$$
\lim_{h \to 0} \varphi(x, u; h) = f(x, u),
$$

uniformemente en $(x, u) \in G$.

::: {.callout-caution collapse="true" title="Prueba"}

Como se asume que $f$ es acotado, se tiene que:

$$
\eta(x + t) - \eta(x)
= \int_0^t \eta'(x + s)\, ds
= \int_0^t f(x + s, \eta(x + s))\, ds
\;\;\xrightarrow[t \to 0]{}\;\; 0,
$$

uniformemente $\forall (x, u) \in G$ (ya que $\eta' = f(\xi, \eta)$).  
Por lo tanto, como se asume que $f$ es uniformemente continua, se tiene que:

$$
\frac{1}{h}\int_0^h [\,\eta'(x + t) - \eta'(x)\,]\, dt
\le \max_{0 \le t \le h} |\,\eta'(x + t) - \eta'(x)\,|
= \max_{0 \le t \le h} |\,f(x + t, \eta(x + t)) - f(x, \eta(x))\,|
\;\;\xrightarrow[h \to 0]{}\;\; 0,
$$

uniformemente $\forall (x, u) \in G$.  
De aquí se tiene que:

$$
\begin{aligned}
\Delta(x, u; h) + \varphi(x, u; h) - f(x, u)
&= \frac{1}{h}\,[\,\eta(x + h) - \eta(x)\,] - \varphi(x, u; h) + \varphi(x, u; h) - f(x, u) \\
&= \frac{1}{h}\,[\,\eta(x + h) - \eta(x)\,] - f(x, u) \\
&= \frac{1}{h}\int_0^h [\,\eta'(x + t) - \eta'(x)\,]\, dt \;\;\xrightarrow[h \to 0]{}\;\; 0,
\end{aligned}
$$

uniformemente $\forall (x, u) \in G$ cuando $h \to 0$.  
Entonces:

$$
\lim_{h \to 0} \Delta(x, u; h) = 0
\;\;\Longleftrightarrow\;\;
\varphi(x, u; h) \xrightarrow[h \to 0]{} f(x, u).
$$

$\blacksquare$

:::

:::

::: {.callout-note title="Teorema: Consistencia del método de Euler y orden 1"}

El método de **Euler** es consistente. Además, si $f$ es continuamente diferenciable en $G$, entonces el método de Euler tiene **orden de consistencia uno**.

::: {.callout-caution collapse="true" title="Prueba"}

- **Consistencia.**  
Se debe probar que $\displaystyle \lim_{h\to 0}\varphi(x,u;h)=f(x,u)$.  
Esto es inmediato por la definición del método de Euler, pues $\varphi(x,u;h)=f(x,u)$.

- **Orden uno.**  
Se debe mostrar que $\big|\Delta(x,u_j;h)\big|\le kh$.  
Si $f$ es continuamente diferenciable, de la ecuación diferencial $\eta' = f(\xi,\eta)$ se deduce que $\eta$ es dos veces continuamente diferenciable y que
$$
\eta''=f_x(\xi,\eta)+f_u(\xi,\eta)\,f(\xi,\eta),
$$
ya que $u' = f(\xi,\eta)$. Por la fórmula de Taylor:
$$
\begin{aligned}
\big|\Delta(x,u_j;h)\big|
&=\left|\frac{1}{h}\,[\eta(x+h)-\eta(x)]-\eta'(x)\right|
\qquad\text{[pues } \varphi(x,u_j;h)=f(x,u)=\eta'(x)\text{]} \\
&=\frac{h}{2}\,|\eta''(x+\theta h)| \le kh,
\end{aligned}
$$
para algún $0<\theta<1$ y alguna cota $k$ de la función $2(f_x+f_u f)$.  
$\blacksquare$

:::

:::

::: {.callout-note title="Teorema: Consistencia del método de Euler Mejorado y orden 2"}

El método de **Euler Mejorado** es consistente. Además, si $f$ es dos veces continuamente diferenciable en $G$, entonces este método tiene **orden de consistencia igual a 2**.

::: {.callout-caution collapse="true" title="Prueba (Euler Mejorado: consistencia y orden 2)"}

- **Consistencia.**  
Para el método de Euler Mejorado
$$
\varphi(x,u;h)=\frac{1}{2}\,[\,f(x,u)+f(x+h,u+h f(x,u))\,]\xrightarrow[h\to 0]{} f(x,u).
$$

- **Orden de convergencia.**  
Si $f$ es dos veces continuamente diferenciable, entonces la solución $\eta$ del PVI es **tres veces** continuamente diferenciable y, como
$$
\eta''=f_x(\xi,\eta)+f_u(\xi,\eta)\,f(\xi,\eta),
$$
se tiene (por regla de la cadena) que
$$
\begin{aligned}
\eta''' \;&=\; f_{xx}(\xi,\eta) \;+\; f_{xu}(\xi,\eta)\,\eta' \;+\;[\,f_{ux}(\xi,\eta)+f_{uu}(\xi,\eta)\,\eta'\,]\,f(\xi,\eta) \\[2pt]
&\qquad {} + f_u(\xi,\eta)\big[f_x(\xi,\eta)+f_u(\xi,\eta)\eta'\big] \\[2pt]
&=\; f_{xx}(\xi,\eta)+f_{xu}(\xi,\eta)f(\xi,\eta)+f_{ux}(\xi,\eta)f(\xi,\eta)+f_{uu}(\xi,\eta)f(\xi,\eta)^2 \\[2pt]
&\qquad {}+f_u(\xi,\eta)f_x(\xi,\eta)+f_u(\xi,\eta)^2 f(\xi,\eta).
\end{aligned}
$$

Aplicando Taylor (en $x$) se obtiene
$$
\left|\,\eta(x+h)-\eta(x)-h\eta'(x)-\frac{h^2}{2}\eta''(x)\,\right|
= \frac{h^3}{6}\,|\eta'''(x+\theta h)| \;\le\; k_1 h^3, \qquad (1.11)
$$
para algún $0<\theta<1$ y $k_1$ una cota de $6(f_{xx}+2f_{xu}+f_{uu}f^2+f_u f_x+f_u^2 f)$.

Asimismo, usando la fórmula de Taylor **para dos variables**,
$$
\big|\,f(x+h,u+k)-f(x,u)-h f_x(x,u)-k f_u(x,u)\,\big|
\;\le\; \frac{1}{2}\,k_2\,(|h|+|k|)^2,
$$
donde $k_2$ es una constante apropiada que acota $f_{xx}$, $f_{xu}$ y $f_{uu}$ (completar detalles).


Tomando $k = h f(x,u)$ y usando que
$$
\eta'' = f_x(\xi,\eta) + f_u(\xi,\eta) f(\xi,\eta),
$$
se tiene que
$$
\big|\,f(x+h, u + h f(x,u)) - f(x,u) - h \eta''(x)\,\big|
\le \frac{1}{2} k_2 (1+k_0)^2 h^2,
$$
donde $k_0$ es una cota de $f$.  
Por tanto:
$$
\frac{1}{2}\,\big|\,f(x,u) + f(x+h, u + h f(x,u)) - 2 f(x,u) - h \eta''(x)\,\big|
\le \frac{1}{4}\,k_2 (1+k_0)^2 h^2,
$$
y se deduce que:
$$
\left|\frac{1}{2}\,[\,f(x,u) + f(x+h, u + h f(x,u))\,] - f(x,u) - \frac{h}{2}\,\eta''(x)\right|
\le \frac{1}{4}\,k_2 (1+k_0)^2 h^2.
$$

Como
$$
\varphi(x,u_j;h)=\frac{1}{2}\,[\,f(x,u)+f(x+h,u+h f(x,u))\,],
$$
entonces:
$$
\left|\,\varphi(x,u_j;h) - f(x,u) - \frac{h}{2}\eta''(x)\,\right|
\le \frac{1}{4}\,k_2 (1+k_0)^2 h^2. \tag{1.12}
$$

Por otro lado, de (1.11) se tiene que:
$$
\left|\frac{1}{h}\,[\,\eta(x+h)-\eta(x)\,] - \eta'(x) - \frac{h}{2}\eta''(x)\right|
\le k_1 h^2. \tag{1.13}
$$

Combinando (1.12) y (1.13), y usando que $\eta'(x)=f(x,u)$, aplicando la desigualdad triangular se obtiene:
$$
\left|\frac{1}{h}\,[\,\eta(x+h)-\eta(x)\,] - \varphi(x,u;h)\right|
\le \left(\frac{1}{4}k_2(1+k_0)^2 + k_1\right)h^2.
$$

Por lo tanto:
$$
\boxed{\Delta(x,u;h) \le k h^2, \quad\text{con} \quad k = \frac{1}{4}k_2(1+k_0)^2 + k_1.}
$$

$\blacksquare$

:::

:::

::: {.callout-note title="Definición: Convergencia"}

Sea $[a,b]$ un intervalo y una malla de puntos equidistantes
$$
x_j := x_0 + jh, \quad j = 0,1,2,\ldots,n,
$$
con $x_0 = a$ y $x_n = b$.

Sea $u_j$ la **solución aproximada** de $u(x_j)$ para el problema de valor inicial
$$
\begin{cases}
u' = f(x,u), \\
u(x_0) = u_0,
\end{cases}
$$
obtenida mediante un método de un paso, entonces se definen:

1. **Error Global:**  
   $$
   e_j = e_j(h) := u_j - u(x_j), \quad j = 1,2,\ldots,n.
   $$

2. **Error Global Máximo:**  
   $$
   E = E(h) := \max_{j=1,2,\ldots,n} |e_j(h)|.
   $$

3. **Convergencia:**  
   Un método de un paso se dice **convergente** si
   $$
   \lim_{h\to 0} E(h) = 0.
   $$

4. **Orden de convergencia:**  
   Un método de un paso tiene **orden de convergencia $p$** si
   $$
   E(h) \le H h^p
   $$
   para todo $h > 0$ y alguna constante $H > 0$.

:::

::: {.callout-note title="Lema"}

Sea $(\xi_j)$ una sucesión en $\mathbb{R}$ con la propiedad

$$
|\xi_{j+1}| \le (1+A)\,|\xi_j| + B, \qquad j=0,1,2,\ldots,
$$

para algunas constantes $A>0$ y $B\ge 0$. Entonces, para todo $j\ge 0$ se cumple

$$
|\xi_j| \le |\xi_0|\,e^{jA} + \frac{B}{A}\big(e^{jA}-1\big).
$$

::: {.callout-caution collapse="true" title="Prueba"}

Sea $a_j := |\xi_j| \ge 0$. La hipótesis dice
$$
a_{j+1} \le (1+A)\,a_j + B, \qquad j\ge 0.
$$

**Paso 1 (desenrollando la recurrencia).**  
Probamos por inducción que para todo $j\ge 0$,
$$
a_j \le (1+A)^j a_0 + B\sum_{k=0}^{j-1}(1+A)^k. \tag{1}
$$

- Para $j=0$, la afirmación es trivial:
  $a_0 \le (1+A)^0 a_0 + 0$.

- Suponga que vale para $j$. Entonces, usando la recurrencia,
  $$
  \begin{aligned}
  a_{j+1}
  &\le (1+A)\,a_j + B \\
  &\le (1+A)\Big[(1+A)^j a_0 + B\sum_{k=0}^{j-1}(1+A)^k\Big] + B \\
  &= (1+A)^{j+1} a_0 + B\sum_{k=1}^{j}(1+A)^k + B \\
  &= (1+A)^{j+1} a_0 + B\sum_{k=0}^{j}(1+A)^k,
  \end{aligned}
  $$
  que es la fórmula (1) con $j$ reemplazado por $j+1$. Queda probado por inducción.

De (1) y la suma geométrica,
$$
\sum_{k=0}^{j-1}(1+A)^k=\frac{(1+A)^j-1}{A},
$$
obtenemos
$$
a_j \le (1+A)^j a_0 + \frac{B}{A}\big((1+A)^j-1\big). \tag{2}
$$

**Paso 2 (paso de \((1+A)^j\) a \(e^{Aj}\)).**  
Usando que $(1+A)^j \le e^{Aj}$ para $A>0$ (por $1+x\le e^x$), en (2) se concluye
$$
a_j \le a_0\,e^{Aj} + \frac{B}{A}\big(e^{Aj}-1\big).
$$

Recordando que $a_j=|\xi_j|$ y $a_0=|\xi_0|$, queda
$$
|\xi_j| \le |\xi_0|\,e^{jA} + \frac{B}{A}\big(e^{jA}-1\big), \qquad j=0,1,2,\ldots
$$
como se quería. $\blacksquare$

:::

:::

::: {.callout-note title="Teorema"}

Si la función $\varphi$ en un método de un paso es continua (con respecto a $h$) y satisface la **condición de Lipschitz**

$$
|\varphi(x,u;h) - \varphi(x,v;h)| \le M\,|u - v|,
$$

para todo $(x,u),(x,v)\in G$ y para $h$ suficientemente pequeño,  
entonces **un método de un paso es convergente si y solo si el método es consistente.**

::: {.callout-caution collapse="true" title="Prueba"}

**1. (Consistencia $\Rightarrow$ Convergencia)**

Sea $e_j = u_j - u(x_j)$ el error global en el nodo $x_j$.  
Se tiene que:

$$
\begin{aligned}
e_{j+1} - e_j
&= [u_{j+1} - u_j] - [u(x_{j+1}) - u(x_j)] \\
&= h\,\varphi(x_j,u_j;h) - [u(x_{j+1}) - u(x_j)] \\
&= h\,[\,\varphi(x_j,u_j;h) - \varphi(x_j,u(x_j);h) - \Delta(x_j,u(x_j);h)\,].
\end{aligned}
$$

Por la condición de Lipschitz, se obtiene:

$$
|e_{j+1} - e_j| \le h\,[\,M\,|u_j - u(x_j)| + c(h)\,], \tag{1.14}
$$

donde
$$
c(h) := \max_{a \le x \le b} |\Delta(x,u(x);h)|.
$$

Nótese que $c(h)\to 0$ cuando $h\to 0$, pues el método es consistente.

**2. (Aplicación del Lema 3)**

Como $e_j = u_j - u(x_j)$, la desigualdad (1.14) implica que:

$$
|e_{j+1}| \le (1 + hM)\,|e_j| + h\,c(h), \qquad j = 0,1,2,\ldots,n.
$$

Aplicando el **Lema 3** con $A = hM$, $B = h\,c(h)$ y observando que $e_0 = 0$, se deduce:

$$
|e_j| \le \frac{h\,c(h)}{hM}\,\big(e^{j hM} - 1\big), \qquad j=0,1,2,\ldots,n.
$$

Como $x_j = x_0 + jh$, esto se puede reescribir como:

$$
|e_j| \le \frac{c(h)}{M}\,\big(e^{M(x_j - x_0)} - 1\big), \qquad j=0,1,2,\ldots,n. \tag{1.15}
$$

Por tanto, el **error global máximo** cumple:

$$
E(h) = \max_j |e_j|
\le \frac{c(h)}{M}\,\big(e^{M(b-a)} - 1\big)
\longrightarrow 0 \quad \text{cuando } h\to 0.
$$

Esto prueba que el método es **convergente**.



**3. (Convergencia $\Rightarrow$ Consistencia)**


Recordemos (Def. 7) que el **error de discretización local** en un punto $(x,u)$ se
define por
$$
\Delta(x,u;h)=\frac{\eta(x+h)-\eta(x)}{h}-\varphi(x,u;h),
$$
donde $\eta$ es la solución del problema auxiliar
$\eta' = f(\xi,\eta)$ con condición inicial $\eta(x)=u$.
Además, por el Teorema 3, un método de un paso es **consistente**
si y sólo si
$$
\lim_{h\to0}\varphi(x,u;h)=f(x,u)
\qquad(\text{uniformemente en }(x,u)\in G).
$$

Sea ahora un método de un paso **convergente** en el sentido de la Def. 10.
Fijemos $(x,u)\in G$ y apliquemos el método sobre el intervalo corto
$[x,x+h]$ partiendo exactamente de $u(x)=u$.
Denotemos por
$$
U_1 := u + h\,\varphi(x,u;h)
$$
la aproximación numérica al valor exacto $\eta(x+h)$ tras un solo paso.
El error en ese único nodo es
$$
e_1 \;=\; U_1-\eta(x+h)
      \;=\; h\Big(\varphi(x,u;h)-\frac{\eta(x+h)-\eta(x)}{h}\Big)
      \;=\; -\,h\,\Delta(x,u;h).                                  \tag{1}
$$

Como el método es convergente, aplicado al problema en $[x,x+h]$
(se trata de un problema del mismo tipo pero en un intervalo de longitud $h$),
se tiene que el error global máximo en ese intervalo, que aquí coincide con
$|e_1|$, satisface
$$
|e_1| \longrightarrow 0 \qquad \text{cuando } h\to0 .
$$

Dividiendo (1) entre $h$ obtenemos
$$
|\Delta(x,u;h)| \;=\; \frac{|e_1|}{h}.
$$

Usando que $\varphi$ es Lipschitz en la segunda variable y continua en $h$
y que $f$ es localmente acotada, el error tras un paso satisface
la cota estándar (obtenida con Grönwall)
$$
|e_1| \;\le\; C\,h \qquad \text{para $h$ suficientemente pequeño,}
$$
con $C$ independiente de $h$ y de $(x,u)$ en compactos de $G$.
Por lo tanto
$$
|\Delta(x,u;h)|
 \;=\; \frac{|e_1|}{h}
 \;\le\; C \qquad\text{y, además,}\qquad
\lim_{h\to0} |\Delta(x,u;h)|=\lim_{h\to0}\frac{|e_1|}{h}=0,
$$
ya que $|e_1|=o(h)$ cuando $h\to0$ (convergencia en un paso).

Con ello se concluye que
$$
\lim_{h\to0}\Delta(x,u;h)=0
\quad\Longleftrightarrow\quad
\lim_{h\to0}\varphi(x,u;h)=\lim_{h\to0}\frac{\eta(x+h)-\eta(x)}{h}
= \eta'(x)=f(x,u),
$$
uniformemente en $(x,u)\in G$. Es decir, el método es **consistente**.

$\blacksquare$

:::

:::

::: {.callout-note title="Teorema"}
 
Asumiendo todas las hipótesis del teorema anterior, si además el método de un paso tiene **consistencia de orden** $p$  
(es decir, $|\Delta(x,u;h)| \le K h^p$), entonces:

$$
|e_j| \le \frac{K}{M}\big(e^{M(x_j - x_0)} - 1\big) h^p, 
\qquad j = 0,1,2,\ldots,n.
$$

Por lo tanto, la **convergencia** del método también tiene **orden $p$**.

::: {.callout-caution collapse="true" title="Prueba"}

De la ecuación $(1.15)$ se tiene:

$$
|e_j| \le \frac{c(h)}{M}\big(e^{M(x_j - x_0)} - 1\big), 
\qquad j = 0,1,2,\ldots,n,
$$

donde $c(h)$ está definido como:

$$
c(h) = \max_{a \le x \le b} |\Delta(x, u(x); h)|.
$$

Pero, por hipótesis de **consistencia de orden $p$**, se cumple:

$$
|\Delta(x_j, u(x_j); h)| \le K h^p.
$$

Entonces, sustituyendo en la desigualdad anterior:

$$
|e_j| \le \frac{K h^p}{M}\big(e^{M(x_j - x_0)} - 1\big).
$$

Así, el error global máximo satisface:

$$
E(h) = \max_j |e_j| \le \frac{K}{M}\big(e^{M(b - a)} - 1\big)h^p,
$$

lo cual demuestra que el método de un paso **convergente de orden $p$**.

$\blacksquare$

:::

:::

::: {.callout-note title="Corolario"}

1. El **método de Euler** es convergente y, si $f$ es continuamente diferenciable, entonces el **orden de convergencia** es $1$.

2. El **método de Euler modificado** (o de Heun) es convergente y, si $f$ es dos veces continuamente diferenciable, entonces el **orden de convergencia** es $2$.

::: {.callout-caution collapse="true" title="Prueba"}

**Para 1. (Euler clásico).**  
El método de Euler tiene
$$
\varphi(x,u;h)=f(x,u).
$$

- **Lipschitz en $u$:** Si $f$ es Lipschitz en su segunda variable con constante $L$ (uniforme en $x$), entonces
$$
|\varphi(x,u;h)-\varphi(x,v;h)|=|f(x,u)-f(x,v)|\le L\,|u-v|.
$$

- **Consistencia y orden:** Por el **Teorema 4** (ya demostrado), Euler es consistente y, si $f$ es $C^1$, su **orden de consistencia es $1$**.

Aplicando el **Teorema 6** (consistencia $\Longleftrightarrow$ convergencia bajo Lipschitz) y el **Teorema 7** (el orden de convergencia coincide con el orden de consistencia), se concluye que Euler **converge con orden $1$**.

**Para 2. (Euler modificado / Heun).**  
En este caso
$$
\varphi(x,u;h)=\frac12\Big[f(x,u)+f\!\big(x+h,\,u+h\,f(x,u)\big)\Big].
$$

- **Lipschitz en $u$:** Suponga que $f$ es Lipschitz en su segunda variable con constante $L$ (uniforme en $x$). Entonces, para $u,v$ cualesquiera,

\begin{align*}
|\varphi(x,u;h)-\varphi(x,v;h)|
&\le \tfrac12\,|f(x,u)-f(x,v)|
   +\tfrac12\,\Big|f\!\big(x+h,\,u+h f(x,u)\big)-f\!\big(x+h,\,v+h f(x,v)\big)\Big| \\
&\le \tfrac12\,L|u-v|
   +\tfrac12\,L\,\big|[u-v]+h\,[f(x,u)-f(x,v)]\big| \\
&\le \tfrac12\,L|u-v|
   +\tfrac12\,L\,(|u-v|+h\,L|u-v|) \\
&= L\Big(1+\tfrac12\,hL\Big)\,|u-v|.
\end{align*}

Así, $\varphi$ es Lipschitz en $u$ con constante $M=L(1+\tfrac12 hL)$ (válida para $h$ pequeño).

- **Consistencia y orden:** Por el resultado demostrado para Euler mejorado, si $f\in C^2$ entonces el método es **consistente de orden $2$**.

Aplicando de nuevo el **Teorema 6** (bajo Lipschitz hay equivalencia entre consistencia y convergencia) y el **Teorema 7** (el orden de convergencia coincide con el de consistencia), se concluye que el método de Euler modificado **converge con orden $2$**.

$\blacksquare$

:::

:::

::: {.callout-important collapse="true" title="Observación"}
Es importante notar que todos los **métodos de un paso** se construyen como sigue. Sean

- $s_l \in \mathbb{R}$ con $l=2,\ldots,m$,
- $c_{li}\in\mathbb{R}$ con $i=1,2,\ldots,l-1$; $l=2,\ldots,m$,
- $\alpha_l\in\mathbb{R}$ con $l=1,\ldots,m$,

y defínanse

$$
\begin{aligned}
k_1 &= f(x_j,u_j),\\
k_2 &= f\big(x_j+s_2h,\;u_j + c_{21}h\,k_1\big),\\
k_3 &= f\big(x_j+s_3h,\;u_j + c_{31}h\,k_1 + c_{32}h\,k_2\big),\\
&\ \ \vdots \\
k_m &= f\!\left(x_j+s_mh,\;u_j + h\sum_{i=1}^{m-1} c_{mi}k_i\right).
\end{aligned}
$$

Luego, recursivamente se calcula

$$
u_{j+1}=u_j + h\sum_{i=1}^{m}\alpha_i k_i.
$$
:::

::: {.callout-tip collapse="true" title="Ejemplo"}

- **Método de Euler.** Basta tomar $m=1$ y $\alpha_1=1$.

- **Método de Euler Mejorado (Heun).** Tomar $m=2$, $s_2=1$, $c_{21}=1$ y
  $$
  \alpha_1=\alpha_2=\tfrac12.
  $$

- **Runge–Kutta del Punto Medio.** Tomar $s_1=\tfrac12$ y $c_{m1}=\tfrac12$.
  En forma recursiva se obtiene

  $$
  \begin{cases}
  w_0=\alpha,\\[2mm]
  w_{i+1}=w_i+h\,f\!\left(t_i+\tfrac{h}{2},\,w_i+\tfrac{h}{2}f(t_i,w_i)\right),
  \end{cases}
  \qquad i=1,2,\ldots,N.
  $$
:::

::: {.callout-note title="Algoritmo: Método de Runge–Kutta del Punto Medio"}

**Entrada:** $a, b, N, \alpha, f(t,y)$  

**Salida:** Los pares $(t_i, w_i)$.

**Paso 1.** Tomar  
$$
h = \frac{b - a}{N}, \qquad t = a, \qquad w = \alpha.
$$  
     Salida $(t, w)$.

**Paso 2.**  
Para $i = 1, 2, 3, \ldots, N$ siga los pasos 3–6:

   **Paso 3.**  
\begin{align*}
k_1 &= h f(t, w), \\
k_2 &= h f\!\left(t + \frac{h}{2},\, w + \frac{k_1}{2}\right), \\
k_3 &= h f\!\left(t + \frac{h}{2},\, w + \frac{k_2}{2}\right), \\
k_4 &= h f(t + h,\, w + k_3).
\end{align*}

   **Paso 4.**  
$$
w = w + \frac{1}{6}\,(k_1 + 2k_2 + 2k_3 + k_4).
$$

   **Paso 5.**  
$$
t = t + h.
$$

   **Paso 6.**  Salida $(t, w)$.

**Paso 7.**  
Parar.


:::

::: {.callout-note title="Definición: Método de Runge–Kutta de 4º Orden"}

El método de **Runge–Kutta de cuarto orden (RK4)** para resolver el problema de valor inicial:

$$
\begin{cases}
u' = f(x, u), \\
u(x_0) = u_0,
\end{cases}
$$

construye una aproximación $u_j$ de $u(x_j)$ en la malla de puntos equidistantes  
$x_j := x_0 + jh$ para $j = 1, 2, \ldots$, usando las siguientes ecuaciones:

\begin{align*}
k_1 &= f(x_j, u_j), \\
k_2 &= f\!\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_1\right), \\
k_3 &= f\!\left(x_j + \frac{h}{2},\, u_j + \frac{h}{2}k_2\right), \\
k_4 &= f(x_j + h,\, u_j + h k_3), \\
u_{j+1} &= u_j + \frac{h}{6}\,(k_1 + 2k_2 + 2k_3 + k_4).
\end{align*}

::: {.callout-important collapse="true" title="Observaciones"}

1. El método fue introducido por **Runge** en 1895 y extendido por **Kutta** en 1901 para sistemas de ecuaciones diferenciales.

2. Si $u' = f(x)$, el método de Runge–Kutta y la **regla de Simpson** son equivalentes (pruébelo).

3. Resumiendo, el método de **Runge–Kutta** se puede escribir como:

$$
\begin{cases}
w_0 = \alpha, \\
w_{i+1} = w_i + \dfrac{1}{6}\,(k_1 + 2k_2 + 2k_3 + k_4),
\end{cases}
$$

donde:

\begin{align*}
k_1 &= h f(t_i, w_i), \\[0.5em]
k_2 &= h f\!\left(t_i + \frac{h}{2},\, w_i + \frac{k_1}{2}\right), \\[0.5em]
k_3 &= h f\!\left(t_i + \frac{h}{2},\, w_i + \frac{k_2}{2}\right), \\[0.5em]
k_4 &= h f(t_i + h,\, w_i + k_3).
\end{align*}

:::

:::

::: {.callout-note title="Algoritmo: Método de Runge–Kutta de 4º Orden"}

**Entrada:** $a, b, N, \alpha, f(t,y)$  

**Salida:** Los pares $(t_i, w_i)$.

**Paso 1.**  
Tomar
$$
h=\frac{b-a}{N}, \qquad t=a, \qquad w=\alpha.
$$
      Salida $(t,w)$.

**Paso 2.** Para $i=1,2,3,\ldots,N$ siga pasos 3–6:

   **Paso 3.**
\begin{align*}
k_1 &= h\,f(t,w),\\
k_2 &= h\,f\!\left(t+\frac{h}{2},\, w+\frac{k_1}{2}\right),\\
k_3 &= h\,f\!\left(t+\frac{h}{2},\, w+\frac{k_2}{2}\right),\\
k_4 &= h\,f(t+h,\, w+k_3).
\end{align*}

   **Paso 4.**  
$$
w \;=\; w + \frac{1}{6}\,(k_1 + 2k_2 + 2k_3 + k_4).
$$

   **Paso 5.**  
$$
t \;=\; t + h.
$$

   **Paso 6.** Salida $(t,w)$.

**Paso 7.** Parar.

:::

::: {.callout-tip collapse="true" title="Ejemplo"}
Resolver numéricamente con RK4
$$
\begin{cases}
y'=\dfrac{2}{t}\,y + t^{2}e^{t},\\[2mm]
y(1)=0,
\end{cases}
\qquad 1\le t\le 2,
$$
usando $N=10$, $a=1$, $b=2$, $\alpha=0$.
:::

::: {.callout-note title="Teorema"}
El método de Runge–Kutta (clásico de 4º orden) es **consistente**; además, si $f$ es $4$ veces continuamente diferenciable entonces su **orden de consistencia** es $4$.

::: {.callout-caution collapse="true" title="Prueba"}
Como
$$
\varphi \;=\; \frac{1}{6}\big(\varphi_1 + 2\varphi_2 + 2\varphi_3 + \varphi_4\big),
$$
donde

\begin{align*}
\varphi_1(x,u;h) &= f(x,u),\\[2mm]
\varphi_2(x,u;h) &= f\!\left(x+\frac{h}{2},\,u+\frac{h}{2}\,\varphi_1(x,u;h)\right),\\[2mm]
\varphi_3(x,u;h) &= f\!\left(x+\frac{h}{2},\,u+\frac{h}{2}\,\varphi_2(x,u;h)\right),\\[2mm]
\varphi_4(x,u;h) &= f\!\left(x+h,\,u+h\,\varphi_3(x,u;h)\right),
\end{align*}

es claro que, si $h\to 0$, entonces $\varphi \to f(x,u)$, de donde el método es **consistente**.

Probar que el orden es $4$ queda como ejercicio; sugerencia: utilice expansiones de Taylor y la ecuación diferencial. $\blacksquare$
:::

:::